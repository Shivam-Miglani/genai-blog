{
    "cells": [
        {
            "cell_type": "raw",
            "id": "6da18859",
            "metadata": {
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "---\n",
                "title: \"Retrieval-Augmented Generation (RAG) Fundamentals\"\n",
                "author: \"Shivam Miglani\"\n",
                "date: \"2025-12-22\"\n",
                "categories: RAG\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f08dc2f9",
            "metadata": {},
            "source": [
                "::: {.callout-note appearance=\"simple\"}\n",
                "### Licensing Notice\n",
                "Text and media: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)\n",
                "Code and snippets: [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
                ":::"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b81e3ae",
            "metadata": {},
            "source": [
                "## Motivation and Meaning\n",
                "\n",
                "**R**etrieval **A**ugmented **G**eneration (RAG) is inference-time context injection: pull **relevant** external data and condition generation on it. This *grounds* responses in authoritative sources while keeping LLMs parametric knowledge (weights) frozen.\n",
                "\n",
                "#### Motivation\n",
                "- **Knowledge limited to training data**: Your proprietary/domain-specific data isn't there\n",
                "- **Fixed knowledge cut-off date**: The model can't answer questions about recent events. Without RAG, they will either refuse to answer or *hallucinate*.\n",
                "\n",
                "##### Alternatives & Trade-offs\n",
                "- Why not fine-tune?\n",
                "    - Fine-tuning excels at teaching *task formats* (SQL generation, JSON output) and *reasoning styles*, not injecting factual knowledge.\n",
                "    - **Catastrophic forgetting**: Updating knowledge degrades performance on other tasks as model weights get overwritten.\n",
                "    - **Inefficient**: requires separate fine-tuned checkpoints per domain or use-case or document. It is tricky to learn new knowledge without regressing on old knowledge even with LoRA/QLoRA. \n",
                "- Ok, why not just stuff everything in the context window?\n",
                "    - **Recall Degradation**: While 1M+ token models ace \"single-needle\" tests, performance drops significantly (to ~60-70%) when retrieving multiple distributed facts\n",
                "    - **Cost & Latency**: Processing massive contexts is computationally expensive and slow compared to vector search. Retrieval remains necessary for corpora exceeding the window size.\n",
                "- RAG is a reasonable pattern:\n",
                "    - When you need fresh, attributable knowledge with minimal model changes and can tolerate added latency.\n",
                "\n",
                "#### The RAG Pipeline\n",
                "RAG acts as a filter to inject only *relevant* context. A typical production pipeline looks like this:\n",
                "\n",
                "0. **Ingestion & Indexing**: Chunk documents, generate embeddings, and upsert into a vector database. *Note: In production, this is a continuous sync pipeline, not a one-time setup.*\n",
                "1. **Retrieval**: For a user query, search your indexed corpus (vector/keyword) and pull the top‑k relevant chunks, often followed by a *re-ranking* step for precision.\n",
                "2. **Augmented (prompt)**: Inject selected chunks into the system prompt or user message with appropriate metadata (source citations).\n",
                "3. **Generation**: The LLM generates an answer conditioned *strictly* on the provided context, minimizing external knowledge leakage."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6888caad",
            "metadata": {
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "```{mermaid}\n",
                "%%| label: fig-mermaid-ragv1\n",
                "%%| fig-cap: \"RAG pipeline flowchart showing ingestion pipeline, query processing, retrieval, augmentation and LLM generation.\"\n",
                "\n",
                "flowchart TB\n",
                "    n2[\"LLM\"] L_n2_n4_0@-- generates grounded answer --> n4[\"Answer\"]\n",
                "    n3[\"Document Corpus\"] L_n3_n5_0@<-- ingestion pipeline<br>(chunk + embed) --> n5[\"Hybrid index<br>(inverted keywords<br>+ <br>vector embeddings)<br><br>\"]\n",
                "    n5 L_n5_n6_0@-- \"top-k\" --> n6[\"Retrieval &amp; Re-ranking\"]\n",
                "    n6 L_n6_n7_0@-- \"top-k re-ranked chunks + citation metadata\" --> n7[\"Prompt Builder\"]\n",
                "    n7 L_n7_n2_0@-- \"system prompt (use only given context) + user query + <br>top-k re-ranked chunks &amp; citation metadata\" --> n2\n",
                "    n1[\"User Query\"] L_n1_n8_0@--> n8[\"Query processing <br>&amp; embedding\"]\n",
                "    n1 L_n1_n7_0@-- user query --> n7\n",
                "    n8 L_n8_n6_0@-- text + query expansions + embeddings --> n6\n",
                "\n",
                "    n3@{ shape: docs}\n",
                "    n5@{ shape: cyl}\n",
                "    n6@{ shape: rect}\n",
                "    n7@{ shape: rect}\n",
                "    n1@{ shape: rect}\n",
                "    n8@{ shape: rect}\n",
                "\n",
                "    L_n2_n4_0@{ animation: slow } \n",
                "    L_n3_n5_0@{ animation: none } \n",
                "    L_n5_n6_0@{ animation: slow } \n",
                "    L_n6_n7_0@{ animation: slow } \n",
                "    L_n7_n2_0@{ animation: slow } \n",
                "    L_n1_n8_0@{ animation: slow } \n",
                "    L_n1_n7_0@{ animation: slow } \n",
                "    L_n8_n6_0@{ animation: slow }\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "4536ff63",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Spark + Delta session created!\n"
                    ]
                }
            ],
            "source": [
                "import pyspark\n",
                "from delta import *\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "builder = (SparkSession.builder\n",
                "           .appName(\"LocalDatabricksPrep\")\n",
                "           .master(\"local[*]\")\n",
                "           .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
                "           .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
                "           # This downloads the Delta jar automatically:\n",
                "           .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\") \n",
                ")\n",
                "\n",
                "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
                "\n",
                "print(\"Spark + Delta session created!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "3f52f383",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
                        "|show_id|type   |title|director         |cast                                                                                                                                                                      |country      |date_added       |release_year|rating|duration |listed_in                                               |description                                                                                                                                          |\n",
                        "+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
                        "|s1     |TV Show|3%   |NULL             |João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi|Brazil       |August 14, 2020  |2020        |TV-MA |4 Seasons|International TV Shows, TV Dramas, TV Sci-Fi & Fantasy  |In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.             |\n",
                        "|s2     |Movie  |7:19 |Jorge Michel Grau|Demián Bichir, Héctor Bonilla, Oscar Serrano, Azalia Ortiz, Octavio Michel, Carmen Beato                                                                                  |Mexico       |December 23, 2016|2016        |TV-MA |93 min   |Dramas, International Movies                            |After a devastating earthquake hits Mexico City, trapped survivors from all walks of life wait to be rescued while trying desperately to stay alive. |\n",
                        "|s3     |Movie  |23:59|Gilbert Chan     |Tedd Chan, Stella Chung, Henley Hii, Lawrence Koh, Tommy Kuan, Josh Lai, Mark Lee, Susan Leong, Benjamin Lim                                                              |Singapore    |December 20, 2018|2011        |R     |78 min   |Horror Movies, International Movies                     |When an army recruit is found dead, his fellow soldiers are forced to confront a terrifying secret that's haunting their jungle island training camp.|\n",
                        "|s4     |Movie  |9    |Shane Acker      |Elijah Wood, John C. Reilly, Jennifer Connelly, Christopher Plummer, Crispin Glover, Martin Landau, Fred Tatasciore, Alan Oppenheimer, Tom Kane                           |United States|November 16, 2017|2009        |PG-13 |80 min   |Action & Adventure, Independent Movies, Sci-Fi & Fantasy|In a postapocalyptic world, rag-doll robots hide in fear from dangerous machines out to exterminate them, until a brave newcomer joins the group.    |\n",
                        "|s5     |Movie  |21   |Robert Luketic   |Jim Sturgess, Kevin Spacey, Kate Bosworth, Aaron Yoo, Liza Lapira, Jacob Pitts, Laurence Fishburne, Jack McGee, Josh Gad, Sam Golzari, Helen Carey, Jack Gilpin           |United States|January 1, 2020  |2008        |PG-13 |123 min  |Dramas                                                  |A brilliant group of students become card-counting experts with the intent of swindling millions out of Las Vegas casinos by playing blackjack.      |\n",
                        "+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
                        "only showing top 5 rows\n"
                    ]
                }
            ],
            "source": [
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql.window import Window\n",
                "import pyspark.sql.types as T\n",
                "import pandas as pd\n",
                "from pyspark import SparkFiles\n",
                "\n",
                "\n",
                "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv\"\n",
                "spark.sparkContext.addFile(url)\n",
                "\n",
                "\n",
                "# 1. Download Raw CSV directly\n",
                "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"file://\"+ SparkFiles.get(\"netflix_titles.csv\"))\n",
                "df.show(truncate=False, n=5)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "f78c8d83",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "root\n",
                        " |-- show_id: string (nullable = true)\n",
                        " |-- type: string (nullable = true)\n",
                        " |-- title: string (nullable = true)\n",
                        " |-- director: string (nullable = true)\n",
                        " |-- cast: string (nullable = true)\n",
                        " |-- country: string (nullable = true)\n",
                        " |-- date_added: string (nullable = true)\n",
                        " |-- release_year: string (nullable = true)\n",
                        " |-- rating: string (nullable = true)\n",
                        " |-- duration: string (nullable = true)\n",
                        " |-- listed_in: string (nullable = true)\n",
                        " |-- description: string (nullable = true)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "ed3f5d68",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----+------------+-----------------+----------------+------------------+---------+\n",
                        "|title|release_year|       date_added|date_added_clean|release_year_clean|days_diff|\n",
                        "+-----+------------+-----------------+----------------+------------------+---------+\n",
                        "|   3%|        2020|  August 14, 2020|      2020-08-14|        2020-01-01|      226|\n",
                        "| 7:19|        2016|December 23, 2016|      2016-12-23|        2016-01-01|      357|\n",
                        "|23:59|        2011|December 20, 2018|      2018-12-20|        2011-01-01|     2910|\n",
                        "|    9|        2009|November 16, 2017|      2017-11-16|        2009-01-01|     3241|\n",
                        "|   21|        2008|  January 1, 2020|      2020-01-01|        2008-01-01|     4383|\n",
                        "+-----+------------+-----------------+----------------+------------------+---------+\n",
                        "only showing top 5 rows\n"
                    ]
                }
            ],
            "source": [
                "# The Task:\n",
                "\n",
                "# From netflix_bronze, select title, release_year, and date_added.\n",
                "\n",
                "# Clean date_added (currently string like \"September 25, 2021\") into a real date.\n",
                "\n",
                "# Calculate days_diff = date_added - release_year (assume Jan 1st).\n",
                "\n",
                "\n",
                "sub_df = df.select(\n",
                "    F.col(\"title\"),\n",
                "    F.col(\"release_year\"),\n",
                "    F.col(\"date_added\"),\n",
                ")\n",
                "\n",
                "sub_df = sub_df.withColumn(\"date_added_clean\", F.to_date(F.col(\"date_added\"), \"MMMM d, yyyy\"))\n",
                "sub_df = sub_df.withColumn(\"release_year_clean\", F.to_date(F.col(\"release_year\"), \"y\"))\n",
                "# Assuming 1 jan of release year\n",
                "sub_df = sub_df.withColumn(\"days_diff\", F.date_diff(F.col(\"date_added_clean\"), F.col(\"release_year_clean\"))) # default is somehow days\n",
                "\n",
                "\n",
                "sub_df.show(n=5)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "4d2bf442",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+--------------------+\n",
                        "|               title|\n",
                        "+--------------------+\n",
                        "|A Stoning in Fulh...|\n",
                        "|               Babel|\n",
                        "|          By the Sea|\n",
                        "|Inglourious Basterds|\n",
                        "| Killing Them Softly|\n",
                        "|    Ocean's Thirteen|\n",
                        "|      Ocean's Twelve|\n",
                        "|         War Machine|\n",
                        "+--------------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Get a simple list of movies for \"Brad Pitt\".\n",
                "\n",
                "df.filter(F.lower(F.col(\"cast\")).contains(\"brad pitt\")).select(F.col(\"title\")).show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "f5d6cb74",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------------+-----+\n",
                        "|    cast_exploded|count|\n",
                        "+-----------------+-----+\n",
                        "|      Anupam Kher|   38|\n",
                        "| Takahiro Sakurai|   28|\n",
                        "|          Om Puri|   27|\n",
                        "|   Shah Rukh Khan|   27|\n",
                        "|      Boman Irani|   25|\n",
                        "+-----------------+-----+\n",
                        "only showing top 5 rows\n"
                    ]
                }
            ],
            "source": [
                "# \"Who are the top 5 most frequent actors in the dataset?\"\n",
                "temp_df = df.withColumn(\"cast_split\", F.split(F.col(\"cast\"), \",\")).withColumn(\"cast_exploded\", F.explode(F.col(\"cast_split\")))\n",
                "temp_df.groupBy(F.col(\"cast_exploded\")).agg({\"cast_exploded\": 'count'}).select(\"cast_exploded\", F.col(\"count(cast_exploded)\").alias(\"count\")).orderBy(F.col(\"count\").desc()).show(n=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "db0c5cb8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------------+-----+\n",
                        "|    cast_exploded|count|\n",
                        "+-----------------+-----+\n",
                        "|      Anupam Kher|   38|\n",
                        "| Takahiro Sakurai|   28|\n",
                        "|          Om Puri|   27|\n",
                        "|   Shah Rukh Khan|   27|\n",
                        "|      Boman Irani|   25|\n",
                        "+-----------------+-----+\n",
                        "only showing top 5 rows\n"
                    ]
                }
            ],
            "source": [
                "temp_df.groupBy(\n",
                "    F.col(\"cast_exploded\")\n",
                ").agg(\n",
                "    F.count(\"*\").alias(\"count\")\n",
                ").orderBy(F.col(\"count\").desc()).show(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "f8acd96f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+\n",
                        "|show_id|   type| title|            director|                cast|             country|       date_added|release_year|rating| duration|           listed_in|         description|          cast_split|    cast_split_clean|\n",
                        "+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+\n",
                        "|     s1|TV Show|    3%|                NULL|João Miguel, Bian...|              Brazil|  August 14, 2020|        2020| TV-MA|4 Seasons|International TV ...|In a future where...|[João Miguel,  Bi...|[João Miguel, Bia...|\n",
                        "|     s2|  Movie|  7:19|   Jorge Michel Grau|Demián Bichir, Hé...|              Mexico|December 23, 2016|        2016| TV-MA|   93 min|Dramas, Internati...|After a devastati...|[Demián Bichir,  ...|[Demián Bichir, H...|\n",
                        "|     s3|  Movie| 23:59|        Gilbert Chan|Tedd Chan, Stella...|           Singapore|December 20, 2018|        2011|     R|   78 min|Horror Movies, In...|When an army recr...|[Tedd Chan,  Stel...|[Tedd Chan, Stell...|\n",
                        "|     s4|  Movie|     9|         Shane Acker|Elijah Wood, John...|       United States|November 16, 2017|        2009| PG-13|   80 min|Action & Adventur...|In a postapocalyp...|[Elijah Wood,  Jo...|[Elijah Wood, Joh...|\n",
                        "|     s5|  Movie|    21|      Robert Luketic|Jim Sturgess, Kev...|       United States|  January 1, 2020|        2008| PG-13|  123 min|              Dramas|A brilliant group...|[Jim Sturgess,  K...|[Jim Sturgess, Ke...|\n",
                        "|     s6|TV Show|    46|         Serdar Akar|Erdal Beşikçioğlu...|              Turkey|     July 1, 2017|        2016| TV-MA| 1 Season|International TV ...|A genetics profes...|[Erdal Beşikçioğl...|[Erdal Beşikçioğl...|\n",
                        "|     s7|  Movie|   122|     Yasir Al Yasiri|Amina Khalil, Ahm...|               Egypt|     June 1, 2020|        2019| TV-MA|   95 min|Horror Movies, In...|After an awful ac...|[Amina Khalil,  A...|[Amina Khalil, Ah...|\n",
                        "|     s8|  Movie|   187|      Kevin Reynolds|Samuel L. Jackson...|       United States| November 1, 2019|        1997|     R|  119 min|              Dramas|After one of his ...|[Samuel L. Jackso...|[Samuel L. Jackso...|\n",
                        "|     s9|  Movie|   706|       Shravan Kumar|Divya Dutta, Atul...|               India|    April 1, 2019|        2019| TV-14|  118 min|Horror Movies, In...|When a doctor goe...|[Divya Dutta,  At...|[Divya Dutta, Atu...|\n",
                        "|    s10|  Movie|  1920|        Vikram Bhatt|Rajneesh Duggal, ...|               India|December 15, 2017|        2008| TV-MA|  143 min|Horror Movies, In...|An architect and ...|[Rajneesh Duggal,...|[Rajneesh Duggal,...|\n",
                        "|    s11|  Movie|  1922|        Zak Hilditch|Thomas Jane, Moll...|       United States| October 20, 2017|        2017| TV-MA|  103 min|   Dramas, Thrillers|A farmer pens a c...|[Thomas Jane,  Mo...|[Thomas Jane, Mol...|\n",
                        "|    s12|TV Show|  1983|                NULL|Robert Więckiewic...|Poland, United St...|November 30, 2018|        2018| TV-MA| 1 Season|Crime TV Shows, I...|In this dark alt-...|[Robert Więckiewi...|[Robert Więckiewi...|\n",
                        "|    s13|TV Show|  1994|Diego Enrique Osorno|                NULL|              Mexico|     May 17, 2019|        2019| TV-MA| 1 Season|Crime TV Shows, D...|Archival video an...|                NULL|                NULL|\n",
                        "|    s14|  Movie| 2,215| Nottapon Boonprakob|  Artiwara Kongmalai|            Thailand|    March 1, 2019|        2018| TV-MA|   89 min|Documentaries, In...|This intimate doc...|[Artiwara Kongmalai]|[Artiwara Kongmalai]|\n",
                        "|    s15|  Movie|  3022|          John Suits|Omar Epps, Kate W...|       United States|   March 19, 2020|        2019|     R|   91 min|Independent Movie...|Stranded when the...|[Omar Epps,  Kate...|[Omar Epps, Kate ...|\n",
                        "|    s16|  Movie|Oct-01|      Kunle Afolayan|Sadiq Daba, David...|             Nigeria|September 1, 2019|        2014| TV-14|  149 min|Dramas, Internati...|Against the backd...|[Sadiq Daba,  Dav...|[Sadiq Daba, Davi...|\n",
                        "|    s17|TV Show|Feb-09|                NULL|Shahd El Yaseen, ...|                NULL|   March 20, 2019|        2018| TV-14| 1 Season|International TV ...|As a psychology p...|[Shahd El Yaseen,...|[Shahd El Yaseen,...|\n",
                        "|    s18|  Movie|22-Jul|     Paul Greengrass|Anders Danielsen ...|Norway, Iceland, ...| October 10, 2018|        2018|     R|  144 min|   Dramas, Thrillers|After devastating...|[Anders Danielsen...|[Anders Danielsen...|\n",
                        "|    s19|  Movie|15-Aug|  Swapnaneel Jayakar|Rahul Pethe, Mrun...|               India|   March 29, 2019|        2019| TV-14|  124 min|Comedies, Dramas,...|On India's Indepe...|[Rahul Pethe,  Mr...|[Rahul Pethe, Mru...|\n",
                        "|    s20|  Movie|   '89|                NULL|Lee Dixon, Ian Wr...|      United Kingdom|     May 16, 2018|        2017| TV-PG|   87 min|       Sports Movies|Mixing old footag...|[Lee Dixon,  Ian ...|[Lee Dixon, Ian W...|\n",
                        "+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "temp_df = df.withColumn(\"cast_split\", F.split(F.col(\"cast\"), \",\")).withColumn(\"cast_split_clean\", F.transform(F.col(\"cast_split\"), lambda x: F.trim(x)))\n",
                "temp_df.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "id": "60d14329",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------------+--------------------+------------+-----+---------+------------------+\n",
                        "|       actor|               title|release_year|count|prev_year|         gap_years|\n",
                        "+------------+--------------------+------------+-----+---------+------------------+\n",
                        "|Akshay Kumar|Mujhse Shaadi Karogi|        2004|   29|     2004|               0.0|\n",
                        "|Akshay Kumar|             Bewafaa|        2005|   29|     2004|1.0027397260273974|\n",
                        "|Akshay Kumar|               Insan|        2005|   29|     2005|               0.0|\n",
                        "|Akshay Kumar|         Bhagam Bhag|        2006|   29|     2005|               1.0|\n",
                        "|Akshay Kumar|Humko Deewana Kar...|        2006|   29|     2006|               0.0|\n",
                        "|Akshay Kumar|Jaan-E-Mann: Let'...|        2006|   29|     2006|               0.0|\n",
                        "|Akshay Kumar|     Phir Hera Pheri|        2006|   29|     2006|               0.0|\n",
                        "|Akshay Kumar|     Bhool Bhulaiyaa|        2007|   29|     2006|               1.0|\n",
                        "|Akshay Kumar|     Namastey London|        2007|   29|     2007|               0.0|\n",
                        "|Akshay Kumar|             Welcome|        2007|   29|     2007|               0.0|\n",
                        "|Akshay Kumar|      Action Replayy|        2010|   29|     2007|3.0027397260273974|\n",
                        "|Akshay Kumar|      Tees Maar Khan|        2010|   29|     2010|               0.0|\n",
                        "|Akshay Kumar|       Patiala House|        2011|   29|     2010|               1.0|\n",
                        "|Akshay Kumar|           Thank You|        2011|   29|     2011|               0.0|\n",
                        "|Akshay Kumar|               Joker|        2012|   29|     2011|               1.0|\n",
                        "|Akshay Kumar|           Oh My God|        2012|   29|     2012|               0.0|\n",
                        "|Akshay Kumar|       Rowdy Rathore|        2012|   29|     2012|               0.0|\n",
                        "|Akshay Kumar|                Boss|        2013|   29|     2012|1.0027397260273974|\n",
                        "|Akshay Kumar|Once Upon a Time ...|        2013|   29|     2013|               0.0|\n",
                        "|Akshay Kumar|          Special 26|        2013|   29|     2013|               0.0|\n",
                        "+------------+--------------------+------------+-----+---------+------------------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "# “For the top 10 actors by movie count, what is the average gap in years between their consecutive movies?”\n",
                "exploded_temp_df = temp_df.withColumn(\"actor\", F.explode(F.col(\"cast_split_clean\"))).filter(F.col(\"actor\") != \"\").select(\"actor\", \"title\", \"release_year\")\n",
                "# exploded_temp_df.show()\n",
                "\n",
                "actor_counts = exploded_temp_df.groupBy(F.col(\"actor\")).agg(F.count(\"*\").alias(\"count\")).orderBy(F.col(\"count\").desc())\n",
                "# actor_counts.show()\n",
                "\n",
                "top10 = actor_counts.limit(10)\n",
                "\n",
                "\n",
                "# join back on original table\n",
                "top10_actors = exploded_temp_df.join(F.broadcast(top10), on=\"actor\", how=\"inner\")\n",
                "# top10_actors.show()\n",
                "\n",
                "\n",
                "# window func.\n",
                "\n",
                "w = Window.partitionBy(\"actor\").orderBy(\"release_year\")\n",
                "\n",
                "\n",
                "# 5. Compute previous movie year using lag, then gap\n",
                "gaps_df = (top10_actors\n",
                "    .withColumn(\"prev_year\", F.lag(\"release_year\").over(w))\n",
                "    .filter(F.col(\"prev_year\").isNotNull())\n",
                "    .withColumn(\"gap_years\", F.datediff(F.col(\"release_year\"), F.col(\"prev_year\"))/F.lit(365.0))\n",
                ")\n",
                "\n",
                "gaps_df.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "id": "f623a1d3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+----------------+-------------------+\n",
                        "|           actor|       avg_gap_year|\n",
                        "+----------------+-------------------+\n",
                        "|       Yuki Kaji|0.38482613277133826|\n",
                        "|Takahiro Sakurai| 0.4288649706457926|\n",
                        "|    Akshay Kumar| 0.5361056751467711|\n",
                        "|     Boman Irani| 0.6158061116965227|\n",
                        "|     Anupam Kher| 0.7077848312729702|\n",
                        "|  Shah Rukh Khan| 0.7946817082997581|\n",
                        "|    Paresh Rawal|  1.116122233930453|\n",
                        "|         Om Puri|  1.207746811525744|\n",
                        "|Naseeruddin Shah| 1.2422295701464334|\n",
                        "|Amitabh Bachchan| 1.6934668071654373|\n",
                        "+----------------+-------------------+\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "25/12/23 16:39:25 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 956198 ms exceeds timeout 120000 ms\n",
                        "25/12/23 16:39:25 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
                        "25/12/23 16:39:26 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:39:26 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:39:36 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:39:36 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:39:46 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:39:46 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:39:56 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:39:56 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:54:23 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:54:23 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:54:33 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:54:33 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:54:43 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:54:43 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:54:53 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:54:53 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:55:03 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 16:55:03 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:10:17 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:10:17 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:10:27 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:10:27 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:10:37 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:10:37 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:10:47 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:10:47 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:10:57 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:10:57 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:26:18 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:26:18 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:26:28 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:26:28 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:26:38 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:26:38 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:26:48 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:26:48 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:26:58 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:26:58 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:42:57 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:42:57 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:43:07 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:43:07 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:43:17 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:43:17 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:43:27 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:43:27 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:43:37 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:43:37 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:55:28 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:55:28 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:55:38 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:55:38 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:55:48 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:55:48 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:55:58 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:55:58 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:56:08 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 17:56:08 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:12:53 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:12:53 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:13:03 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:13:03 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:13:13 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:13:13 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:13:23 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:13:23 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:30:28 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:30:28 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:30:38 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:30:38 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:30:48 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:30:48 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:30:58 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:30:58 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:31:08 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:31:08 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:31:18 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:31:18 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:31:28 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:31:28 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:48:04 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:48:04 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:48:14 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:48:14 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:48:24 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:48:24 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:48:34 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:48:34 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:48:44 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:48:44 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:56:32 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:56:32 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:56:42 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:56:42 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:56:52 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:56:52 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:57:02 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 18:57:02 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:12:17 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:12:17 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:12:27 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:12:27 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:12:37 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:12:37 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:12:47 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:12:47 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:12:57 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:12:57 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:28:57 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:28:57 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:07 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:07 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:17 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:17 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:27 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:27 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:37 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:37 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:47 WARN Executor: Issue communicating with driver in heartbeater\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
                        "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
                        "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
                        "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
                        "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
                        "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
                        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
                        "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
                        "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
                        "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
                        "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
                        "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\t... 3 more\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:47 ERROR Inbox: Ignoring error\n",
                        "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
                        "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
                        "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
                        "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
                        "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
                        "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
                        "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
                        "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
                        "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
                        "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
                        "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
                        "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
                        "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
                        "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
                        "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
                        "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
                        "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
                        "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
                        "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
                        "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
                        "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
                        "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
                        "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
                        "\t... 8 more\n",
                        "25/12/23 19:29:47 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"
                    ]
                }
            ],
            "source": [
                "gaps_df.groupBy(\"actor\").agg(F.avg(F.col(\"gap_years\")).alias(\"avg_gap_year\")).orderBy(F.col(\"avg_gap_year\").asc()).show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "950e0541",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 7787 entries, 0 to 7786\n",
                        "Data columns (total 12 columns):\n",
                        " #   Column        Non-Null Count  Dtype \n",
                        "---  ------        --------------  ----- \n",
                        " 0   show_id       7787 non-null   object\n",
                        " 1   type          7787 non-null   object\n",
                        " 2   title         7787 non-null   object\n",
                        " 3   director      5398 non-null   object\n",
                        " 4   cast          7069 non-null   object\n",
                        " 5   country       7280 non-null   object\n",
                        " 6   date_added    7777 non-null   object\n",
                        " 7   release_year  7787 non-null   int64 \n",
                        " 8   rating        7780 non-null   object\n",
                        " 9   duration      7787 non-null   object\n",
                        " 10  listed_in     7787 non-null   object\n",
                        " 11  description   7787 non-null   object\n",
                        "dtypes: int64(1), object(11)\n",
                        "memory usage: 730.2+ KB\n"
                    ]
                }
            ],
            "source": [
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "9823034e",
            "metadata": {},
            "outputs": [],
            "source": [
                "df['release_year'] = df['release_year'].astype('category')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "c82f4880",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Help on method describe in module pandas.core.generic:\n",
                        "\n",
                        "describe(percentiles=None, include=None, exclude=None) -> 'Self' method of pandas.core.frame.DataFrame instance\n",
                        "    Generate descriptive statistics.\n",
                        "\n",
                        "    Descriptive statistics include those that summarize the central\n",
                        "    tendency, dispersion and shape of a\n",
                        "    dataset's distribution, excluding ``NaN`` values.\n",
                        "\n",
                        "    Analyzes both numeric and object series, as well\n",
                        "    as ``DataFrame`` column sets of mixed data types. The output\n",
                        "    will vary depending on what is provided. Refer to the notes\n",
                        "    below for more detail.\n",
                        "\n",
                        "    Parameters\n",
                        "    ----------\n",
                        "    percentiles : list-like of numbers, optional\n",
                        "        The percentiles to include in the output. All should\n",
                        "        fall between 0 and 1. The default is\n",
                        "        ``[.25, .5, .75]``, which returns the 25th, 50th, and\n",
                        "        75th percentiles.\n",
                        "    include : 'all', list-like of dtypes or None (default), optional\n",
                        "        A white list of data types to include in the result. Ignored\n",
                        "        for ``Series``. Here are the options:\n",
                        "\n",
                        "        - 'all' : All columns of the input will be included in the output.\n",
                        "        - A list-like of dtypes : Limits the results to the\n",
                        "          provided data types.\n",
                        "          To limit the result to numeric types submit\n",
                        "          ``numpy.number``. To limit it instead to object columns submit\n",
                        "          the ``numpy.object`` data type. Strings\n",
                        "          can also be used in the style of\n",
                        "          ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To\n",
                        "          select pandas categorical columns, use ``'category'``\n",
                        "        - None (default) : The result will include all numeric columns.\n",
                        "    exclude : list-like of dtypes or None (default), optional,\n",
                        "        A black list of data types to omit from the result. Ignored\n",
                        "        for ``Series``. Here are the options:\n",
                        "\n",
                        "        - A list-like of dtypes : Excludes the provided data types\n",
                        "          from the result. To exclude numeric types submit\n",
                        "          ``numpy.number``. To exclude object columns submit the data\n",
                        "          type ``numpy.object``. Strings can also be used in the style of\n",
                        "          ``select_dtypes`` (e.g. ``df.describe(exclude=['O'])``). To\n",
                        "          exclude pandas categorical columns, use ``'category'``\n",
                        "        - None (default) : The result will exclude nothing.\n",
                        "\n",
                        "    Returns\n",
                        "    -------\n",
                        "    Series or DataFrame\n",
                        "        Summary statistics of the Series or Dataframe provided.\n",
                        "\n",
                        "    See Also\n",
                        "    --------\n",
                        "    DataFrame.count: Count number of non-NA/null observations.\n",
                        "    DataFrame.max: Maximum of the values in the object.\n",
                        "    DataFrame.min: Minimum of the values in the object.\n",
                        "    DataFrame.mean: Mean of the values.\n",
                        "    DataFrame.std: Standard deviation of the observations.\n",
                        "    DataFrame.select_dtypes: Subset of a DataFrame including/excluding\n",
                        "        columns based on their dtype.\n",
                        "\n",
                        "    Notes\n",
                        "    -----\n",
                        "    For numeric data, the result's index will include ``count``,\n",
                        "    ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\n",
                        "    upper percentiles. By default the lower percentile is ``25`` and the\n",
                        "    upper percentile is ``75``. The ``50`` percentile is the\n",
                        "    same as the median.\n",
                        "\n",
                        "    For object data (e.g. strings or timestamps), the result's index\n",
                        "    will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\n",
                        "    is the most common value. The ``freq`` is the most common value's\n",
                        "    frequency. Timestamps also include the ``first`` and ``last`` items.\n",
                        "\n",
                        "    If multiple object values have the highest count, then the\n",
                        "    ``count`` and ``top`` results will be arbitrarily chosen from\n",
                        "    among those with the highest count.\n",
                        "\n",
                        "    For mixed data types provided via a ``DataFrame``, the default is to\n",
                        "    return only an analysis of numeric columns. If the dataframe consists\n",
                        "    only of object and categorical data without any numeric columns, the\n",
                        "    default is to return an analysis of both the object and categorical\n",
                        "    columns. If ``include='all'`` is provided as an option, the result\n",
                        "    will include a union of attributes of each type.\n",
                        "\n",
                        "    The `include` and `exclude` parameters can be used to limit\n",
                        "    which columns in a ``DataFrame`` are analyzed for the output.\n",
                        "    The parameters are ignored when analyzing a ``Series``.\n",
                        "\n",
                        "    Examples\n",
                        "    --------\n",
                        "    Describing a numeric ``Series``.\n",
                        "\n",
                        "    >>> s = pd.Series([1, 2, 3])\n",
                        "    >>> s.describe()\n",
                        "    count    3.0\n",
                        "    mean     2.0\n",
                        "    std      1.0\n",
                        "    min      1.0\n",
                        "    25%      1.5\n",
                        "    50%      2.0\n",
                        "    75%      2.5\n",
                        "    max      3.0\n",
                        "    dtype: float64\n",
                        "\n",
                        "    Describing a categorical ``Series``.\n",
                        "\n",
                        "    >>> s = pd.Series(['a', 'a', 'b', 'c'])\n",
                        "    >>> s.describe()\n",
                        "    count     4\n",
                        "    unique    3\n",
                        "    top       a\n",
                        "    freq      2\n",
                        "    dtype: object\n",
                        "\n",
                        "    Describing a timestamp ``Series``.\n",
                        "\n",
                        "    >>> s = pd.Series([\n",
                        "    ...     np.datetime64(\"2000-01-01\"),\n",
                        "    ...     np.datetime64(\"2010-01-01\"),\n",
                        "    ...     np.datetime64(\"2010-01-01\")\n",
                        "    ... ])\n",
                        "    >>> s.describe()\n",
                        "    count                      3\n",
                        "    mean     2006-09-01 08:00:00\n",
                        "    min      2000-01-01 00:00:00\n",
                        "    25%      2004-12-31 12:00:00\n",
                        "    50%      2010-01-01 00:00:00\n",
                        "    75%      2010-01-01 00:00:00\n",
                        "    max      2010-01-01 00:00:00\n",
                        "    dtype: object\n",
                        "\n",
                        "    Describing a ``DataFrame``. By default only numeric fields\n",
                        "    are returned.\n",
                        "\n",
                        "    >>> df = pd.DataFrame({'categorical': pd.Categorical(['d', 'e', 'f']),\n",
                        "    ...                    'numeric': [1, 2, 3],\n",
                        "    ...                    'object': ['a', 'b', 'c']\n",
                        "    ...                    })\n",
                        "    >>> df.describe()\n",
                        "           numeric\n",
                        "    count      3.0\n",
                        "    mean       2.0\n",
                        "    std        1.0\n",
                        "    min        1.0\n",
                        "    25%        1.5\n",
                        "    50%        2.0\n",
                        "    75%        2.5\n",
                        "    max        3.0\n",
                        "\n",
                        "    Describing all columns of a ``DataFrame`` regardless of data type.\n",
                        "\n",
                        "    >>> df.describe(include='all')  # doctest: +SKIP\n",
                        "           categorical  numeric object\n",
                        "    count            3      3.0      3\n",
                        "    unique           3      NaN      3\n",
                        "    top              f      NaN      a\n",
                        "    freq             1      NaN      1\n",
                        "    mean           NaN      2.0    NaN\n",
                        "    std            NaN      1.0    NaN\n",
                        "    min            NaN      1.0    NaN\n",
                        "    25%            NaN      1.5    NaN\n",
                        "    50%            NaN      2.0    NaN\n",
                        "    75%            NaN      2.5    NaN\n",
                        "    max            NaN      3.0    NaN\n",
                        "\n",
                        "    Describing a column from a ``DataFrame`` by accessing it as\n",
                        "    an attribute.\n",
                        "\n",
                        "    >>> df.numeric.describe()\n",
                        "    count    3.0\n",
                        "    mean     2.0\n",
                        "    std      1.0\n",
                        "    min      1.0\n",
                        "    25%      1.5\n",
                        "    50%      2.0\n",
                        "    75%      2.5\n",
                        "    max      3.0\n",
                        "    Name: numeric, dtype: float64\n",
                        "\n",
                        "    Including only numeric columns in a ``DataFrame`` description.\n",
                        "\n",
                        "    >>> df.describe(include=[np.number])\n",
                        "           numeric\n",
                        "    count      3.0\n",
                        "    mean       2.0\n",
                        "    std        1.0\n",
                        "    min        1.0\n",
                        "    25%        1.5\n",
                        "    50%        2.0\n",
                        "    75%        2.5\n",
                        "    max        3.0\n",
                        "\n",
                        "    Including only string columns in a ``DataFrame`` description.\n",
                        "\n",
                        "    >>> df.describe(include=[object])  # doctest: +SKIP\n",
                        "           object\n",
                        "    count       3\n",
                        "    unique      3\n",
                        "    top         a\n",
                        "    freq        1\n",
                        "\n",
                        "    Including only categorical columns from a ``DataFrame`` description.\n",
                        "\n",
                        "    >>> df.describe(include=['category'])\n",
                        "           categorical\n",
                        "    count            3\n",
                        "    unique           3\n",
                        "    top              d\n",
                        "    freq             1\n",
                        "\n",
                        "    Excluding numeric columns from a ``DataFrame`` description.\n",
                        "\n",
                        "    >>> df.describe(exclude=[np.number])  # doctest: +SKIP\n",
                        "           categorical object\n",
                        "    count            3      3\n",
                        "    unique           3      3\n",
                        "    top              f      a\n",
                        "    freq             1      1\n",
                        "\n",
                        "    Excluding object columns from a ``DataFrame`` description.\n",
                        "\n",
                        "    >>> df.describe(exclude=[object])  # doctest: +SKIP\n",
                        "           categorical  numeric\n",
                        "    count            3      3.0\n",
                        "    unique           3      NaN\n",
                        "    top              f      NaN\n",
                        "    freq             1      NaN\n",
                        "    mean           NaN      2.0\n",
                        "    std            NaN      1.0\n",
                        "    min            NaN      1.0\n",
                        "    25%            NaN      1.5\n",
                        "    50%            NaN      2.0\n",
                        "    75%            NaN      2.5\n",
                        "    max            NaN      3.0\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "help(df.describe)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "734f911d",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>show_id</th>\n",
                            "      <th>type</th>\n",
                            "      <th>title</th>\n",
                            "      <th>director</th>\n",
                            "      <th>cast</th>\n",
                            "      <th>country</th>\n",
                            "      <th>date_added</th>\n",
                            "      <th>release_year</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>duration</th>\n",
                            "      <th>listed_in</th>\n",
                            "      <th>description</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>s1</td>\n",
                            "      <td>TV Show</td>\n",
                            "      <td>3%</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>João Miguel, Bianca Comparato, Michel Gomes, R...</td>\n",
                            "      <td>Brazil</td>\n",
                            "      <td>August 14, 2020</td>\n",
                            "      <td>2020</td>\n",
                            "      <td>TV-MA</td>\n",
                            "      <td>4 Seasons</td>\n",
                            "      <td>International TV Shows, TV Dramas, TV Sci-Fi &amp;...</td>\n",
                            "      <td>In a future where the elite inhabit an island ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>s2</td>\n",
                            "      <td>Movie</td>\n",
                            "      <td>7:19</td>\n",
                            "      <td>Jorge Michel Grau</td>\n",
                            "      <td>Demián Bichir, Héctor Bonilla, Oscar Serrano, ...</td>\n",
                            "      <td>Mexico</td>\n",
                            "      <td>December 23, 2016</td>\n",
                            "      <td>2016</td>\n",
                            "      <td>TV-MA</td>\n",
                            "      <td>93 min</td>\n",
                            "      <td>Dramas, International Movies</td>\n",
                            "      <td>After a devastating earthquake hits Mexico Cit...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>s3</td>\n",
                            "      <td>Movie</td>\n",
                            "      <td>23:59</td>\n",
                            "      <td>Gilbert Chan</td>\n",
                            "      <td>Tedd Chan, Stella Chung, Henley Hii, Lawrence ...</td>\n",
                            "      <td>Singapore</td>\n",
                            "      <td>December 20, 2018</td>\n",
                            "      <td>2011</td>\n",
                            "      <td>R</td>\n",
                            "      <td>78 min</td>\n",
                            "      <td>Horror Movies, International Movies</td>\n",
                            "      <td>When an army recruit is found dead, his fellow...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>s4</td>\n",
                            "      <td>Movie</td>\n",
                            "      <td>9</td>\n",
                            "      <td>Shane Acker</td>\n",
                            "      <td>Elijah Wood, John C. Reilly, Jennifer Connelly...</td>\n",
                            "      <td>United States</td>\n",
                            "      <td>November 16, 2017</td>\n",
                            "      <td>2009</td>\n",
                            "      <td>PG-13</td>\n",
                            "      <td>80 min</td>\n",
                            "      <td>Action &amp; Adventure, Independent Movies, Sci-Fi...</td>\n",
                            "      <td>In a postapocalyptic world, rag-doll robots hi...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>s5</td>\n",
                            "      <td>Movie</td>\n",
                            "      <td>21</td>\n",
                            "      <td>Robert Luketic</td>\n",
                            "      <td>Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...</td>\n",
                            "      <td>United States</td>\n",
                            "      <td>January 1, 2020</td>\n",
                            "      <td>2008</td>\n",
                            "      <td>PG-13</td>\n",
                            "      <td>123 min</td>\n",
                            "      <td>Dramas</td>\n",
                            "      <td>A brilliant group of students become card-coun...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7782</th>\n",
                            "      <td>s7783</td>\n",
                            "      <td>Movie</td>\n",
                            "      <td>Zozo</td>\n",
                            "      <td>Josef Fares</td>\n",
                            "      <td>Imad Creidi, Antoinette Turk, Elias Gergi, Car...</td>\n",
                            "      <td>Sweden, Czech Republic, United Kingdom, Denmar...</td>\n",
                            "      <td>October 19, 2020</td>\n",
                            "      <td>2005</td>\n",
                            "      <td>TV-MA</td>\n",
                            "      <td>99 min</td>\n",
                            "      <td>Dramas, International Movies</td>\n",
                            "      <td>When Lebanon's Civil War deprives Zozo of his ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7783</th>\n",
                            "      <td>s7784</td>\n",
                            "      <td>Movie</td>\n",
                            "      <td>Zubaan</td>\n",
                            "      <td>Mozez Singh</td>\n",
                            "      <td>Vicky Kaushal, Sarah-Jane Dias, Raaghav Chanan...</td>\n",
                            "      <td>India</td>\n",
                            "      <td>March 2, 2019</td>\n",
                            "      <td>2015</td>\n",
                            "      <td>TV-14</td>\n",
                            "      <td>111 min</td>\n",
                            "      <td>Dramas, International Movies, Music &amp; Musicals</td>\n",
                            "      <td>A scrappy but poor boy worms his way into a ty...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7784</th>\n",
                            "      <td>s7785</td>\n",
                            "      <td>Movie</td>\n",
                            "      <td>Zulu Man in Japan</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>Nasty C</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>September 25, 2020</td>\n",
                            "      <td>2019</td>\n",
                            "      <td>TV-MA</td>\n",
                            "      <td>44 min</td>\n",
                            "      <td>Documentaries, International Movies, Music &amp; M...</td>\n",
                            "      <td>In this documentary, South African rapper Nast...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7785</th>\n",
                            "      <td>s7786</td>\n",
                            "      <td>TV Show</td>\n",
                            "      <td>Zumbo's Just Desserts</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>Adriano Zumbo, Rachel Khoo</td>\n",
                            "      <td>Australia</td>\n",
                            "      <td>October 31, 2020</td>\n",
                            "      <td>2019</td>\n",
                            "      <td>TV-PG</td>\n",
                            "      <td>1 Season</td>\n",
                            "      <td>International TV Shows, Reality TV</td>\n",
                            "      <td>Dessert wizard Adriano Zumbo looks for the nex...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7786</th>\n",
                            "      <td>s7787</td>\n",
                            "      <td>Movie</td>\n",
                            "      <td>ZZ TOP: THAT LITTLE OL' BAND FROM TEXAS</td>\n",
                            "      <td>Sam Dunn</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>United Kingdom, Canada, United States</td>\n",
                            "      <td>March 1, 2020</td>\n",
                            "      <td>2019</td>\n",
                            "      <td>TV-MA</td>\n",
                            "      <td>90 min</td>\n",
                            "      <td>Documentaries, Music &amp; Musicals</td>\n",
                            "      <td>This documentary delves into the mystique behi...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>7787 rows × 12 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "     show_id     type                                    title  \\\n",
                            "0         s1  TV Show                                       3%   \n",
                            "1         s2    Movie                                     7:19   \n",
                            "2         s3    Movie                                    23:59   \n",
                            "3         s4    Movie                                        9   \n",
                            "4         s5    Movie                                       21   \n",
                            "...      ...      ...                                      ...   \n",
                            "7782   s7783    Movie                                     Zozo   \n",
                            "7783   s7784    Movie                                   Zubaan   \n",
                            "7784   s7785    Movie                        Zulu Man in Japan   \n",
                            "7785   s7786  TV Show                    Zumbo's Just Desserts   \n",
                            "7786   s7787    Movie  ZZ TOP: THAT LITTLE OL' BAND FROM TEXAS   \n",
                            "\n",
                            "               director                                               cast  \\\n",
                            "0                   NaN  João Miguel, Bianca Comparato, Michel Gomes, R...   \n",
                            "1     Jorge Michel Grau  Demián Bichir, Héctor Bonilla, Oscar Serrano, ...   \n",
                            "2          Gilbert Chan  Tedd Chan, Stella Chung, Henley Hii, Lawrence ...   \n",
                            "3           Shane Acker  Elijah Wood, John C. Reilly, Jennifer Connelly...   \n",
                            "4        Robert Luketic  Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...   \n",
                            "...                 ...                                                ...   \n",
                            "7782        Josef Fares  Imad Creidi, Antoinette Turk, Elias Gergi, Car...   \n",
                            "7783        Mozez Singh  Vicky Kaushal, Sarah-Jane Dias, Raaghav Chanan...   \n",
                            "7784                NaN                                            Nasty C   \n",
                            "7785                NaN                         Adriano Zumbo, Rachel Khoo   \n",
                            "7786           Sam Dunn                                                NaN   \n",
                            "\n",
                            "                                                country          date_added  \\\n",
                            "0                                                Brazil     August 14, 2020   \n",
                            "1                                                Mexico   December 23, 2016   \n",
                            "2                                             Singapore   December 20, 2018   \n",
                            "3                                         United States   November 16, 2017   \n",
                            "4                                         United States     January 1, 2020   \n",
                            "...                                                 ...                 ...   \n",
                            "7782  Sweden, Czech Republic, United Kingdom, Denmar...    October 19, 2020   \n",
                            "7783                                              India       March 2, 2019   \n",
                            "7784                                                NaN  September 25, 2020   \n",
                            "7785                                          Australia    October 31, 2020   \n",
                            "7786              United Kingdom, Canada, United States       March 1, 2020   \n",
                            "\n",
                            "     release_year rating   duration  \\\n",
                            "0            2020  TV-MA  4 Seasons   \n",
                            "1            2016  TV-MA     93 min   \n",
                            "2            2011      R     78 min   \n",
                            "3            2009  PG-13     80 min   \n",
                            "4            2008  PG-13    123 min   \n",
                            "...           ...    ...        ...   \n",
                            "7782         2005  TV-MA     99 min   \n",
                            "7783         2015  TV-14    111 min   \n",
                            "7784         2019  TV-MA     44 min   \n",
                            "7785         2019  TV-PG   1 Season   \n",
                            "7786         2019  TV-MA     90 min   \n",
                            "\n",
                            "                                              listed_in  \\\n",
                            "0     International TV Shows, TV Dramas, TV Sci-Fi &...   \n",
                            "1                          Dramas, International Movies   \n",
                            "2                   Horror Movies, International Movies   \n",
                            "3     Action & Adventure, Independent Movies, Sci-Fi...   \n",
                            "4                                                Dramas   \n",
                            "...                                                 ...   \n",
                            "7782                       Dramas, International Movies   \n",
                            "7783     Dramas, International Movies, Music & Musicals   \n",
                            "7784  Documentaries, International Movies, Music & M...   \n",
                            "7785                 International TV Shows, Reality TV   \n",
                            "7786                    Documentaries, Music & Musicals   \n",
                            "\n",
                            "                                            description  \n",
                            "0     In a future where the elite inhabit an island ...  \n",
                            "1     After a devastating earthquake hits Mexico Cit...  \n",
                            "2     When an army recruit is found dead, his fellow...  \n",
                            "3     In a postapocalyptic world, rag-doll robots hi...  \n",
                            "4     A brilliant group of students become card-coun...  \n",
                            "...                                                 ...  \n",
                            "7782  When Lebanon's Civil War deprives Zozo of his ...  \n",
                            "7783  A scrappy but poor boy worms his way into a ty...  \n",
                            "7784  In this documentary, South African rapper Nast...  \n",
                            "7785  Dessert wizard Adriano Zumbo looks for the nex...  \n",
                            "7786  This documentary delves into the mystique behi...  \n",
                            "\n",
                            "[7787 rows x 12 columns]"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d746d1c2",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
