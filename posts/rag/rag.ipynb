{
    "cells": [
        {
            "cell_type": "raw",
            "id": "6da18859",
            "metadata": {
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "---\n",
                "title: \"Retrieval-Augmented Generation (RAG) Fundamentals\"\n",
                "author: \"Shivam Miglani\"\n",
                "date: \"2025-12-22\"\n",
                "categories: RAG\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f08dc2f9",
            "metadata": {},
            "source": [
                "::: {.callout-note appearance=\"simple\"}\n",
                "### Licensing Notice\n",
                "Text and media: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)\n",
                "Code and snippets: [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
                ":::"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b81e3ae",
            "metadata": {},
            "source": [
                "## Motivation and Meaning\n",
                "\n",
                "**R**etrieval **A**ugmented **G**eneration (RAG) is inference-time context injection: pull **relevant** external data and condition generation on it. This *grounds* responses in authoritative sources while keeping LLMs parametric knowledge (weights) frozen.\n",
                "\n",
                "#### Motivation\n",
                "- **Knowledge limited to training data**: Your proprietary/domain-specific data isn't there\n",
                "- **Fixed knowledge cut-off date**: The model can't answer questions about recent events. Without RAG, they will either refuse to answer or *hallucinate*.\n",
                "\n",
                "##### Alternatives & Trade-offs\n",
                "- Why not fine-tune?\n",
                "    - Fine-tuning excels at teaching *task formats* (SQL generation, JSON output) and *reasoning styles*, not injecting factual knowledge.\n",
                "    - **Catastrophic forgetting**: Updating knowledge degrades performance on other tasks as model weights get overwritten.\n",
                "    - **Inefficient**: requires separate fine-tuned checkpoints per domain or use-case or document. It is tricky to learn new knowledge without regressing on old knowledge even with LoRA/QLoRA. \n",
                "- Ok, why not just stuff everything in the context window?\n",
                "    - **Recall Degradation**: While 1M+ token models ace \"single-needle\" tests, performance drops significantly (to ~60-70%) when retrieving multiple distributed facts\n",
                "    - **Cost & Latency**: Processing massive contexts is computationally expensive and slow compared to vector search. Retrieval remains necessary for corpora exceeding the window size.\n",
                "- RAG is a reasonable pattern:\n",
                "    - When you need fresh, attributable knowledge with minimal model changes and can tolerate added latency.\n",
                "\n",
                "#### The RAG Pipeline\n",
                "RAG acts as a filter to inject only *relevant* context. A typical production pipeline looks like this:\n",
                "\n",
                "0. **Ingestion & Indexing**: Chunk documents, generate embeddings, and upsert into a vector database. *Note: In production, this is a continuous sync pipeline, not a one-time setup.*\n",
                "1. **Retrieval**: For a user query, search your indexed corpus (vector/keyword) and pull the topâ€‘k relevant chunks, often followed by a *re-ranking* step for precision.\n",
                "2. **Augmented (prompt)**: Inject selected chunks into the system prompt or user message with appropriate metadata (source citations).\n",
                "3. **Generation**: The LLM generates an answer conditioned *strictly* on the provided context, minimizing external knowledge leakage."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6888caad",
            "metadata": {
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "```{mermaid}\n",
                "%%| label: fig-mermaid-ragv1\n",
                "%%| fig-cap: \"RAG pipeline flowchart showing ingestion pipeline, query processing, retrieval, augmentation and LLM generation.\"\n",
                "\n",
                "flowchart TB\n",
                "    n2[\"LLM\"] L_n2_n4_0@-- generates grounded answer --> n4[\"Answer\"]\n",
                "    n3[\"Document Corpus\"] L_n3_n5_0@<-- ingestion pipeline<br>(chunk + embed) --> n5[\"Hybrid index<br>(inverted keywords<br>+ <br>vector embeddings)<br><br>\"]\n",
                "    n5 L_n5_n6_0@-- \"top-k\" --> n6[\"Retrieval &amp; Re-ranking\"]\n",
                "    n6 L_n6_n7_0@-- \"top-k re-ranked chunks + citation metadata\" --> n7[\"Prompt Builder\"]\n",
                "    n7 L_n7_n2_0@-- \"system prompt (use only given context) + user query + <br>top-k re-ranked chunks &amp; citation metadata\" --> n2\n",
                "    n1[\"User Query\"] L_n1_n8_0@--> n8[\"Query processing <br>&amp; embedding\"]\n",
                "    n1 L_n1_n7_0@-- user query --> n7\n",
                "    n8 L_n8_n6_0@-- text + query expansions + embeddings --> n6\n",
                "\n",
                "    n3@{ shape: docs}\n",
                "    n5@{ shape: cyl}\n",
                "    n6@{ shape: rect}\n",
                "    n7@{ shape: rect}\n",
                "    n1@{ shape: rect}\n",
                "    n8@{ shape: rect}\n",
                "\n",
                "    L_n2_n4_0@{ animation: slow } \n",
                "    L_n3_n5_0@{ animation: none } \n",
                "    L_n5_n6_0@{ animation: slow } \n",
                "    L_n6_n7_0@{ animation: slow } \n",
                "    L_n7_n2_0@{ animation: slow } \n",
                "    L_n1_n8_0@{ animation: slow } \n",
                "    L_n1_n7_0@{ animation: slow } \n",
                "    L_n8_n6_0@{ animation: slow }\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d746d1c2",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
