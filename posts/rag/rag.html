<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shivam Miglani">
<meta name="dcterms.date" content="2025-12-22">

<title>Retrieval-Augmented Generation (RAG) Fundamentals – My Context Window</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e97fc0c53e03b0d912b748b5c8c98638.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6503b539e860678ebd7969f11388801c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<style>
#quarto-header nav.navbar > .container-fluid{
  padding-left: .75rem;
  padding-right: .75rem;
  max-width: calc(800px + 1.5rem);
  margin: 0 auto;
}
</style>


</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Context Window</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Retrieval-Augmented Generation (RAG) Fundamentals</h1>
  <div class="quarto-categories">
    <div class="quarto-category">RAG</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Shivam Miglani </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 22, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Licensing Notice
</div>
</div>
<div class="callout-body-container callout-body">
<p>Text and media: <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> Code and snippets: <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></p>
</div>
</div>
<section id="motivation-and-meaning" class="level2">
<h2 class="anchored" data-anchor-id="motivation-and-meaning">Motivation and Meaning</h2>
<p><strong>R</strong>etrieval <strong>A</strong>ugmented <strong>G</strong>eneration (RAG) is inference-time context injection: pull <strong>relevant</strong> external data and condition generation on it. This <em>grounds</em> responses in authoritative sources while keeping LLMs parametric knowledge (weights) frozen.</p>
<section id="motivation" class="level4">
<h4 class="anchored" data-anchor-id="motivation">Motivation</h4>
<ul>
<li><strong>Knowledge limited to training data</strong>: Your proprietary/domain-specific data isn’t there</li>
<li><strong>Fixed knowledge cut-off date</strong>: The model can’t answer questions about recent events. Without RAG, they will either refuse to answer or <em>hallucinate</em>.</li>
</ul>
<section id="alternatives-trade-offs" class="level5">
<h5 class="anchored" data-anchor-id="alternatives-trade-offs">Alternatives &amp; Trade-offs</h5>
<ul>
<li>Why not fine-tune?
<ul>
<li>Fine-tuning excels at teaching <em>task formats</em> (SQL generation, JSON output) and <em>reasoning styles</em>, not injecting factual knowledge.</li>
<li><strong>Catastrophic forgetting</strong>: Updating knowledge degrades performance on other tasks as model weights get overwritten.</li>
<li><strong>Inefficient</strong>: requires separate fine-tuned checkpoints per domain or use-case or document. It is tricky to learn new knowledge without regressing on old knowledge even with LoRA/QLoRA.</li>
</ul></li>
<li>Ok, why not just stuff everything in the context window?
<ul>
<li><strong>Recall Degradation</strong>: While 1M+ token models ace “single-needle” tests, performance drops significantly (to ~60-70%) when retrieving multiple distributed facts</li>
<li><strong>Cost &amp; Latency</strong>: Processing massive contexts is computationally expensive and slow compared to vector search. Retrieval remains necessary for corpora exceeding the window size.</li>
</ul></li>
<li>RAG is a reasonable pattern:
<ul>
<li>When you need fresh, attributable knowledge with minimal model changes and can tolerate added latency.</li>
</ul></li>
</ul>
</section>
</section>
<section id="the-rag-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="the-rag-pipeline">The RAG Pipeline</h4>
<p>RAG acts as a filter to inject only <em>relevant</em> context. A typical production pipeline looks like this:</p>
<ol start="0" type="1">
<li><strong>Ingestion &amp; Indexing</strong>: Chunk documents, generate embeddings, and upsert into a vector database. <em>Note: In production, this is a continuous sync pipeline, not a one-time setup.</em></li>
<li><strong>Retrieval</strong>: For a user query, search your indexed corpus (vector/keyword) and pull the top‑k relevant chunks, often followed by a <em>re-ranking</em> step for precision.</li>
<li><strong>Augmented (prompt)</strong>: Inject selected chunks into the system prompt or user message with appropriate metadata (source citations).</li>
<li><strong>Generation</strong>: The LLM generates an answer conditioned <em>strictly</em> on the provided context, minimizing external knowledge leakage.</li>
</ol>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-mermaid-ragv1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mermaid-ragv1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-mermaid-ragv1">flowchart TB
    n2["LLM"] L_n2_n4_0@-- generates grounded answer --&gt; n4["Answer"]
    n3["Document Corpus"] L_n3_n5_0@&lt;-- ingestion pipeline&lt;br&gt;(chunk + embed) --&gt; n5["Hybrid index&lt;br&gt;(inverted keywords&lt;br&gt;+ &lt;br&gt;vector embeddings)&lt;br&gt;&lt;br&gt;"]
    n5 L_n5_n6_0@-- "top-k" --&gt; n6["Retrieval &amp;amp; Re-ranking"]
    n6 L_n6_n7_0@-- "top-k re-ranked chunks + citation metadata" --&gt; n7["Prompt Builder"]
    n7 L_n7_n2_0@-- "system prompt (use only given context) + user query + &lt;br&gt;top-k re-ranked chunks &amp;amp; citation metadata" --&gt; n2
    n1["User Query"] L_n1_n8_0@--&gt; n8["Query processing &lt;br&gt;&amp;amp; embedding"]
    n1 L_n1_n7_0@-- user query --&gt; n7
    n8 L_n8_n6_0@-- text + query expansions + embeddings --&gt; n6

    n3@{ shape: docs}
    n5@{ shape: cyl}
    n6@{ shape: rect}
    n7@{ shape: rect}
    n1@{ shape: rect}
    n8@{ shape: rect}

    L_n2_n4_0@{ animation: slow } 
    L_n3_n5_0@{ animation: none } 
    L_n5_n6_0@{ animation: slow } 
    L_n6_n7_0@{ animation: slow } 
    L_n7_n2_0@{ animation: slow } 
    L_n1_n8_0@{ animation: slow } 
    L_n1_n7_0@{ animation: slow } 
    L_n8_n6_0@{ animation: slow }
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mermaid-ragv1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: RAG pipeline flowchart showing ingestion pipeline, query processing, retrieval, augmentation and LLM generation.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="4536ff63" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyspark</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> delta <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql <span class="im">import</span> SparkSession</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>builder <span class="op">=</span> (SparkSession.builder</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>           .appName(<span class="st">"LocalDatabricksPrep"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>           .master(<span class="st">"local[*]"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>           .config(<span class="st">"spark.sql.extensions"</span>, <span class="st">"io.delta.sql.DeltaSparkSessionExtension"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>           .config(<span class="st">"spark.sql.catalog.spark_catalog"</span>, <span class="st">"org.apache.spark.sql.delta.catalog.DeltaCatalog"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>           <span class="co"># This downloads the Delta jar automatically:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>           .config(<span class="st">"spark.jars.packages"</span>, <span class="st">"io.delta:delta-spark_2.12:3.2.0"</span>) </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>spark <span class="op">=</span> configure_spark_with_delta_pip(builder).getOrCreate()</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Spark + Delta session created!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Spark + Delta session created!</code></pre>
</div>
</div>
<div id="3f52f383" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyspark.sql.functions <span class="im">as</span> F</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.window <span class="im">import</span> Window</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyspark.sql.types <span class="im">as</span> T</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark <span class="im">import</span> SparkFiles</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv"</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>spark.sparkContext.addFile(url)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Download Raw CSV directly</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> spark.read.option(<span class="st">"header"</span>, <span class="st">"true"</span>).option(<span class="st">"inferSchema"</span>, <span class="st">"true"</span>).csv(<span class="st">"file://"</span><span class="op">+</span> SparkFiles.get(<span class="st">"netflix_titles.csv"</span>))</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>df.show(truncate<span class="op">=</span><span class="va">False</span>, n<span class="op">=</span><span class="dv">5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
|show_id|type   |title|director         |cast                                                                                                                                                                      |country      |date_added       |release_year|rating|duration |listed_in                                               |description                                                                                                                                          |
+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
|s1     |TV Show|3%   |NULL             |João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi|Brazil       |August 14, 2020  |2020        |TV-MA |4 Seasons|International TV Shows, TV Dramas, TV Sci-Fi &amp; Fantasy  |In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.             |
|s2     |Movie  |7:19 |Jorge Michel Grau|Demián Bichir, Héctor Bonilla, Oscar Serrano, Azalia Ortiz, Octavio Michel, Carmen Beato                                                                                  |Mexico       |December 23, 2016|2016        |TV-MA |93 min   |Dramas, International Movies                            |After a devastating earthquake hits Mexico City, trapped survivors from all walks of life wait to be rescued while trying desperately to stay alive. |
|s3     |Movie  |23:59|Gilbert Chan     |Tedd Chan, Stella Chung, Henley Hii, Lawrence Koh, Tommy Kuan, Josh Lai, Mark Lee, Susan Leong, Benjamin Lim                                                              |Singapore    |December 20, 2018|2011        |R     |78 min   |Horror Movies, International Movies                     |When an army recruit is found dead, his fellow soldiers are forced to confront a terrifying secret that's haunting their jungle island training camp.|
|s4     |Movie  |9    |Shane Acker      |Elijah Wood, John C. Reilly, Jennifer Connelly, Christopher Plummer, Crispin Glover, Martin Landau, Fred Tatasciore, Alan Oppenheimer, Tom Kane                           |United States|November 16, 2017|2009        |PG-13 |80 min   |Action &amp; Adventure, Independent Movies, Sci-Fi &amp; Fantasy|In a postapocalyptic world, rag-doll robots hide in fear from dangerous machines out to exterminate them, until a brave newcomer joins the group.    |
|s5     |Movie  |21   |Robert Luketic   |Jim Sturgess, Kevin Spacey, Kate Bosworth, Aaron Yoo, Liza Lapira, Jacob Pitts, Laurence Fishburne, Jack McGee, Josh Gad, Sam Golzari, Helen Carey, Jack Gilpin           |United States|January 1, 2020  |2008        |PG-13 |123 min  |Dramas                                                  |A brilliant group of students become card-counting experts with the intent of swindling millions out of Las Vegas casinos by playing blackjack.      |
+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
only showing top 5 rows</code></pre>
</div>
</div>
<div id="f78c8d83" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df.printSchema()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>root
 |-- show_id: string (nullable = true)
 |-- type: string (nullable = true)
 |-- title: string (nullable = true)
 |-- director: string (nullable = true)
 |-- cast: string (nullable = true)
 |-- country: string (nullable = true)
 |-- date_added: string (nullable = true)
 |-- release_year: string (nullable = true)
 |-- rating: string (nullable = true)
 |-- duration: string (nullable = true)
 |-- listed_in: string (nullable = true)
 |-- description: string (nullable = true)
</code></pre>
</div>
</div>
<div id="ed3f5d68" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The Task:</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># From netflix_bronze, select title, release_year, and date_added.</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean date_added (currently string like "September 25, 2021") into a real date.</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate days_diff = date_added - release_year (assume Jan 1st).</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>sub_df <span class="op">=</span> df.select(</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    F.col(<span class="st">"title"</span>),</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    F.col(<span class="st">"release_year"</span>),</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    F.col(<span class="st">"date_added"</span>),</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>sub_df <span class="op">=</span> sub_df.withColumn(<span class="st">"date_added_clean"</span>, F.to_date(F.col(<span class="st">"date_added"</span>), <span class="st">"MMMM d, yyyy"</span>))</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>sub_df <span class="op">=</span> sub_df.withColumn(<span class="st">"release_year_clean"</span>, F.to_date(F.col(<span class="st">"release_year"</span>), <span class="st">"y"</span>))</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 1 jan of release year</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>sub_df <span class="op">=</span> sub_df.withColumn(<span class="st">"days_diff"</span>, F.date_diff(F.col(<span class="st">"date_added_clean"</span>), F.col(<span class="st">"release_year_clean"</span>))) <span class="co"># default is somehow days</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>sub_df.show(n<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-----+------------+-----------------+----------------+------------------+---------+
|title|release_year|       date_added|date_added_clean|release_year_clean|days_diff|
+-----+------------+-----------------+----------------+------------------+---------+
|   3%|        2020|  August 14, 2020|      2020-08-14|        2020-01-01|      226|
| 7:19|        2016|December 23, 2016|      2016-12-23|        2016-01-01|      357|
|23:59|        2011|December 20, 2018|      2018-12-20|        2011-01-01|     2910|
|    9|        2009|November 16, 2017|      2017-11-16|        2009-01-01|     3241|
|   21|        2008|  January 1, 2020|      2020-01-01|        2008-01-01|     4383|
+-----+------------+-----------------+----------------+------------------+---------+
only showing top 5 rows</code></pre>
</div>
</div>
<div id="4d2bf442" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a simple list of movies for "Brad Pitt".</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>df.<span class="bu">filter</span>(F.lower(F.col(<span class="st">"cast"</span>)).contains(<span class="st">"brad pitt"</span>)).select(F.col(<span class="st">"title"</span>)).show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------------------+
|               title|
+--------------------+
|A Stoning in Fulh...|
|               Babel|
|          By the Sea|
|Inglourious Basterds|
| Killing Them Softly|
|    Ocean's Thirteen|
|      Ocean's Twelve|
|         War Machine|
+--------------------+
</code></pre>
</div>
</div>
<div id="f5d6cb74" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># "Who are the top 5 most frequent actors in the dataset?"</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>temp_df <span class="op">=</span> df.withColumn(<span class="st">"cast_split"</span>, F.split(F.col(<span class="st">"cast"</span>), <span class="st">","</span>)).withColumn(<span class="st">"cast_exploded"</span>, F.explode(F.col(<span class="st">"cast_split"</span>)))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>temp_df.groupBy(F.col(<span class="st">"cast_exploded"</span>)).agg({<span class="st">"cast_exploded"</span>: <span class="st">'count'</span>}).select(<span class="st">"cast_exploded"</span>, F.col(<span class="st">"count(cast_exploded)"</span>).alias(<span class="st">"count"</span>)).orderBy(F.col(<span class="st">"count"</span>).desc()).show(n<span class="op">=</span><span class="dv">5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-----------------+-----+
|    cast_exploded|count|
+-----------------+-----+
|      Anupam Kher|   38|
| Takahiro Sakurai|   28|
|          Om Puri|   27|
|   Shah Rukh Khan|   27|
|      Boman Irani|   25|
+-----------------+-----+
only showing top 5 rows</code></pre>
</div>
</div>
<div id="db0c5cb8" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>temp_df.groupBy(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    F.col(<span class="st">"cast_exploded"</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>).agg(</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    F.count(<span class="st">"*"</span>).alias(<span class="st">"count"</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>).orderBy(F.col(<span class="st">"count"</span>).desc()).show(<span class="dv">5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-----------------+-----+
|    cast_exploded|count|
+-----------------+-----+
|      Anupam Kher|   38|
| Takahiro Sakurai|   28|
|          Om Puri|   27|
|   Shah Rukh Khan|   27|
|      Boman Irani|   25|
+-----------------+-----+
only showing top 5 rows</code></pre>
</div>
</div>
<div id="f8acd96f" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>temp_df <span class="op">=</span> df.withColumn(<span class="st">"cast_split"</span>, F.split(F.col(<span class="st">"cast"</span>), <span class="st">","</span>)).withColumn(<span class="st">"cast_split_clean"</span>, F.transform(F.col(<span class="st">"cast_split"</span>), <span class="kw">lambda</span> x: F.trim(x)))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>temp_df.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+
|show_id|   type| title|            director|                cast|             country|       date_added|release_year|rating| duration|           listed_in|         description|          cast_split|    cast_split_clean|
+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+
|     s1|TV Show|    3%|                NULL|João Miguel, Bian...|              Brazil|  August 14, 2020|        2020| TV-MA|4 Seasons|International TV ...|In a future where...|[João Miguel,  Bi...|[João Miguel, Bia...|
|     s2|  Movie|  7:19|   Jorge Michel Grau|Demián Bichir, Hé...|              Mexico|December 23, 2016|        2016| TV-MA|   93 min|Dramas, Internati...|After a devastati...|[Demián Bichir,  ...|[Demián Bichir, H...|
|     s3|  Movie| 23:59|        Gilbert Chan|Tedd Chan, Stella...|           Singapore|December 20, 2018|        2011|     R|   78 min|Horror Movies, In...|When an army recr...|[Tedd Chan,  Stel...|[Tedd Chan, Stell...|
|     s4|  Movie|     9|         Shane Acker|Elijah Wood, John...|       United States|November 16, 2017|        2009| PG-13|   80 min|Action &amp; Adventur...|In a postapocalyp...|[Elijah Wood,  Jo...|[Elijah Wood, Joh...|
|     s5|  Movie|    21|      Robert Luketic|Jim Sturgess, Kev...|       United States|  January 1, 2020|        2008| PG-13|  123 min|              Dramas|A brilliant group...|[Jim Sturgess,  K...|[Jim Sturgess, Ke...|
|     s6|TV Show|    46|         Serdar Akar|Erdal Beşikçioğlu...|              Turkey|     July 1, 2017|        2016| TV-MA| 1 Season|International TV ...|A genetics profes...|[Erdal Beşikçioğl...|[Erdal Beşikçioğl...|
|     s7|  Movie|   122|     Yasir Al Yasiri|Amina Khalil, Ahm...|               Egypt|     June 1, 2020|        2019| TV-MA|   95 min|Horror Movies, In...|After an awful ac...|[Amina Khalil,  A...|[Amina Khalil, Ah...|
|     s8|  Movie|   187|      Kevin Reynolds|Samuel L. Jackson...|       United States| November 1, 2019|        1997|     R|  119 min|              Dramas|After one of his ...|[Samuel L. Jackso...|[Samuel L. Jackso...|
|     s9|  Movie|   706|       Shravan Kumar|Divya Dutta, Atul...|               India|    April 1, 2019|        2019| TV-14|  118 min|Horror Movies, In...|When a doctor goe...|[Divya Dutta,  At...|[Divya Dutta, Atu...|
|    s10|  Movie|  1920|        Vikram Bhatt|Rajneesh Duggal, ...|               India|December 15, 2017|        2008| TV-MA|  143 min|Horror Movies, In...|An architect and ...|[Rajneesh Duggal,...|[Rajneesh Duggal,...|
|    s11|  Movie|  1922|        Zak Hilditch|Thomas Jane, Moll...|       United States| October 20, 2017|        2017| TV-MA|  103 min|   Dramas, Thrillers|A farmer pens a c...|[Thomas Jane,  Mo...|[Thomas Jane, Mol...|
|    s12|TV Show|  1983|                NULL|Robert Więckiewic...|Poland, United St...|November 30, 2018|        2018| TV-MA| 1 Season|Crime TV Shows, I...|In this dark alt-...|[Robert Więckiewi...|[Robert Więckiewi...|
|    s13|TV Show|  1994|Diego Enrique Osorno|                NULL|              Mexico|     May 17, 2019|        2019| TV-MA| 1 Season|Crime TV Shows, D...|Archival video an...|                NULL|                NULL|
|    s14|  Movie| 2,215| Nottapon Boonprakob|  Artiwara Kongmalai|            Thailand|    March 1, 2019|        2018| TV-MA|   89 min|Documentaries, In...|This intimate doc...|[Artiwara Kongmalai]|[Artiwara Kongmalai]|
|    s15|  Movie|  3022|          John Suits|Omar Epps, Kate W...|       United States|   March 19, 2020|        2019|     R|   91 min|Independent Movie...|Stranded when the...|[Omar Epps,  Kate...|[Omar Epps, Kate ...|
|    s16|  Movie|Oct-01|      Kunle Afolayan|Sadiq Daba, David...|             Nigeria|September 1, 2019|        2014| TV-14|  149 min|Dramas, Internati...|Against the backd...|[Sadiq Daba,  Dav...|[Sadiq Daba, Davi...|
|    s17|TV Show|Feb-09|                NULL|Shahd El Yaseen, ...|                NULL|   March 20, 2019|        2018| TV-14| 1 Season|International TV ...|As a psychology p...|[Shahd El Yaseen,...|[Shahd El Yaseen,...|
|    s18|  Movie|22-Jul|     Paul Greengrass|Anders Danielsen ...|Norway, Iceland, ...| October 10, 2018|        2018|     R|  144 min|   Dramas, Thrillers|After devastating...|[Anders Danielsen...|[Anders Danielsen...|
|    s19|  Movie|15-Aug|  Swapnaneel Jayakar|Rahul Pethe, Mrun...|               India|   March 29, 2019|        2019| TV-14|  124 min|Comedies, Dramas,...|On India's Indepe...|[Rahul Pethe,  Mr...|[Rahul Pethe, Mru...|
|    s20|  Movie|   '89|                NULL|Lee Dixon, Ian Wr...|      United Kingdom|     May 16, 2018|        2017| TV-PG|   87 min|       Sports Movies|Mixing old footag...|[Lee Dixon,  Ian ...|[Lee Dixon, Ian W...|
+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+
only showing top 20 rows</code></pre>
</div>
</div>
<div id="60d14329" class="cell" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># “For the top 10 actors by movie count, what is the average gap in years between their consecutive movies?”</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>exploded_temp_df <span class="op">=</span> temp_df.withColumn(<span class="st">"actor"</span>, F.explode(F.col(<span class="st">"cast_split_clean"</span>))).<span class="bu">filter</span>(F.col(<span class="st">"actor"</span>) <span class="op">!=</span> <span class="st">""</span>).select(<span class="st">"actor"</span>, <span class="st">"title"</span>, <span class="st">"release_year"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># exploded_temp_df.show()</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>actor_counts <span class="op">=</span> exploded_temp_df.groupBy(F.col(<span class="st">"actor"</span>)).agg(F.count(<span class="st">"*"</span>).alias(<span class="st">"count"</span>)).orderBy(F.col(<span class="st">"count"</span>).desc())</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># actor_counts.show()</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>top10 <span class="op">=</span> actor_counts.limit(<span class="dv">10</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co"># join back on original table</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>top10_actors <span class="op">=</span> exploded_temp_df.join(F.broadcast(top10), on<span class="op">=</span><span class="st">"actor"</span>, how<span class="op">=</span><span class="st">"inner"</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co"># top10_actors.show()</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co"># window func.</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> Window.partitionBy(<span class="st">"actor"</span>).orderBy(<span class="st">"release_year"</span>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Compute previous movie year using lag, then gap</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>gaps_df <span class="op">=</span> (top10_actors</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    .withColumn(<span class="st">"prev_year"</span>, F.lag(<span class="st">"release_year"</span>).over(w))</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">filter</span>(F.col(<span class="st">"prev_year"</span>).isNotNull())</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    .withColumn(<span class="st">"gap_years"</span>, F.datediff(F.col(<span class="st">"release_year"</span>), F.col(<span class="st">"prev_year"</span>))<span class="op">/</span>F.lit(<span class="fl">365.0</span>))</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>gaps_df.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------------+--------------------+------------+-----+---------+------------------+
|       actor|               title|release_year|count|prev_year|         gap_years|
+------------+--------------------+------------+-----+---------+------------------+
|Akshay Kumar|Mujhse Shaadi Karogi|        2004|   29|     2004|               0.0|
|Akshay Kumar|             Bewafaa|        2005|   29|     2004|1.0027397260273974|
|Akshay Kumar|               Insan|        2005|   29|     2005|               0.0|
|Akshay Kumar|         Bhagam Bhag|        2006|   29|     2005|               1.0|
|Akshay Kumar|Humko Deewana Kar...|        2006|   29|     2006|               0.0|
|Akshay Kumar|Jaan-E-Mann: Let'...|        2006|   29|     2006|               0.0|
|Akshay Kumar|     Phir Hera Pheri|        2006|   29|     2006|               0.0|
|Akshay Kumar|     Bhool Bhulaiyaa|        2007|   29|     2006|               1.0|
|Akshay Kumar|     Namastey London|        2007|   29|     2007|               0.0|
|Akshay Kumar|             Welcome|        2007|   29|     2007|               0.0|
|Akshay Kumar|      Action Replayy|        2010|   29|     2007|3.0027397260273974|
|Akshay Kumar|      Tees Maar Khan|        2010|   29|     2010|               0.0|
|Akshay Kumar|       Patiala House|        2011|   29|     2010|               1.0|
|Akshay Kumar|           Thank You|        2011|   29|     2011|               0.0|
|Akshay Kumar|               Joker|        2012|   29|     2011|               1.0|
|Akshay Kumar|           Oh My God|        2012|   29|     2012|               0.0|
|Akshay Kumar|       Rowdy Rathore|        2012|   29|     2012|               0.0|
|Akshay Kumar|                Boss|        2013|   29|     2012|1.0027397260273974|
|Akshay Kumar|Once Upon a Time ...|        2013|   29|     2013|               0.0|
|Akshay Kumar|          Special 26|        2013|   29|     2013|               0.0|
+------------+--------------------+------------+-----+---------+------------------+
only showing top 20 rows</code></pre>
</div>
</div>
<div id="f623a1d3" class="cell" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>gaps_df.groupBy(<span class="st">"actor"</span>).agg(F.avg(F.col(<span class="st">"gap_years"</span>)).alias(<span class="st">"avg_gap_year"</span>)).orderBy(F.col(<span class="st">"avg_gap_year"</span>).asc()).show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>+----------------+-------------------+
|           actor|       avg_gap_year|
+----------------+-------------------+
|       Yuki Kaji|0.38482613277133826|
|Takahiro Sakurai| 0.4288649706457926|
|    Akshay Kumar| 0.5361056751467711|
|     Boman Irani| 0.6158061116965227|
|     Anupam Kher| 0.7077848312729702|
|  Shah Rukh Khan| 0.7946817082997581|
|    Paresh Rawal|  1.116122233930453|
|         Om Puri|  1.207746811525744|
|Naseeruddin Shah| 1.2422295701464334|
|Amitabh Bachchan| 1.6934668071654373|
+----------------+-------------------+
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>25/12/23 16:39:25 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 956198 ms exceeds timeout 120000 ms
25/12/23 16:39:25 WARN SparkContext: Killing executors is not supported by current scheduler.
25/12/23 16:39:26 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:26 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:36 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:36 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:46 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:46 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:56 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:56 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:23 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:23 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:33 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:33 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:43 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:43 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:53 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:53 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:55:03 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:55:03 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:17 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:17 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:27 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:27 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:37 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:37 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:47 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:47 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:57 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:57 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:18 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:18 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:28 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:28 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:38 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:38 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:48 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:48 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:58 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:58 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:42:57 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:42:57 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:07 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:07 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:17 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:17 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:27 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:27 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:37 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:37 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:28 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:28 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:38 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:38 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:48 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:48 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:58 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:58 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:56:08 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:56:08 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:12:53 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:12:53 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:03 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:03 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:13 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:13 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:23 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:23 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:28 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:28 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:38 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:38 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:48 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:48 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:58 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:58 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:08 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:08 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:18 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:18 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:28 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:28 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:04 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:04 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:14 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:14 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:24 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:24 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:34 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:34 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:44 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:44 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:32 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:32 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:42 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:42 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:52 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:52 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:57:02 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:57:02 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:17 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:17 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:27 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:27 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:37 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:37 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:47 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:47 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:57 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:57 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:28:57 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:28:57 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:07 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:07 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:17 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:17 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:27 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:27 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:37 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:37 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:47 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:47 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:47 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times</code></pre>
</div>
</div>
<div id="950e0541" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>df.info()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 7787 entries, 0 to 7786
Data columns (total 12 columns):
 #   Column        Non-Null Count  Dtype 
---  ------        --------------  ----- 
 0   show_id       7787 non-null   object
 1   type          7787 non-null   object
 2   title         7787 non-null   object
 3   director      5398 non-null   object
 4   cast          7069 non-null   object
 5   country       7280 non-null   object
 6   date_added    7777 non-null   object
 7   release_year  7787 non-null   int64 
 8   rating        7780 non-null   object
 9   duration      7787 non-null   object
 10  listed_in     7787 non-null   object
 11  description   7787 non-null   object
dtypes: int64(1), object(11)
memory usage: 730.2+ KB</code></pre>
</div>
</div>
<div id="9823034e" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'release_year'</span>] <span class="op">=</span> df[<span class="st">'release_year'</span>].astype(<span class="st">'category'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c82f4880" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">help</span>(df.describe)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Help on method describe in module pandas.core.generic:

describe(percentiles=None, include=None, exclude=None) -&gt; 'Self' method of pandas.core.frame.DataFrame instance
    Generate descriptive statistics.

    Descriptive statistics include those that summarize the central
    tendency, dispersion and shape of a
    dataset's distribution, excluding ``NaN`` values.

    Analyzes both numeric and object series, as well
    as ``DataFrame`` column sets of mixed data types. The output
    will vary depending on what is provided. Refer to the notes
    below for more detail.

    Parameters
    ----------
    percentiles : list-like of numbers, optional
        The percentiles to include in the output. All should
        fall between 0 and 1. The default is
        ``[.25, .5, .75]``, which returns the 25th, 50th, and
        75th percentiles.
    include : 'all', list-like of dtypes or None (default), optional
        A white list of data types to include in the result. Ignored
        for ``Series``. Here are the options:

        - 'all' : All columns of the input will be included in the output.
        - A list-like of dtypes : Limits the results to the
          provided data types.
          To limit the result to numeric types submit
          ``numpy.number``. To limit it instead to object columns submit
          the ``numpy.object`` data type. Strings
          can also be used in the style of
          ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To
          select pandas categorical columns, use ``'category'``
        - None (default) : The result will include all numeric columns.
    exclude : list-like of dtypes or None (default), optional,
        A black list of data types to omit from the result. Ignored
        for ``Series``. Here are the options:

        - A list-like of dtypes : Excludes the provided data types
          from the result. To exclude numeric types submit
          ``numpy.number``. To exclude object columns submit the data
          type ``numpy.object``. Strings can also be used in the style of
          ``select_dtypes`` (e.g. ``df.describe(exclude=['O'])``). To
          exclude pandas categorical columns, use ``'category'``
        - None (default) : The result will exclude nothing.

    Returns
    -------
    Series or DataFrame
        Summary statistics of the Series or Dataframe provided.

    See Also
    --------
    DataFrame.count: Count number of non-NA/null observations.
    DataFrame.max: Maximum of the values in the object.
    DataFrame.min: Minimum of the values in the object.
    DataFrame.mean: Mean of the values.
    DataFrame.std: Standard deviation of the observations.
    DataFrame.select_dtypes: Subset of a DataFrame including/excluding
        columns based on their dtype.

    Notes
    -----
    For numeric data, the result's index will include ``count``,
    ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and
    upper percentiles. By default the lower percentile is ``25`` and the
    upper percentile is ``75``. The ``50`` percentile is the
    same as the median.

    For object data (e.g. strings or timestamps), the result's index
    will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``
    is the most common value. The ``freq`` is the most common value's
    frequency. Timestamps also include the ``first`` and ``last`` items.

    If multiple object values have the highest count, then the
    ``count`` and ``top`` results will be arbitrarily chosen from
    among those with the highest count.

    For mixed data types provided via a ``DataFrame``, the default is to
    return only an analysis of numeric columns. If the dataframe consists
    only of object and categorical data without any numeric columns, the
    default is to return an analysis of both the object and categorical
    columns. If ``include='all'`` is provided as an option, the result
    will include a union of attributes of each type.

    The `include` and `exclude` parameters can be used to limit
    which columns in a ``DataFrame`` are analyzed for the output.
    The parameters are ignored when analyzing a ``Series``.

    Examples
    --------
    Describing a numeric ``Series``.

    &gt;&gt;&gt; s = pd.Series([1, 2, 3])
    &gt;&gt;&gt; s.describe()
    count    3.0
    mean     2.0
    std      1.0
    min      1.0
    25%      1.5
    50%      2.0
    75%      2.5
    max      3.0
    dtype: float64

    Describing a categorical ``Series``.

    &gt;&gt;&gt; s = pd.Series(['a', 'a', 'b', 'c'])
    &gt;&gt;&gt; s.describe()
    count     4
    unique    3
    top       a
    freq      2
    dtype: object

    Describing a timestamp ``Series``.

    &gt;&gt;&gt; s = pd.Series([
    ...     np.datetime64("2000-01-01"),
    ...     np.datetime64("2010-01-01"),
    ...     np.datetime64("2010-01-01")
    ... ])
    &gt;&gt;&gt; s.describe()
    count                      3
    mean     2006-09-01 08:00:00
    min      2000-01-01 00:00:00
    25%      2004-12-31 12:00:00
    50%      2010-01-01 00:00:00
    75%      2010-01-01 00:00:00
    max      2010-01-01 00:00:00
    dtype: object

    Describing a ``DataFrame``. By default only numeric fields
    are returned.

    &gt;&gt;&gt; df = pd.DataFrame({'categorical': pd.Categorical(['d', 'e', 'f']),
    ...                    'numeric': [1, 2, 3],
    ...                    'object': ['a', 'b', 'c']
    ...                    })
    &gt;&gt;&gt; df.describe()
           numeric
    count      3.0
    mean       2.0
    std        1.0
    min        1.0
    25%        1.5
    50%        2.0
    75%        2.5
    max        3.0

    Describing all columns of a ``DataFrame`` regardless of data type.

    &gt;&gt;&gt; df.describe(include='all')  # doctest: +SKIP
           categorical  numeric object
    count            3      3.0      3
    unique           3      NaN      3
    top              f      NaN      a
    freq             1      NaN      1
    mean           NaN      2.0    NaN
    std            NaN      1.0    NaN
    min            NaN      1.0    NaN
    25%            NaN      1.5    NaN
    50%            NaN      2.0    NaN
    75%            NaN      2.5    NaN
    max            NaN      3.0    NaN

    Describing a column from a ``DataFrame`` by accessing it as
    an attribute.

    &gt;&gt;&gt; df.numeric.describe()
    count    3.0
    mean     2.0
    std      1.0
    min      1.0
    25%      1.5
    50%      2.0
    75%      2.5
    max      3.0
    Name: numeric, dtype: float64

    Including only numeric columns in a ``DataFrame`` description.

    &gt;&gt;&gt; df.describe(include=[np.number])
           numeric
    count      3.0
    mean       2.0
    std        1.0
    min        1.0
    25%        1.5
    50%        2.0
    75%        2.5
    max        3.0

    Including only string columns in a ``DataFrame`` description.

    &gt;&gt;&gt; df.describe(include=[object])  # doctest: +SKIP
           object
    count       3
    unique      3
    top         a
    freq        1

    Including only categorical columns from a ``DataFrame`` description.

    &gt;&gt;&gt; df.describe(include=['category'])
           categorical
    count            3
    unique           3
    top              d
    freq             1

    Excluding numeric columns from a ``DataFrame`` description.

    &gt;&gt;&gt; df.describe(exclude=[np.number])  # doctest: +SKIP
           categorical object
    count            3      3
    unique           3      3
    top              f      a
    freq             1      1

    Excluding object columns from a ``DataFrame`` description.

    &gt;&gt;&gt; df.describe(exclude=[object])  # doctest: +SKIP
           categorical  numeric
    count            3      3.0
    unique           3      NaN
    top              f      NaN
    freq             1      NaN
    mean           NaN      2.0
    std            NaN      1.0
    min            NaN      1.0
    25%            NaN      1.5
    50%            NaN      2.0
    75%            NaN      2.5
    max            NaN      3.0
</code></pre>
</div>
</div>
<div id="734f911d" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">show_id</th>
<th data-quarto-table-cell-role="th">type</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">director</th>
<th data-quarto-table-cell-role="th">cast</th>
<th data-quarto-table-cell-role="th">country</th>
<th data-quarto-table-cell-role="th">date_added</th>
<th data-quarto-table-cell-role="th">release_year</th>
<th data-quarto-table-cell-role="th">rating</th>
<th data-quarto-table-cell-role="th">duration</th>
<th data-quarto-table-cell-role="th">listed_in</th>
<th data-quarto-table-cell-role="th">description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>s1</td>
<td>TV Show</td>
<td>3%</td>
<td>NaN</td>
<td>João Miguel, Bianca Comparato, Michel Gomes, R...</td>
<td>Brazil</td>
<td>August 14, 2020</td>
<td>2020</td>
<td>TV-MA</td>
<td>4 Seasons</td>
<td>International TV Shows, TV Dramas, TV Sci-Fi &amp;...</td>
<td>In a future where the elite inhabit an island ...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>s2</td>
<td>Movie</td>
<td>7:19</td>
<td>Jorge Michel Grau</td>
<td>Demián Bichir, Héctor Bonilla, Oscar Serrano, ...</td>
<td>Mexico</td>
<td>December 23, 2016</td>
<td>2016</td>
<td>TV-MA</td>
<td>93 min</td>
<td>Dramas, International Movies</td>
<td>After a devastating earthquake hits Mexico Cit...</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>s3</td>
<td>Movie</td>
<td>23:59</td>
<td>Gilbert Chan</td>
<td>Tedd Chan, Stella Chung, Henley Hii, Lawrence ...</td>
<td>Singapore</td>
<td>December 20, 2018</td>
<td>2011</td>
<td>R</td>
<td>78 min</td>
<td>Horror Movies, International Movies</td>
<td>When an army recruit is found dead, his fellow...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>s4</td>
<td>Movie</td>
<td>9</td>
<td>Shane Acker</td>
<td>Elijah Wood, John C. Reilly, Jennifer Connelly...</td>
<td>United States</td>
<td>November 16, 2017</td>
<td>2009</td>
<td>PG-13</td>
<td>80 min</td>
<td>Action &amp; Adventure, Independent Movies, Sci-Fi...</td>
<td>In a postapocalyptic world, rag-doll robots hi...</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>s5</td>
<td>Movie</td>
<td>21</td>
<td>Robert Luketic</td>
<td>Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...</td>
<td>United States</td>
<td>January 1, 2020</td>
<td>2008</td>
<td>PG-13</td>
<td>123 min</td>
<td>Dramas</td>
<td>A brilliant group of students become card-coun...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">...</th>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">7782</th>
<td>s7783</td>
<td>Movie</td>
<td>Zozo</td>
<td>Josef Fares</td>
<td>Imad Creidi, Antoinette Turk, Elias Gergi, Car...</td>
<td>Sweden, Czech Republic, United Kingdom, Denmar...</td>
<td>October 19, 2020</td>
<td>2005</td>
<td>TV-MA</td>
<td>99 min</td>
<td>Dramas, International Movies</td>
<td>When Lebanon's Civil War deprives Zozo of his ...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">7783</th>
<td>s7784</td>
<td>Movie</td>
<td>Zubaan</td>
<td>Mozez Singh</td>
<td>Vicky Kaushal, Sarah-Jane Dias, Raaghav Chanan...</td>
<td>India</td>
<td>March 2, 2019</td>
<td>2015</td>
<td>TV-14</td>
<td>111 min</td>
<td>Dramas, International Movies, Music &amp; Musicals</td>
<td>A scrappy but poor boy worms his way into a ty...</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">7784</th>
<td>s7785</td>
<td>Movie</td>
<td>Zulu Man in Japan</td>
<td>NaN</td>
<td>Nasty C</td>
<td>NaN</td>
<td>September 25, 2020</td>
<td>2019</td>
<td>TV-MA</td>
<td>44 min</td>
<td>Documentaries, International Movies, Music &amp; M...</td>
<td>In this documentary, South African rapper Nast...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">7785</th>
<td>s7786</td>
<td>TV Show</td>
<td>Zumbo's Just Desserts</td>
<td>NaN</td>
<td>Adriano Zumbo, Rachel Khoo</td>
<td>Australia</td>
<td>October 31, 2020</td>
<td>2019</td>
<td>TV-PG</td>
<td>1 Season</td>
<td>International TV Shows, Reality TV</td>
<td>Dessert wizard Adriano Zumbo looks for the nex...</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">7786</th>
<td>s7787</td>
<td>Movie</td>
<td>ZZ TOP: THAT LITTLE OL' BAND FROM TEXAS</td>
<td>Sam Dunn</td>
<td>NaN</td>
<td>United Kingdom, Canada, United States</td>
<td>March 1, 2020</td>
<td>2019</td>
<td>TV-MA</td>
<td>90 min</td>
<td>Documentaries, Music &amp; Musicals</td>
<td>This documentary delves into the mystique behi...</td>
</tr>
</tbody>
</table>

<p>7787 rows × 12 columns</p>
</div>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script>
    document.addEventListener("DOMContentLoaded", function () {
        if (window.location.pathname.includes("/posts/")) {
            var repoUrl = "https://github.com/shivam-miglani/genai-blog/blob/main";
            var path = window.location.pathname;

            // Remove deployment path prefix (e.g. /genai-blog/) if present
            // We assume the structure is .../posts/...
            var parts = path.split("/posts/");
            if (parts.length > 1) {
                var relativePath = parts[1];

                if (relativePath.endsWith("/") || relativePath === "") {
                    relativePath += "index.qmd";
                } else if (relativePath.endsWith(".html")) {
                    relativePath = relativePath.replace(".html", ".ipynb");
                }

                var gitHubUrl = repoUrl + "/posts/" + relativePath;

                var footer = document.createElement("div");
                footer.style.marginTop = "3rem";
                footer.style.paddingTop = "1rem";
                footer.style.borderTop = "1px solid #e9ecef";
                footer.style.textAlign = "center";
                footer.style.fontSize = "0.8em";
                footer.innerHTML = "<a href='" + gitHubUrl + "' target='_blank' style='color: inherit; text-decoration: none; font-weight: bold;'>View source on <i class='bi bi-github'></i></a>";

                var main = document.querySelector("main");
                if (main) {
                    main.appendChild(footer);
                }
            }
        }
    });
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/shivam-miglani\.github\.io\/genai-blog\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="../../about.html">About</a> · © 2025 Shivam Miglani</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>