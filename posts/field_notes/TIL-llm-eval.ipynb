{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947685b9",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"TIL from Stanford CME295 Transformers & LLMs | Lecture 8 - LLM Evaluation\"\n",
    "date: \"2025-12-26\"\n",
    "categories: \n",
    "    - LLM Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e859f",
   "metadata": {},
   "source": [
    "Output quality\n",
    "- instruction following\n",
    "- coherence\n",
    "- factuality\n",
    "\n",
    "\n",
    "System performance\n",
    "- latency\n",
    "- cost\n",
    "- reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af0c14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "- human ratings are gold standard. \n",
    "    - Subjectivity of human ratings -> agreement rate = $p_{o}$ -> o stands for observed\n",
    "        - \"How much better is our agreement than what weâ€™d expect just by\n",
    "    chance, given how the raters actually use the categories?\" \"A coefficient of agreement for nominal scales\", Cohen, 1960, \"Measuring nominal scale agreement among many raters\", Fleiss, 1971.\n",
    "        - Variants. Cohen's Kappa, Fleiss' Kappa, Krippendorff's alpha\n",
    "\n",
    "    - human ratings are slow and expensive\n",
    "- rule-based metrics\n",
    "    - METEOR. Metric for Evaluation of Translation with Explicit ORdering\n",
    "        - write labels once, use as reference\n",
    "    - BLEU (BiLingual Evaluation Understudy)\n",
    "    - ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "        - Many variants (ROUGE-N, ROUGE-L, etc.) with suite of metrics\n",
    "    - Problem with rule-based: \n",
    "        - Does not take into account stylistic variations\n",
    "        - Correlation with human rating is not that great\n",
    "        - still requires human ratings\n",
    "\n",
    "- LLM-as-a-Judge\n",
    "    -  Use LLM to rate quality of response.\n",
    "    - <prompt, model-response, criteria> -> <score (pass/fail), reason>\n",
    "    - Structured Outputs (in practice)\n",
    "    - Benefit: No need for reference / label; Interpretability via rationales\n",
    "    - Variations: Point-wise or Pair-wise (A or B is better)\n",
    "        - Position A - position bias    \n",
    "            - Remedy. \"Take the average\", or tweak position embeddings\n",
    "        - Verbosity bias\n",
    "            - Remedy. Explicit guidelines, few-shot, and/or penalty on output length\n",
    "        - Self-enhancement bias\n",
    "            - Remedy, Don't use agent as judge\n",
    "\n",
    "    - Best pracitices\n",
    "        - crisp guidelines\n",
    "        - binary scale over granular ones\n",
    "        - write rationaile before outputting score (autoregressive)\n",
    "        - mitigate biases (position, verbosity, self-enhancement)\n",
    "        - calibrate with human judgements\n",
    "        - low temperature for reproducibility\n",
    "\n",
    "\n",
    "- Typical things we want to evaluate for \n",
    "    - task performance\n",
    "        - Usefulness\n",
    "        - Factuality\n",
    "        - Relevance\n",
    "    - alignment\n",
    "        - tone\n",
    "        - style\n",
    "        - safety\n",
    "\n",
    "- Factuality\n",
    "    - quantify factuality - president roosevelt and teddy example\n",
    "    - decompose into various facts and do weighted average\n",
    "\n",
    "- ReAct\n",
    "    - Tool prediction error \n",
    "        - LLM directly issues reposnse -> tool router error -> retrain tool router\n",
    "        - LLM directly issues response -> tool found but model doesn't know how to use it -> SFT train model or better adjust prompt associated to target API\n",
    "        - hallucinates tool? -> llm using tool that doesn't exist -> perhaps model too weak -> upgrade model\n",
    "            - or API naming is not logical -> revamp API\n",
    "            - or instructions are unclear -> revamp top-level instructions\n",
    "\n",
    "        - LLM uses wrong tool \n",
    "            - tool router error -> retrain tool router\n",
    "            - model chose wrong tool -> SFT train or better adjust prompt associated to target API\n",
    "        \n",
    "        - Infers wrong argument:\n",
    "            - argument can't be inferred -> Introduce a helper tool and/or ensure the context carries the right information\n",
    "            - Model doesn't know how to use the tool -> SFT train or better adjust prompt associated to target API  \n",
    "    - Tool call errors\n",
    "        - wrong response or error returned\n",
    "            - sometimes, errors are legitimitate\n",
    "            - otherwise, fix the tool implementation\n",
    "        - no response\n",
    "            - when final response is hallucinated, we often see this\n",
    "            - Return something, even if it's an empty JSON; in general, return meaningful tool outputs\n",
    "    - Response generation error: wrong response\n",
    "        - Final response doesn't convey the tool's response\n",
    "            - model lacks grounding capabilities - upgrade LLM\n",
    "            - Tool response spams the context window - Trim/summarize information returned by the backend\n",
    "            - Tool response does not convey information meaningfully - Make the tool output format descriptive\n",
    "\n",
    "Summary        \n",
    "- Failures: does not use/hallucinates/uses the wrong tool, infers wrong argument\n",
    "- Failures: wrong response, no response (tool impl.)\n",
    "- Failure: wrong response (generation)\n",
    "\n",
    "Modeling\n",
    "- Weak reasoning, grounding capabilities\n",
    "- Too much going on in the context window\n",
    "- Tool modeling isn't right\n",
    "Tool\n",
    "- Tool itself has a problem\n",
    "- Output of the tool isn't interpretable\n",
    "\n",
    "\n",
    "Common benchmarks\n",
    "- MMLU for knowledge - Massive Multitask Language Understanding  - MCQ\n",
    "â€¢ Accurately state\n",
    "facts about the world\n",
    "â€¢ More \"breadth\" than\n",
    "\"depth\"\n",
    "â€¢ Reflects pretraining\n",
    "quality\n",
    "\n",
    "- Reasoning benchmarks\n",
    "    - AIME (American Invitational Mathematics Examination)\n",
    "        - output correct answer on ~30 math problems Geometry, Algebra, Analysis\n",
    "    - PIQA (Physical Interaction: Question Answering )\n",
    "        - Characteristics. 2 possible choices per question on everyday situations\n",
    "anchored in Physics.\n",
    "Has approximately 20,000 examples.\n",
    "Evaluation criteria. Find the right choice among Sol1/Sol2.\n",
    "\n",
    "\n",
    "- Coding\n",
    "    - SWE bench (SoftWare Engineering benchmark)\n",
    "        - â€¢ Generate\n",
    "syntactically correct\n",
    "code\n",
    "â€¢ Tests for\n",
    "programming\n",
    "proficiency\n",
    "â€¢ Proxy for tool use\n",
    "abilities\n",
    "\n",
    "Characteristics. 2,294 software engineering problems from real GitHub issues\n",
    "across 12 popular Python repositories.\n",
    "Each problem has:\n",
    "â— a base commit\n",
    "â— an already merged PR with tests\n",
    "\n",
    "\n",
    "Evaluation criteria. Generated PR passes all test cases.\n",
    "\n",
    "\n",
    "\n",
    "- Safety\n",
    "- Prevent harmful,\n",
    "toxic, inappropriate\n",
    "behavior\n",
    "â€¢ Surfaces\n",
    "vulnerabilities before\n",
    "deployment\n",
    "â€¢ Alignment with\n",
    "custom preferences\n",
    "\n",
    "- HarmBench (Harmful Behavior Benchmark)\n",
    "Characteristics. 510 unique harmful behaviors (400 text-based, 110 multimodal)\n",
    "split into:\n",
    "â— \"Standard\"\n",
    "â— \"Copyright\"\n",
    "â— \"Contextual\"\n",
    "â— \"Multimodal\"\n",
    "Criteria: violate laws and widely-held norms\n",
    "Evaluation criteria. Attack success rate (ASR).\n",
    "\n",
    "- classifier as output\n",
    "\n",
    "- there are both contextual as well as multi-modal behaviors\n",
    "\n",
    "\n",
    "- Agents\n",
    "  - ðœ-bench = Tool-Agent-User Interaction Benchmark\n",
    "  - Characteristics. A given set of database schema, APIs and policies across 2\n",
    "domains:\n",
    "â— Airline agent\n",
    "â—‹ 500 users, 300 flights, 2000 reservations\n",
    "â—‹ ~10 tools and 50 tasks\n",
    "â— Retail agent\n",
    "â—‹ 500 users, 50 products, 1000 orders\n",
    "â—‹ ~10 tools and 115 tasks\n",
    "Evaluation criteria. Maximize reward and pass^k\n",
    "\n",
    "\n",
    "- Pass^k = \"Probability that all k attempts succeed\"\n",
    "\n",
    "\n",
    "\n",
    "Role of benchmarks.\n",
    "â— A projection of performance across a given axis\n",
    "â— Different models may be good at different\n",
    "things\n",
    "\n",
    "Definition. Pareto curve = set of solutions that \"optimizes\" a trade-off.\n",
    "https://winston-bosan.github.io/llm-pareto-frontier/\n",
    "\n",
    "Possible trade-offs.\n",
    "â— quality vs. cost/latency\n",
    "â— quality vs. safety\n",
    "â— quality vs. context length\n",
    "\n",
    "---\n",
    "Beware of data contamination\n",
    "Problem. Benchmark's clues may be contained in the training set\n",
    "\n",
    "Precautions.\n",
    "â— Use an identifier such as a hash\n",
    "â— For tools, use a blocklist\n",
    "â— Evaluate on newer test versions!\n",
    "\n",
    "Lessons.\n",
    "â— Should not over-index on benchmarks\n",
    "â— Need for ~organic perspectives to complete the picture: Chatbot Arena\n",
    "â— ...just try a few models out yourself!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a7c420",
   "metadata": {},
   "source": [
    ">**Licensing Notice**: Text and media: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/); Code: [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
