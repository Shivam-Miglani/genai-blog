[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "My Context Window",
    "section": "",
    "text": "Hi, I am Shivam Miglani\nI build AI/ML systems and ship them to production.\nGenAI today feels like early deep learning: raw potential hitting the messy reality of deployment. The challenge isn’t “can an LLM do this?” but “can we make it reliable, observable, and affordable at scale?” That’s where I operate.\nThis is my learning space—intuition and code-first.\n\n\nWhat I Write About\nThis blog is my scratchpad for full-stack GenAI engineering, focused on:\n\nLLMs & Prompting: Making them work in production without breaking the bank\nRAG Systems: Patterns that actually work, not just demo well\nEvals: Measuring if your AI system is actually good\nAgents: Composable intelligence blocks, not magic wizards\nPapers & Case Studies: What research looks like when you implement it\n\n\n\nWhy This Space?\nI believe the best way to learn is to strip away the abstractions and build things from first principles. I hope the intuitions I write about are useful to you.\n\n\nConnect"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Context Window",
    "section": "",
    "text": "Hi, I’m Shivam Miglani. Welcome to my context window. Here I document my journey through the Generative AI landscape, from foundational concepts to production engineering.\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\nDec 22, 2025\n\n\nRetrieval-Augmented Generation (RAG) Fundamentals\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Licensing",
    "section": "",
    "text": "This repository contains both literary content and functional source code. To ensure appropriate use of each, this project is licensed under a dual-license model:\n\n\nAll textual content, documentation, and images found within the blog posts and repository are licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). - View License: https://creativecommons.org/licenses/by/4.0/\n\n\n\nAll source code, software scripts, and executable code snippets within the notebooks or repository are licensed under the Apache License, Version 2.0. - View License: http://www.apache.org/licenses/LICENSE-2.0\n\n\n\n\nCopyright 2025 - Shivam Miglani\nLicensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
  },
  {
    "objectID": "LICENSE.html#apache-license-2.0-summary",
    "href": "LICENSE.html#apache-license-2.0-summary",
    "title": "Licensing",
    "section": "",
    "text": "Copyright 2025 - Shivam Miglani\nLicensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
  },
  {
    "objectID": "posts/rag/rag.html",
    "href": "posts/rag/rag.html",
    "title": "Retrieval-Augmented Generation (RAG) Fundamentals",
    "section": "",
    "text": "NoteLicensing Notice\n\n\n\nText and media: CC BY 4.0 Code and snippets: Apache License 2.0"
  },
  {
    "objectID": "posts/rag/rag.html#motivation-and-meaning",
    "href": "posts/rag/rag.html#motivation-and-meaning",
    "title": "Retrieval-Augmented Generation (RAG) Fundamentals",
    "section": "Motivation and Meaning",
    "text": "Motivation and Meaning\nRetrieval Augmented Generation (RAG) is inference-time context injection: pull relevant external data and condition generation on it. This grounds responses in authoritative sources while keeping LLMs parametric knowledge (weights) frozen.\n\nMotivation\n\nKnowledge limited to training data: Your proprietary/domain-specific data isn’t there\nFixed knowledge cut-off date: The model can’t answer questions about recent events. Without RAG, they will either refuse to answer or hallucinate.\n\n\nAlternatives & Trade-offs\n\nWhy not fine-tune?\n\nFine-tuning excels at teaching task formats (SQL generation, JSON output) and reasoning styles, not injecting factual knowledge.\nCatastrophic forgetting: Updating knowledge degrades performance on other tasks as model weights get overwritten.\nInefficient: requires separate fine-tuned checkpoints per domain or use-case or document. It is tricky to learn new knowledge without regressing on old knowledge even with LoRA/QLoRA.\n\nOk, why not just stuff everything in the context window?\n\nRecall Degradation: While 1M+ token models ace “single-needle” tests, performance drops significantly (to ~60-70%) when retrieving multiple distributed facts\nCost & Latency: Processing massive contexts is computationally expensive and slow compared to vector search. Retrieval remains necessary for corpora exceeding the window size.\n\nRAG is a reasonable pattern:\n\nWhen you need fresh, attributable knowledge with minimal model changes and can tolerate added latency.\n\n\n\n\n\nThe RAG Pipeline\nRAG acts as a filter to inject only relevant context. A typical production pipeline looks like this:\n\nIngestion & Indexing: Chunk documents, generate embeddings, and upsert into a vector database. Note: In production, this is a continuous sync pipeline, not a one-time setup.\nRetrieval: For a user query, search your indexed corpus (vector/keyword) and pull the top‑k relevant chunks, often followed by a re-ranking step for precision.\nAugmented (prompt): Inject selected chunks into the system prompt or user message with appropriate metadata (source citations).\nGeneration: The LLM generates an answer conditioned strictly on the provided context, minimizing external knowledge leakage.\n\n\n\n\n\n\n\nflowchart TB\n    n2[\"LLM\"] L_n2_n4_0@-- generates grounded answer --&gt; n4[\"Answer\"]\n    n3[\"Document Corpus\"] L_n3_n5_0@&lt;-- ingestion pipeline&lt;br&gt;(chunk + embed) --&gt; n5[\"Hybrid index&lt;br&gt;(inverted keywords&lt;br&gt;+ &lt;br&gt;vector embeddings)&lt;br&gt;&lt;br&gt;\"]\n    n5 L_n5_n6_0@-- \"top-k\" --&gt; n6[\"Retrieval &amp; Re-ranking\"]\n    n6 L_n6_n7_0@-- \"top-k re-ranked chunks + citation metadata\" --&gt; n7[\"Prompt Builder\"]\n    n7 L_n7_n2_0@-- \"system prompt (use only given context) + user query + &lt;br&gt;top-k re-ranked chunks &amp; citation metadata\" --&gt; n2\n    n1[\"User Query\"] L_n1_n8_0@--&gt; n8[\"Query processing &lt;br&gt;&amp; embedding\"]\n    n1 L_n1_n7_0@-- user query --&gt; n7\n    n8 L_n8_n6_0@-- text + query expansions + embeddings --&gt; n6\n\n    n3@{ shape: docs}\n    n5@{ shape: cyl}\n    n6@{ shape: rect}\n    n7@{ shape: rect}\n    n1@{ shape: rect}\n    n8@{ shape: rect}\n\n    L_n2_n4_0@{ animation: slow } \n    L_n3_n5_0@{ animation: none } \n    L_n5_n6_0@{ animation: slow } \n    L_n6_n7_0@{ animation: slow } \n    L_n7_n2_0@{ animation: slow } \n    L_n1_n8_0@{ animation: slow } \n    L_n1_n7_0@{ animation: slow } \n    L_n8_n6_0@{ animation: slow }\n\n\n\n\nFigure 1: RAG pipeline flowchart showing ingestion pipeline, query processing, retrieval, augmentation and LLM generation.\n\n\n\n\n\n\nimport pyspark\nfrom delta import *\nfrom pyspark.sql import SparkSession\n\nbuilder = (SparkSession.builder\n           .appName(\"LocalDatabricksPrep\")\n           .master(\"local[*]\")\n           .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n           .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n           # This downloads the Delta jar automatically:\n           .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\") \n)\n\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\n\nprint(\"Spark + Delta session created!\")\n\nSpark + Delta session created!\n\n\n\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window\nimport pyspark.sql.types as T\nimport pandas as pd\nfrom pyspark import SparkFiles\n\n\nurl = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv\"\nspark.sparkContext.addFile(url)\n\n\n# 1. Download Raw CSV directly\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"file://\"+ SparkFiles.get(\"netflix_titles.csv\"))\ndf.show(truncate=False, n=5)\n\n+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n|show_id|type   |title|director         |cast                                                                                                                                                                      |country      |date_added       |release_year|rating|duration |listed_in                                               |description                                                                                                                                          |\n+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n|s1     |TV Show|3%   |NULL             |João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi|Brazil       |August 14, 2020  |2020        |TV-MA |4 Seasons|International TV Shows, TV Dramas, TV Sci-Fi & Fantasy  |In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.             |\n|s2     |Movie  |7:19 |Jorge Michel Grau|Demián Bichir, Héctor Bonilla, Oscar Serrano, Azalia Ortiz, Octavio Michel, Carmen Beato                                                                                  |Mexico       |December 23, 2016|2016        |TV-MA |93 min   |Dramas, International Movies                            |After a devastating earthquake hits Mexico City, trapped survivors from all walks of life wait to be rescued while trying desperately to stay alive. |\n|s3     |Movie  |23:59|Gilbert Chan     |Tedd Chan, Stella Chung, Henley Hii, Lawrence Koh, Tommy Kuan, Josh Lai, Mark Lee, Susan Leong, Benjamin Lim                                                              |Singapore    |December 20, 2018|2011        |R     |78 min   |Horror Movies, International Movies                     |When an army recruit is found dead, his fellow soldiers are forced to confront a terrifying secret that's haunting their jungle island training camp.|\n|s4     |Movie  |9    |Shane Acker      |Elijah Wood, John C. Reilly, Jennifer Connelly, Christopher Plummer, Crispin Glover, Martin Landau, Fred Tatasciore, Alan Oppenheimer, Tom Kane                           |United States|November 16, 2017|2009        |PG-13 |80 min   |Action & Adventure, Independent Movies, Sci-Fi & Fantasy|In a postapocalyptic world, rag-doll robots hide in fear from dangerous machines out to exterminate them, until a brave newcomer joins the group.    |\n|s5     |Movie  |21   |Robert Luketic   |Jim Sturgess, Kevin Spacey, Kate Bosworth, Aaron Yoo, Liza Lapira, Jacob Pitts, Laurence Fishburne, Jack McGee, Josh Gad, Sam Golzari, Helen Carey, Jack Gilpin           |United States|January 1, 2020  |2008        |PG-13 |123 min  |Dramas                                                  |A brilliant group of students become card-counting experts with the intent of swindling millions out of Las Vegas casinos by playing blackjack.      |\n+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n\n\ndf.printSchema()\n\nroot\n |-- show_id: string (nullable = true)\n |-- type: string (nullable = true)\n |-- title: string (nullable = true)\n |-- director: string (nullable = true)\n |-- cast: string (nullable = true)\n |-- country: string (nullable = true)\n |-- date_added: string (nullable = true)\n |-- release_year: string (nullable = true)\n |-- rating: string (nullable = true)\n |-- duration: string (nullable = true)\n |-- listed_in: string (nullable = true)\n |-- description: string (nullable = true)\n\n\n\n\n# The Task:\n\n# From netflix_bronze, select title, release_year, and date_added.\n\n# Clean date_added (currently string like \"September 25, 2021\") into a real date.\n\n# Calculate days_diff = date_added - release_year (assume Jan 1st).\n\n\nsub_df = df.select(\n    F.col(\"title\"),\n    F.col(\"release_year\"),\n    F.col(\"date_added\"),\n)\n\nsub_df = sub_df.withColumn(\"date_added_clean\", F.to_date(F.col(\"date_added\"), \"MMMM d, yyyy\"))\nsub_df = sub_df.withColumn(\"release_year_clean\", F.to_date(F.col(\"release_year\"), \"y\"))\n# Assuming 1 jan of release year\nsub_df = sub_df.withColumn(\"days_diff\", F.date_diff(F.col(\"date_added_clean\"), F.col(\"release_year_clean\"))) # default is somehow days\n\n\nsub_df.show(n=5)\n\n\n+-----+------------+-----------------+----------------+------------------+---------+\n|title|release_year|       date_added|date_added_clean|release_year_clean|days_diff|\n+-----+------------+-----------------+----------------+------------------+---------+\n|   3%|        2020|  August 14, 2020|      2020-08-14|        2020-01-01|      226|\n| 7:19|        2016|December 23, 2016|      2016-12-23|        2016-01-01|      357|\n|23:59|        2011|December 20, 2018|      2018-12-20|        2011-01-01|     2910|\n|    9|        2009|November 16, 2017|      2017-11-16|        2009-01-01|     3241|\n|   21|        2008|  January 1, 2020|      2020-01-01|        2008-01-01|     4383|\n+-----+------------+-----------------+----------------+------------------+---------+\nonly showing top 5 rows\n\n\n\n# Get a simple list of movies for \"Brad Pitt\".\n\ndf.filter(F.lower(F.col(\"cast\")).contains(\"brad pitt\")).select(F.col(\"title\")).show()\n\n+--------------------+\n|               title|\n+--------------------+\n|A Stoning in Fulh...|\n|               Babel|\n|          By the Sea|\n|Inglourious Basterds|\n| Killing Them Softly|\n|    Ocean's Thirteen|\n|      Ocean's Twelve|\n|         War Machine|\n+--------------------+\n\n\n\n\n# \"Who are the top 5 most frequent actors in the dataset?\"\ntemp_df = df.withColumn(\"cast_split\", F.split(F.col(\"cast\"), \",\")).withColumn(\"cast_exploded\", F.explode(F.col(\"cast_split\")))\ntemp_df.groupBy(F.col(\"cast_exploded\")).agg({\"cast_exploded\": 'count'}).select(\"cast_exploded\", F.col(\"count(cast_exploded)\").alias(\"count\")).orderBy(F.col(\"count\").desc()).show(n=5)\n\n+-----------------+-----+\n|    cast_exploded|count|\n+-----------------+-----+\n|      Anupam Kher|   38|\n| Takahiro Sakurai|   28|\n|          Om Puri|   27|\n|   Shah Rukh Khan|   27|\n|      Boman Irani|   25|\n+-----------------+-----+\nonly showing top 5 rows\n\n\n\ntemp_df.groupBy(\n    F.col(\"cast_exploded\")\n).agg(\n    F.count(\"*\").alias(\"count\")\n).orderBy(F.col(\"count\").desc()).show(5)\n\n+-----------------+-----+\n|    cast_exploded|count|\n+-----------------+-----+\n|      Anupam Kher|   38|\n| Takahiro Sakurai|   28|\n|          Om Puri|   27|\n|   Shah Rukh Khan|   27|\n|      Boman Irani|   25|\n+-----------------+-----+\nonly showing top 5 rows\n\n\n\ntemp_df = df.withColumn(\"cast_split\", F.split(F.col(\"cast\"), \",\")).withColumn(\"cast_split_clean\", F.transform(F.col(\"cast_split\"), lambda x: F.trim(x)))\ntemp_df.show()\n\n+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+\n|show_id|   type| title|            director|                cast|             country|       date_added|release_year|rating| duration|           listed_in|         description|          cast_split|    cast_split_clean|\n+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+\n|     s1|TV Show|    3%|                NULL|João Miguel, Bian...|              Brazil|  August 14, 2020|        2020| TV-MA|4 Seasons|International TV ...|In a future where...|[João Miguel,  Bi...|[João Miguel, Bia...|\n|     s2|  Movie|  7:19|   Jorge Michel Grau|Demián Bichir, Hé...|              Mexico|December 23, 2016|        2016| TV-MA|   93 min|Dramas, Internati...|After a devastati...|[Demián Bichir,  ...|[Demián Bichir, H...|\n|     s3|  Movie| 23:59|        Gilbert Chan|Tedd Chan, Stella...|           Singapore|December 20, 2018|        2011|     R|   78 min|Horror Movies, In...|When an army recr...|[Tedd Chan,  Stel...|[Tedd Chan, Stell...|\n|     s4|  Movie|     9|         Shane Acker|Elijah Wood, John...|       United States|November 16, 2017|        2009| PG-13|   80 min|Action & Adventur...|In a postapocalyp...|[Elijah Wood,  Jo...|[Elijah Wood, Joh...|\n|     s5|  Movie|    21|      Robert Luketic|Jim Sturgess, Kev...|       United States|  January 1, 2020|        2008| PG-13|  123 min|              Dramas|A brilliant group...|[Jim Sturgess,  K...|[Jim Sturgess, Ke...|\n|     s6|TV Show|    46|         Serdar Akar|Erdal Beşikçioğlu...|              Turkey|     July 1, 2017|        2016| TV-MA| 1 Season|International TV ...|A genetics profes...|[Erdal Beşikçioğl...|[Erdal Beşikçioğl...|\n|     s7|  Movie|   122|     Yasir Al Yasiri|Amina Khalil, Ahm...|               Egypt|     June 1, 2020|        2019| TV-MA|   95 min|Horror Movies, In...|After an awful ac...|[Amina Khalil,  A...|[Amina Khalil, Ah...|\n|     s8|  Movie|   187|      Kevin Reynolds|Samuel L. Jackson...|       United States| November 1, 2019|        1997|     R|  119 min|              Dramas|After one of his ...|[Samuel L. Jackso...|[Samuel L. Jackso...|\n|     s9|  Movie|   706|       Shravan Kumar|Divya Dutta, Atul...|               India|    April 1, 2019|        2019| TV-14|  118 min|Horror Movies, In...|When a doctor goe...|[Divya Dutta,  At...|[Divya Dutta, Atu...|\n|    s10|  Movie|  1920|        Vikram Bhatt|Rajneesh Duggal, ...|               India|December 15, 2017|        2008| TV-MA|  143 min|Horror Movies, In...|An architect and ...|[Rajneesh Duggal,...|[Rajneesh Duggal,...|\n|    s11|  Movie|  1922|        Zak Hilditch|Thomas Jane, Moll...|       United States| October 20, 2017|        2017| TV-MA|  103 min|   Dramas, Thrillers|A farmer pens a c...|[Thomas Jane,  Mo...|[Thomas Jane, Mol...|\n|    s12|TV Show|  1983|                NULL|Robert Więckiewic...|Poland, United St...|November 30, 2018|        2018| TV-MA| 1 Season|Crime TV Shows, I...|In this dark alt-...|[Robert Więckiewi...|[Robert Więckiewi...|\n|    s13|TV Show|  1994|Diego Enrique Osorno|                NULL|              Mexico|     May 17, 2019|        2019| TV-MA| 1 Season|Crime TV Shows, D...|Archival video an...|                NULL|                NULL|\n|    s14|  Movie| 2,215| Nottapon Boonprakob|  Artiwara Kongmalai|            Thailand|    March 1, 2019|        2018| TV-MA|   89 min|Documentaries, In...|This intimate doc...|[Artiwara Kongmalai]|[Artiwara Kongmalai]|\n|    s15|  Movie|  3022|          John Suits|Omar Epps, Kate W...|       United States|   March 19, 2020|        2019|     R|   91 min|Independent Movie...|Stranded when the...|[Omar Epps,  Kate...|[Omar Epps, Kate ...|\n|    s16|  Movie|Oct-01|      Kunle Afolayan|Sadiq Daba, David...|             Nigeria|September 1, 2019|        2014| TV-14|  149 min|Dramas, Internati...|Against the backd...|[Sadiq Daba,  Dav...|[Sadiq Daba, Davi...|\n|    s17|TV Show|Feb-09|                NULL|Shahd El Yaseen, ...|                NULL|   March 20, 2019|        2018| TV-14| 1 Season|International TV ...|As a psychology p...|[Shahd El Yaseen,...|[Shahd El Yaseen,...|\n|    s18|  Movie|22-Jul|     Paul Greengrass|Anders Danielsen ...|Norway, Iceland, ...| October 10, 2018|        2018|     R|  144 min|   Dramas, Thrillers|After devastating...|[Anders Danielsen...|[Anders Danielsen...|\n|    s19|  Movie|15-Aug|  Swapnaneel Jayakar|Rahul Pethe, Mrun...|               India|   March 29, 2019|        2019| TV-14|  124 min|Comedies, Dramas,...|On India's Indepe...|[Rahul Pethe,  Mr...|[Rahul Pethe, Mru...|\n|    s20|  Movie|   '89|                NULL|Lee Dixon, Ian Wr...|      United Kingdom|     May 16, 2018|        2017| TV-PG|   87 min|       Sports Movies|Mixing old footag...|[Lee Dixon,  Ian ...|[Lee Dixon, Ian W...|\n+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n\n\n# “For the top 10 actors by movie count, what is the average gap in years between their consecutive movies?”\nexploded_temp_df = temp_df.withColumn(\"actor\", F.explode(F.col(\"cast_split_clean\"))).filter(F.col(\"actor\") != \"\").select(\"actor\", \"title\", \"release_year\")\n# exploded_temp_df.show()\n\nactor_counts = exploded_temp_df.groupBy(F.col(\"actor\")).agg(F.count(\"*\").alias(\"count\")).orderBy(F.col(\"count\").desc())\n# actor_counts.show()\n\ntop10 = actor_counts.limit(10)\n\n\n# join back on original table\ntop10_actors = exploded_temp_df.join(F.broadcast(top10), on=\"actor\", how=\"inner\")\n# top10_actors.show()\n\n\n# window func.\n\nw = Window.partitionBy(\"actor\").orderBy(\"release_year\")\n\n\n# 5. Compute previous movie year using lag, then gap\ngaps_df = (top10_actors\n    .withColumn(\"prev_year\", F.lag(\"release_year\").over(w))\n    .filter(F.col(\"prev_year\").isNotNull())\n    .withColumn(\"gap_years\", F.datediff(F.col(\"release_year\"), F.col(\"prev_year\"))/F.lit(365.0))\n)\n\ngaps_df.show()\n\n+------------+--------------------+------------+-----+---------+------------------+\n|       actor|               title|release_year|count|prev_year|         gap_years|\n+------------+--------------------+------------+-----+---------+------------------+\n|Akshay Kumar|Mujhse Shaadi Karogi|        2004|   29|     2004|               0.0|\n|Akshay Kumar|             Bewafaa|        2005|   29|     2004|1.0027397260273974|\n|Akshay Kumar|               Insan|        2005|   29|     2005|               0.0|\n|Akshay Kumar|         Bhagam Bhag|        2006|   29|     2005|               1.0|\n|Akshay Kumar|Humko Deewana Kar...|        2006|   29|     2006|               0.0|\n|Akshay Kumar|Jaan-E-Mann: Let'...|        2006|   29|     2006|               0.0|\n|Akshay Kumar|     Phir Hera Pheri|        2006|   29|     2006|               0.0|\n|Akshay Kumar|     Bhool Bhulaiyaa|        2007|   29|     2006|               1.0|\n|Akshay Kumar|     Namastey London|        2007|   29|     2007|               0.0|\n|Akshay Kumar|             Welcome|        2007|   29|     2007|               0.0|\n|Akshay Kumar|      Action Replayy|        2010|   29|     2007|3.0027397260273974|\n|Akshay Kumar|      Tees Maar Khan|        2010|   29|     2010|               0.0|\n|Akshay Kumar|       Patiala House|        2011|   29|     2010|               1.0|\n|Akshay Kumar|           Thank You|        2011|   29|     2011|               0.0|\n|Akshay Kumar|               Joker|        2012|   29|     2011|               1.0|\n|Akshay Kumar|           Oh My God|        2012|   29|     2012|               0.0|\n|Akshay Kumar|       Rowdy Rathore|        2012|   29|     2012|               0.0|\n|Akshay Kumar|                Boss|        2013|   29|     2012|1.0027397260273974|\n|Akshay Kumar|Once Upon a Time ...|        2013|   29|     2013|               0.0|\n|Akshay Kumar|          Special 26|        2013|   29|     2013|               0.0|\n+------------+--------------------+------------+-----+---------+------------------+\nonly showing top 20 rows\n\n\n\ngaps_df.groupBy(\"actor\").agg(F.avg(F.col(\"gap_years\")).alias(\"avg_gap_year\")).orderBy(F.col(\"avg_gap_year\").asc()).show()\n\n+----------------+-------------------+\n|           actor|       avg_gap_year|\n+----------------+-------------------+\n|       Yuki Kaji|0.38482613277133826|\n|Takahiro Sakurai| 0.4288649706457926|\n|    Akshay Kumar| 0.5361056751467711|\n|     Boman Irani| 0.6158061116965227|\n|     Anupam Kher| 0.7077848312729702|\n|  Shah Rukh Khan| 0.7946817082997581|\n|    Paresh Rawal|  1.116122233930453|\n|         Om Puri|  1.207746811525744|\n|Naseeruddin Shah| 1.2422295701464334|\n|Amitabh Bachchan| 1.6934668071654373|\n+----------------+-------------------+\n\n\n\n25/12/23 16:39:25 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 956198 ms exceeds timeout 120000 ms\n25/12/23 16:39:25 WARN SparkContext: Killing executors is not supported by current scheduler.\n25/12/23 16:39:26 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:39:26 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:39:36 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:39:36 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:39:46 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:39:46 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:39:56 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:39:56 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:54:23 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:54:23 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:54:33 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:54:33 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:54:43 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:54:43 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:54:53 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:54:53 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:55:03 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 16:55:03 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:10:17 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:10:17 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:10:27 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:10:27 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:10:37 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:10:37 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:10:47 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:10:47 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:10:57 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:10:57 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:26:18 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:26:18 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:26:28 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:26:28 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:26:38 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:26:38 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:26:48 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:26:48 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:26:58 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:26:58 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:42:57 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:42:57 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:43:07 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:43:07 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:43:17 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:43:17 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:43:27 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:43:27 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:43:37 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:43:37 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:55:28 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:55:28 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:55:38 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:55:38 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:55:48 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:55:48 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:55:58 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:55:58 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:56:08 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 17:56:08 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:12:53 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:12:53 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:13:03 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:13:03 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:13:13 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:13:13 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:13:23 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:13:23 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:30:28 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:30:28 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:30:38 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:30:38 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:30:48 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:30:48 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:30:58 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:30:58 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:31:08 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:31:08 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:31:18 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:31:18 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:31:28 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:31:28 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:48:04 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:48:04 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:48:14 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:48:14 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:48:24 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:48:24 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:48:34 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:48:34 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:48:44 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:48:44 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:56:32 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:56:32 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:56:42 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:56:42 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:56:52 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:56:52 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:57:02 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 18:57:02 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:12:17 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:12:17 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:12:27 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:12:27 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:12:37 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:12:37 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:12:47 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:12:47 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:12:57 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:12:57 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:28:57 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:28:57 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:07 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:07 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:17 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:17 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:27 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:27 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:37 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:37 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:47 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:47 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:99)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:57)\n    at scala.concurrent.Promise.complete$(Promise.scala:56)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n    at scala.concurrent.Promise.success(Promise.scala:91)\n    at scala.concurrent.Promise.success$(Promise.scala:91)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/12/23 19:29:47 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7787 entries, 0 to 7786\nData columns (total 12 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   show_id       7787 non-null   object\n 1   type          7787 non-null   object\n 2   title         7787 non-null   object\n 3   director      5398 non-null   object\n 4   cast          7069 non-null   object\n 5   country       7280 non-null   object\n 6   date_added    7777 non-null   object\n 7   release_year  7787 non-null   int64 \n 8   rating        7780 non-null   object\n 9   duration      7787 non-null   object\n 10  listed_in     7787 non-null   object\n 11  description   7787 non-null   object\ndtypes: int64(1), object(11)\nmemory usage: 730.2+ KB\n\n\n\ndf['release_year'] = df['release_year'].astype('category')\n\n\nhelp(df.describe)\n\nHelp on method describe in module pandas.core.generic:\n\ndescribe(percentiles=None, include=None, exclude=None) -&gt; 'Self' method of pandas.core.frame.DataFrame instance\n    Generate descriptive statistics.\n\n    Descriptive statistics include those that summarize the central\n    tendency, dispersion and shape of a\n    dataset's distribution, excluding ``NaN`` values.\n\n    Analyzes both numeric and object series, as well\n    as ``DataFrame`` column sets of mixed data types. The output\n    will vary depending on what is provided. Refer to the notes\n    below for more detail.\n\n    Parameters\n    ----------\n    percentiles : list-like of numbers, optional\n        The percentiles to include in the output. All should\n        fall between 0 and 1. The default is\n        ``[.25, .5, .75]``, which returns the 25th, 50th, and\n        75th percentiles.\n    include : 'all', list-like of dtypes or None (default), optional\n        A white list of data types to include in the result. Ignored\n        for ``Series``. Here are the options:\n\n        - 'all' : All columns of the input will be included in the output.\n        - A list-like of dtypes : Limits the results to the\n          provided data types.\n          To limit the result to numeric types submit\n          ``numpy.number``. To limit it instead to object columns submit\n          the ``numpy.object`` data type. Strings\n          can also be used in the style of\n          ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To\n          select pandas categorical columns, use ``'category'``\n        - None (default) : The result will include all numeric columns.\n    exclude : list-like of dtypes or None (default), optional,\n        A black list of data types to omit from the result. Ignored\n        for ``Series``. Here are the options:\n\n        - A list-like of dtypes : Excludes the provided data types\n          from the result. To exclude numeric types submit\n          ``numpy.number``. To exclude object columns submit the data\n          type ``numpy.object``. Strings can also be used in the style of\n          ``select_dtypes`` (e.g. ``df.describe(exclude=['O'])``). To\n          exclude pandas categorical columns, use ``'category'``\n        - None (default) : The result will exclude nothing.\n\n    Returns\n    -------\n    Series or DataFrame\n        Summary statistics of the Series or Dataframe provided.\n\n    See Also\n    --------\n    DataFrame.count: Count number of non-NA/null observations.\n    DataFrame.max: Maximum of the values in the object.\n    DataFrame.min: Minimum of the values in the object.\n    DataFrame.mean: Mean of the values.\n    DataFrame.std: Standard deviation of the observations.\n    DataFrame.select_dtypes: Subset of a DataFrame including/excluding\n        columns based on their dtype.\n\n    Notes\n    -----\n    For numeric data, the result's index will include ``count``,\n    ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\n    upper percentiles. By default the lower percentile is ``25`` and the\n    upper percentile is ``75``. The ``50`` percentile is the\n    same as the median.\n\n    For object data (e.g. strings or timestamps), the result's index\n    will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\n    is the most common value. The ``freq`` is the most common value's\n    frequency. Timestamps also include the ``first`` and ``last`` items.\n\n    If multiple object values have the highest count, then the\n    ``count`` and ``top`` results will be arbitrarily chosen from\n    among those with the highest count.\n\n    For mixed data types provided via a ``DataFrame``, the default is to\n    return only an analysis of numeric columns. If the dataframe consists\n    only of object and categorical data without any numeric columns, the\n    default is to return an analysis of both the object and categorical\n    columns. If ``include='all'`` is provided as an option, the result\n    will include a union of attributes of each type.\n\n    The `include` and `exclude` parameters can be used to limit\n    which columns in a ``DataFrame`` are analyzed for the output.\n    The parameters are ignored when analyzing a ``Series``.\n\n    Examples\n    --------\n    Describing a numeric ``Series``.\n\n    &gt;&gt;&gt; s = pd.Series([1, 2, 3])\n    &gt;&gt;&gt; s.describe()\n    count    3.0\n    mean     2.0\n    std      1.0\n    min      1.0\n    25%      1.5\n    50%      2.0\n    75%      2.5\n    max      3.0\n    dtype: float64\n\n    Describing a categorical ``Series``.\n\n    &gt;&gt;&gt; s = pd.Series(['a', 'a', 'b', 'c'])\n    &gt;&gt;&gt; s.describe()\n    count     4\n    unique    3\n    top       a\n    freq      2\n    dtype: object\n\n    Describing a timestamp ``Series``.\n\n    &gt;&gt;&gt; s = pd.Series([\n    ...     np.datetime64(\"2000-01-01\"),\n    ...     np.datetime64(\"2010-01-01\"),\n    ...     np.datetime64(\"2010-01-01\")\n    ... ])\n    &gt;&gt;&gt; s.describe()\n    count                      3\n    mean     2006-09-01 08:00:00\n    min      2000-01-01 00:00:00\n    25%      2004-12-31 12:00:00\n    50%      2010-01-01 00:00:00\n    75%      2010-01-01 00:00:00\n    max      2010-01-01 00:00:00\n    dtype: object\n\n    Describing a ``DataFrame``. By default only numeric fields\n    are returned.\n\n    &gt;&gt;&gt; df = pd.DataFrame({'categorical': pd.Categorical(['d', 'e', 'f']),\n    ...                    'numeric': [1, 2, 3],\n    ...                    'object': ['a', 'b', 'c']\n    ...                    })\n    &gt;&gt;&gt; df.describe()\n           numeric\n    count      3.0\n    mean       2.0\n    std        1.0\n    min        1.0\n    25%        1.5\n    50%        2.0\n    75%        2.5\n    max        3.0\n\n    Describing all columns of a ``DataFrame`` regardless of data type.\n\n    &gt;&gt;&gt; df.describe(include='all')  # doctest: +SKIP\n           categorical  numeric object\n    count            3      3.0      3\n    unique           3      NaN      3\n    top              f      NaN      a\n    freq             1      NaN      1\n    mean           NaN      2.0    NaN\n    std            NaN      1.0    NaN\n    min            NaN      1.0    NaN\n    25%            NaN      1.5    NaN\n    50%            NaN      2.0    NaN\n    75%            NaN      2.5    NaN\n    max            NaN      3.0    NaN\n\n    Describing a column from a ``DataFrame`` by accessing it as\n    an attribute.\n\n    &gt;&gt;&gt; df.numeric.describe()\n    count    3.0\n    mean     2.0\n    std      1.0\n    min      1.0\n    25%      1.5\n    50%      2.0\n    75%      2.5\n    max      3.0\n    Name: numeric, dtype: float64\n\n    Including only numeric columns in a ``DataFrame`` description.\n\n    &gt;&gt;&gt; df.describe(include=[np.number])\n           numeric\n    count      3.0\n    mean       2.0\n    std        1.0\n    min        1.0\n    25%        1.5\n    50%        2.0\n    75%        2.5\n    max        3.0\n\n    Including only string columns in a ``DataFrame`` description.\n\n    &gt;&gt;&gt; df.describe(include=[object])  # doctest: +SKIP\n           object\n    count       3\n    unique      3\n    top         a\n    freq        1\n\n    Including only categorical columns from a ``DataFrame`` description.\n\n    &gt;&gt;&gt; df.describe(include=['category'])\n           categorical\n    count            3\n    unique           3\n    top              d\n    freq             1\n\n    Excluding numeric columns from a ``DataFrame`` description.\n\n    &gt;&gt;&gt; df.describe(exclude=[np.number])  # doctest: +SKIP\n           categorical object\n    count            3      3\n    unique           3      3\n    top              f      a\n    freq             1      1\n\n    Excluding object columns from a ``DataFrame`` description.\n\n    &gt;&gt;&gt; df.describe(exclude=[object])  # doctest: +SKIP\n           categorical  numeric\n    count            3      3.0\n    unique           3      NaN\n    top              f      NaN\n    freq             1      NaN\n    mean           NaN      2.0\n    std            NaN      1.0\n    min            NaN      1.0\n    25%            NaN      1.5\n    50%            NaN      2.0\n    75%            NaN      2.5\n    max            NaN      3.0\n\n\n\n\ndf\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n0\ns1\nTV Show\n3%\nNaN\nJoão Miguel, Bianca Comparato, Michel Gomes, R...\nBrazil\nAugust 14, 2020\n2020\nTV-MA\n4 Seasons\nInternational TV Shows, TV Dramas, TV Sci-Fi &...\nIn a future where the elite inhabit an island ...\n\n\n1\ns2\nMovie\n7:19\nJorge Michel Grau\nDemián Bichir, Héctor Bonilla, Oscar Serrano, ...\nMexico\nDecember 23, 2016\n2016\nTV-MA\n93 min\nDramas, International Movies\nAfter a devastating earthquake hits Mexico Cit...\n\n\n2\ns3\nMovie\n23:59\nGilbert Chan\nTedd Chan, Stella Chung, Henley Hii, Lawrence ...\nSingapore\nDecember 20, 2018\n2011\nR\n78 min\nHorror Movies, International Movies\nWhen an army recruit is found dead, his fellow...\n\n\n3\ns4\nMovie\n9\nShane Acker\nElijah Wood, John C. Reilly, Jennifer Connelly...\nUnited States\nNovember 16, 2017\n2009\nPG-13\n80 min\nAction & Adventure, Independent Movies, Sci-Fi...\nIn a postapocalyptic world, rag-doll robots hi...\n\n\n4\ns5\nMovie\n21\nRobert Luketic\nJim Sturgess, Kevin Spacey, Kate Bosworth, Aar...\nUnited States\nJanuary 1, 2020\n2008\nPG-13\n123 min\nDramas\nA brilliant group of students become card-coun...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7782\ns7783\nMovie\nZozo\nJosef Fares\nImad Creidi, Antoinette Turk, Elias Gergi, Car...\nSweden, Czech Republic, United Kingdom, Denmar...\nOctober 19, 2020\n2005\nTV-MA\n99 min\nDramas, International Movies\nWhen Lebanon's Civil War deprives Zozo of his ...\n\n\n7783\ns7784\nMovie\nZubaan\nMozez Singh\nVicky Kaushal, Sarah-Jane Dias, Raaghav Chanan...\nIndia\nMarch 2, 2019\n2015\nTV-14\n111 min\nDramas, International Movies, Music & Musicals\nA scrappy but poor boy worms his way into a ty...\n\n\n7784\ns7785\nMovie\nZulu Man in Japan\nNaN\nNasty C\nNaN\nSeptember 25, 2020\n2019\nTV-MA\n44 min\nDocumentaries, International Movies, Music & M...\nIn this documentary, South African rapper Nast...\n\n\n7785\ns7786\nTV Show\nZumbo's Just Desserts\nNaN\nAdriano Zumbo, Rachel Khoo\nAustralia\nOctober 31, 2020\n2019\nTV-PG\n1 Season\nInternational TV Shows, Reality TV\nDessert wizard Adriano Zumbo looks for the nex...\n\n\n7786\ns7787\nMovie\nZZ TOP: THAT LITTLE OL' BAND FROM TEXAS\nSam Dunn\nNaN\nUnited Kingdom, Canada, United States\nMarch 1, 2020\n2019\nTV-MA\n90 min\nDocumentaries, Music & Musicals\nThis documentary delves into the mystique behi...\n\n\n\n\n7787 rows × 12 columns"
  }
]