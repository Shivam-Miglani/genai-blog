[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "My Context Window",
    "section": "",
    "text": "Hi, I am Shivam Miglani\nI build AI/ML systems and ship them to production.\nGenAI today feels like early deep learning: raw potential hitting the messy reality of deployment. The challenge isn‚Äôt ‚Äúcan an LLM do this?‚Äù but ‚Äúcan we make it reliable, observable, and affordable at scale?‚Äù That‚Äôs where I operate.\nThis is my learning space‚Äîintuition and code-first.\n\n\nWhat I Write About\nThis blog is my scratchpad for full-stack GenAI engineering, focused on:\n\nLLMs & Prompting: Making them work in production without breaking the bank\nRAG Systems: Patterns that actually work, not just demo well\nEvals: Measuring if your AI system is actually good\nAgents: Composable intelligence blocks, not magic wizards\nPapers & Case Studies: What research looks like when you implement it\n\n\n\nWhy This Space?\nI believe the best way to learn is to strip away the abstractions and build things from first principles. I hope the intuitions I write about are useful to you.\n\n\nConnect"
  },
  {
    "objectID": "posts/field_notes/TIL-llm-agents.html",
    "href": "posts/field_notes/TIL-llm-agents.html",
    "title": "TIL from Stanford CME295 Transformers & LLMs | Lecture 7 - Agentic LLMs",
    "section": "",
    "text": "Lecture emphasises the shift from ‚ÄúPrompt Engineering‚Äù to ‚ÄúContext Engineering‚Äù\n\n\n\nModel has knowledge cut-off and it is tricky to update it (think catastrophic forgetting, LoRA/ fine-tuning and doing it for every knowledge-update or use-case)\neven with large context windows of 100k-1M tokens, we still need retrieval because there are problems like:\n\nfinding needle in haystack - high recall for single needle but for multiple needles (more real-world scenario) recall drops massively -&gt; garbage in, garbage out still applies\nwe have rate limits on #tokens; higher costs for more tokens; full corpus can‚Äôt fit in context window. RAG reduces cost per token.\n\n\n\n\n\n\nCandidate Retrieval: Millions of chunks to hundreds of candidates using bi-encoder embeddings and Approximate Nearest Neighbors (ANN)\n\nBi-Encoder: Query and document chunk encoded independently via SentenceBERT (SBERT) -&gt; compute fast cosine similarity. Also called siamese\nHybrid: embedding search + BM25\nhyperparameters:Embedding size, chunk size, overlap between chunks\n\nReranking (optional): rescore candidates using Cross-Encoder -&gt; query and document fed simultaneously -&gt; self-attention magic -&gt; more accurate score than simple cosine similarity\nContext Composition:\nGeneration\n\n\n\n\n\n\nflowchart LR\n    C[\"Retrieve &lt;br&gt;(High Recall w Hybrid)\"] --&gt; D[\"Rerank &lt;br&gt;(High Precision w Cross-Encoder)\"]\n    D --&gt; n1[\"Compose Context&lt;br&gt;(Clean, Dedup, Scope)\"]\n    n1 --&gt; n2[\"Generate\"]\n\n\n\n\n\n\n\n\n\n\nContextual Retrieval: use cheap LLM to contextualize chunks to make them ‚Äúself-contained‚Äù\n\ne.g., ‚ÄúHe won the election‚Äù -&gt; ‚ÄúDonald Trump won the 2024 election‚Ä¶‚Äù before embedding\nA chunk saying ‚ÄúIt increased by 5%‚Äù is mathematically useless to an embedding model if the subject (‚ÄúRevenue‚Äù or ‚ÄúChurn‚Äù?) was in the previous chunk.\nwidely used, no added latency, solves for ‚Äúlost in the middle‚Äù problem better than overlapping windows\nPrompt Caching: cache activations of static prompt prefixes saves costs (upto ~90%) -&gt; when writing a prompt, put reusable part on top (system, instructions, examples, static context etc.) as it is decoder only\n\nHyDE: generate fake document of query Q (as it is usually shorter, a question and doesn‚Äôt look like document) and then compare with document embeddings\n\nniche, introduces latency\n\n\n\n\n\n\nNDGC (normalized discounted cumulative gain)\n\nNDCG@10:If the right answer is in the top 10 but at rank #9, your system effectively failed (users won‚Äôt read it). NDCG penalizes this heavily.\n\nMRR (mean reciprocal rank) - MRR (Mean Reciprocal Rank) is often more honest than Recall. Recall@10 says: ‚ÄúWe found the answer in the top 10 results!‚Äù (Great, but if it was result #10, the LLM might have ignored it due to attention decay). MRR says:‚ÄúOn average, the answer appeared at Rank 1.2.‚Äù (This confirms the model actually saw the data).\nMAP (mean average precision)\nPrecision@k: Did we fetch only relevant stuff, or did we pollute the context window with noise? (needle in haystack)\nRecall@k: Did we find it at all?\n\n\n\n\nfor embedding model performance\n\n\n\n\n\n\ntool calling for structured data / deterministic operations, modeled as functions with arguments and return values\n\n\n\n\n\nClassification problem. Model outputs a probability distribution over available tools or null (if no tool required)\n\n\n\n\n\nsits between LLM and tool, integration logic is standardised\nStandardizes how an LLM reads a PDF, a SQL row, or a Slack message. It replaces your custom def get_slack_messages(): function.\nNov 2025 version\n\n\n\n\n\nAgent is a system that autonomously pursues goals and completes tasks on a user‚Äôs behalf\nTraditional vs Reasoning vs Agent (tool calls)\nwhile goal is not achieved: Observe -&gt; Plan -&gt; Act\nin practice, ReAct -&gt; LangGraph (state machine) - stricter\nhallucinations are a (big) problem\nagents interact with each other - A2A protocol (Google)\n\nIt‚Äôs gRPC/REST for Agents.\nStandardizes how a ‚ÄúTravel Agent‚Äù asks a ‚ÄúCalendar Agent‚Äù for availability. It handles the negotiation of intent, not just the reading of bytes.\n\n\n\n\n\n\nPrompt Injection - external content (e.g., website) are vulnerable to indirect prompt injection where hidden text on a webpage hijacks agent‚Äôs instructions\nGuardrails - lighter specialized models that can scan inputs/outputs for toxicity and policy violations before LLM processes them\n\n\n\n\n\nGood to start simple, then iterate and progressively scale up\nGood to start with capable models, optimize on size later\nTransparency / observability helps with user trust and debuggability\n\n\n\n\n\nSentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n\nThe reason we can search 10M documents in milliseconds (pre-computed embeddings) vs.¬†seconds (Cross-Encoders).\n\nHyDE paper | Precise Zero-Shot Dense Retrieval without Relevance Labels\nAnthropic Contextual Retrieval\nCross-Encoder\nAutomatic Tool Selection to Reduce Large Language Model Latency\nMCP - Nov 2025 - specification\nReAct paper | ReAct: Synergizing Reasoning and Acting in Language Models\n\nThe ‚ÄúWhile Loop‚Äù of AI. The paper that proved LLMs perform better when they ‚Äútalk to themselves‚Äù before acting.\n\nA2A: A New Era of Agent Interoperability | Google‚Äôs Agent2Agent Protocol (A2A)\n\nThe first major attempt to standardize inter-agent communication (horizontal) rather than just tool communication (vertical)\n\nToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages\n‚ÄúTowards Tool Use Alignment of Large Language Models‚Äù, Chen et al., 2024.\nAGENT-SAFETYBENCH: Evaluating the Safety of LLM Agents\nAnthropic Cyber Attack\n\n\nLicensing Notice: Text and media: CC BY 4.0; Code: Apache License 2.0"
  },
  {
    "objectID": "posts/field_notes/TIL-llm-agents.html#my-notes-and-reflections-on-lecture-7---agentic-llms",
    "href": "posts/field_notes/TIL-llm-agents.html#my-notes-and-reflections-on-lecture-7---agentic-llms",
    "title": "TIL from Stanford CME295 Transformers & LLMs | Lecture 7 - Agentic LLMs",
    "section": "",
    "text": "Lecture emphasises the shift from ‚ÄúPrompt Engineering‚Äù to ‚ÄúContext Engineering‚Äù\n\n\n\nModel has knowledge cut-off and it is tricky to update it (think catastrophic forgetting, LoRA/ fine-tuning and doing it for every knowledge-update or use-case)\neven with large context windows of 100k-1M tokens, we still need retrieval because there are problems like:\n\nfinding needle in haystack - high recall for single needle but for multiple needles (more real-world scenario) recall drops massively -&gt; garbage in, garbage out still applies\nwe have rate limits on #tokens; higher costs for more tokens; full corpus can‚Äôt fit in context window. RAG reduces cost per token.\n\n\n\n\n\n\nCandidate Retrieval: Millions of chunks to hundreds of candidates using bi-encoder embeddings and Approximate Nearest Neighbors (ANN)\n\nBi-Encoder: Query and document chunk encoded independently via SentenceBERT (SBERT) -&gt; compute fast cosine similarity. Also called siamese\nHybrid: embedding search + BM25\nhyperparameters:Embedding size, chunk size, overlap between chunks\n\nReranking (optional): rescore candidates using Cross-Encoder -&gt; query and document fed simultaneously -&gt; self-attention magic -&gt; more accurate score than simple cosine similarity\nContext Composition:\nGeneration\n\n\n\n\n\n\nflowchart LR\n    C[\"Retrieve &lt;br&gt;(High Recall w Hybrid)\"] --&gt; D[\"Rerank &lt;br&gt;(High Precision w Cross-Encoder)\"]\n    D --&gt; n1[\"Compose Context&lt;br&gt;(Clean, Dedup, Scope)\"]\n    n1 --&gt; n2[\"Generate\"]\n\n\n\n\n\n\n\n\n\n\nContextual Retrieval: use cheap LLM to contextualize chunks to make them ‚Äúself-contained‚Äù\n\ne.g., ‚ÄúHe won the election‚Äù -&gt; ‚ÄúDonald Trump won the 2024 election‚Ä¶‚Äù before embedding\nA chunk saying ‚ÄúIt increased by 5%‚Äù is mathematically useless to an embedding model if the subject (‚ÄúRevenue‚Äù or ‚ÄúChurn‚Äù?) was in the previous chunk.\nwidely used, no added latency, solves for ‚Äúlost in the middle‚Äù problem better than overlapping windows\nPrompt Caching: cache activations of static prompt prefixes saves costs (upto ~90%) -&gt; when writing a prompt, put reusable part on top (system, instructions, examples, static context etc.) as it is decoder only\n\nHyDE: generate fake document of query Q (as it is usually shorter, a question and doesn‚Äôt look like document) and then compare with document embeddings\n\nniche, introduces latency\n\n\n\n\n\n\nNDGC (normalized discounted cumulative gain)\n\nNDCG@10:If the right answer is in the top 10 but at rank #9, your system effectively failed (users won‚Äôt read it). NDCG penalizes this heavily.\n\nMRR (mean reciprocal rank) - MRR (Mean Reciprocal Rank) is often more honest than Recall. Recall@10 says: ‚ÄúWe found the answer in the top 10 results!‚Äù (Great, but if it was result #10, the LLM might have ignored it due to attention decay). MRR says:‚ÄúOn average, the answer appeared at Rank 1.2.‚Äù (This confirms the model actually saw the data).\nMAP (mean average precision)\nPrecision@k: Did we fetch only relevant stuff, or did we pollute the context window with noise? (needle in haystack)\nRecall@k: Did we find it at all?\n\n\n\n\nfor embedding model performance\n\n\n\n\n\n\ntool calling for structured data / deterministic operations, modeled as functions with arguments and return values\n\n\n\n\n\nClassification problem. Model outputs a probability distribution over available tools or null (if no tool required)\n\n\n\n\n\nsits between LLM and tool, integration logic is standardised\nStandardizes how an LLM reads a PDF, a SQL row, or a Slack message. It replaces your custom def get_slack_messages(): function.\nNov 2025 version\n\n\n\n\n\nAgent is a system that autonomously pursues goals and completes tasks on a user‚Äôs behalf\nTraditional vs Reasoning vs Agent (tool calls)\nwhile goal is not achieved: Observe -&gt; Plan -&gt; Act\nin practice, ReAct -&gt; LangGraph (state machine) - stricter\nhallucinations are a (big) problem\nagents interact with each other - A2A protocol (Google)\n\nIt‚Äôs gRPC/REST for Agents.\nStandardizes how a ‚ÄúTravel Agent‚Äù asks a ‚ÄúCalendar Agent‚Äù for availability. It handles the negotiation of intent, not just the reading of bytes.\n\n\n\n\n\n\nPrompt Injection - external content (e.g., website) are vulnerable to indirect prompt injection where hidden text on a webpage hijacks agent‚Äôs instructions\nGuardrails - lighter specialized models that can scan inputs/outputs for toxicity and policy violations before LLM processes them\n\n\n\n\n\nGood to start simple, then iterate and progressively scale up\nGood to start with capable models, optimize on size later\nTransparency / observability helps with user trust and debuggability\n\n\n\n\n\nSentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n\nThe reason we can search 10M documents in milliseconds (pre-computed embeddings) vs.¬†seconds (Cross-Encoders).\n\nHyDE paper | Precise Zero-Shot Dense Retrieval without Relevance Labels\nAnthropic Contextual Retrieval\nCross-Encoder\nAutomatic Tool Selection to Reduce Large Language Model Latency\nMCP - Nov 2025 - specification\nReAct paper | ReAct: Synergizing Reasoning and Acting in Language Models\n\nThe ‚ÄúWhile Loop‚Äù of AI. The paper that proved LLMs perform better when they ‚Äútalk to themselves‚Äù before acting.\n\nA2A: A New Era of Agent Interoperability | Google‚Äôs Agent2Agent Protocol (A2A)\n\nThe first major attempt to standardize inter-agent communication (horizontal) rather than just tool communication (vertical)\n\nToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages\n‚ÄúTowards Tool Use Alignment of Large Language Models‚Äù, Chen et al., 2024.\nAGENT-SAFETYBENCH: Evaluating the Safety of LLM Agents\nAnthropic Cyber Attack\n\n\nLicensing Notice: Text and media: CC BY 4.0; Code: Apache License 2.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Context Window",
    "section": "",
    "text": "Hi, I‚Äôm Shivam Miglani. Welcome to my context window. Here I document my journey through the Generative AI landscape, from foundational concepts to production engineering.\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\nDec 26, 2025\n\n\nTIL from Stanford CME295 Transformers & LLMs | Lecture 8 - LLM Evaluation\n\n\n\n\n\n\nDec 26, 2025\n\n\nTIL from Stanford CME295 Transformers & LLMs | Lecture 7 - Agentic LLMs\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Licensing",
    "section": "",
    "text": "This repository contains both literary content and functional source code. To ensure appropriate use of each, this project is licensed under a dual-license model:\n\n\nAll textual content, documentation, and images found within the blog posts and repository are licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). - View License: https://creativecommons.org/licenses/by/4.0/\n\n\n\nAll source code, software scripts, and executable code snippets within the notebooks or repository are licensed under the Apache License, Version 2.0. - View License: http://www.apache.org/licenses/LICENSE-2.0\n\n\n\n\nCopyright 2025 - Shivam Miglani\nLicensed under the Apache License, Version 2.0 (the ‚ÄúLicense‚Äù); you may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
  },
  {
    "objectID": "LICENSE.html#apache-license-2.0-summary",
    "href": "LICENSE.html#apache-license-2.0-summary",
    "title": "Licensing",
    "section": "",
    "text": "Copyright 2025 - Shivam Miglani\nLicensed under the Apache License, Version 2.0 (the ‚ÄúLicense‚Äù); you may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
  },
  {
    "objectID": "posts/field_notes/TIL-llm-eval.html",
    "href": "posts/field_notes/TIL-llm-eval.html",
    "title": "TIL from Stanford CME295 Transformers & LLMs | Lecture 8 - LLM Evaluation",
    "section": "",
    "text": "Output quality - instruction following - coherence - factuality\nSystem performance - latency - cost - reliability\n\nhuman ratings are gold standard.\n\nSubjectivity of human ratings -&gt; agreement rate = \\(p_{o}\\) -&gt; o stands for observed\n\n‚ÄúHow much better is our agreement than what we‚Äôd expect just by chance, given how the raters actually use the categories?‚Äù ‚ÄúA coefficient of agreement for nominal scales‚Äù, Cohen, 1960, ‚ÄúMeasuring nominal scale agreement among many raters‚Äù, Fleiss, 1971.\nVariants. Cohen‚Äôs Kappa, Fleiss‚Äô Kappa, Krippendorff‚Äôs alpha\n\nhuman ratings are slow and expensive\n\nrule-based metrics\n\nMETEOR. Metric for Evaluation of Translation with Explicit ORdering\n\nwrite labels once, use as reference\n\nBLEU (BiLingual Evaluation Understudy)\nROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n\nMany variants (ROUGE-N, ROUGE-L, etc.) with suite of metrics\n\nProblem with rule-based:\n\nDoes not take into account stylistic variations\nCorrelation with human rating is not that great\nstill requires human ratings\n\n\nLLM-as-a-Judge\n\nUse LLM to rate quality of response.\n&lt;prompt, model-response, criteria&gt; -&gt; &lt;score (pass/fail), reason&gt;\nStructured Outputs (in practice)\nBenefit: No need for reference / label; Interpretability via rationales\nVariations: Point-wise or Pair-wise (A or B is better)\n\nPosition A - position bias\n\nRemedy. ‚ÄúTake the average‚Äù, or tweak position embeddings\n\nVerbosity bias\n\nRemedy. Explicit guidelines, few-shot, and/or penalty on output length\n\nSelf-enhancement bias\n\nRemedy, Don‚Äôt use agent as judge\n\n\nBest pracitices\n\ncrisp guidelines\nbinary scale over granular ones\nwrite rationaile before outputting score (autoregressive)\nmitigate biases (position, verbosity, self-enhancement)\ncalibrate with human judgements\nlow temperature for reproducibility\n\n\nTypical things we want to evaluate for\n\ntask performance\n\nUsefulness\nFactuality\nRelevance\n\nalignment\n\ntone\nstyle\nsafety\n\n\nFactuality\n\nquantify factuality - president roosevelt and teddy example\ndecompose into various facts and do weighted average\n\nReAct\n\nTool prediction error\n\nLLM directly issues reposnse -&gt; tool router error -&gt; retrain tool router\nLLM directly issues response -&gt; tool found but model doesn‚Äôt know how to use it -&gt; SFT train model or better adjust prompt associated to target API\nhallucinates tool? -&gt; llm using tool that doesn‚Äôt exist -&gt; perhaps model too weak -&gt; upgrade model\n\nor API naming is not logical -&gt; revamp API\nor instructions are unclear -&gt; revamp top-level instructions\n\nLLM uses wrong tool\n\ntool router error -&gt; retrain tool router\nmodel chose wrong tool -&gt; SFT train or better adjust prompt associated to target API\n\nInfers wrong argument:\n\nargument can‚Äôt be inferred -&gt; Introduce a helper tool and/or ensure the context carries the right information\nModel doesn‚Äôt know how to use the tool -&gt; SFT train or better adjust prompt associated to target API\n\n\n\nTool call errors\n\nwrong response or error returned\n\nsometimes, errors are legitimitate\notherwise, fix the tool implementation\n\nno response\n\nwhen final response is hallucinated, we often see this\nReturn something, even if it‚Äôs an empty JSON; in general, return meaningful tool outputs\n\n\nResponse generation error: wrong response\n\nFinal response doesn‚Äôt convey the tool‚Äôs response\n\nmodel lacks grounding capabilities - upgrade LLM\nTool response spams the context window - Trim/summarize information returned by the backend\nTool response does not convey information meaningfully - Make the tool output format descriptive\n\n\n\n\nSummary\n- Failures: does not use/hallucinates/uses the wrong tool, infers wrong argument - Failures: wrong response, no response (tool impl.) - Failure: wrong response (generation)\nModeling - Weak reasoning, grounding capabilities - Too much going on in the context window - Tool modeling isn‚Äôt right Tool - Tool itself has a problem - Output of the tool isn‚Äôt interpretable\nCommon benchmarks - MMLU for knowledge - Massive Multitask Language Understanding - MCQ ‚Ä¢ Accurately state facts about the world ‚Ä¢ More ‚Äúbreadth‚Äù than ‚Äúdepth‚Äù ‚Ä¢ Reflects pretraining quality\n\nReasoning benchmarks\n\nAIME (American Invitational Mathematics Examination)\n\noutput correct answer on ~30 math problems Geometry, Algebra, Analysis\n\nPIQA (Physical Interaction: Question Answering )\n\nCharacteristics. 2 possible choices per question on everyday situations anchored in Physics. Has approximately 20,000 examples. Evaluation criteria. Find the right choice among Sol1/Sol2.\n\n\nCoding\n\nSWE bench (SoftWare Engineering benchmark)\n\n‚Ä¢ Generate syntactically correct code ‚Ä¢ Tests for programming proficiency ‚Ä¢ Proxy for tool use abilities\n\n\n\nCharacteristics. 2,294 software engineering problems from real GitHub issues across 12 popular Python repositories. Each problem has: ‚óè a base commit ‚óè an already merged PR with tests\nEvaluation criteria. Generated PR passes all test cases.\n\nSafety\nPrevent harmful, toxic, inappropriate behavior ‚Ä¢ Surfaces vulnerabilities before deployment ‚Ä¢ Alignment with custom preferences\nHarmBench (Harmful Behavior Benchmark) Characteristics. 510 unique harmful behaviors (400 text-based, 110 multimodal) split into: ‚óè ‚ÄúStandard‚Äù ‚óè ‚ÄúCopyright‚Äù ‚óè ‚ÄúContextual‚Äù ‚óè ‚ÄúMultimodal‚Äù Criteria: violate laws and widely-held norms Evaluation criteria. Attack success rate (ASR).\nclassifier as output\nthere are both contextual as well as multi-modal behaviors\nAgents\n\nùúè-bench = Tool-Agent-User Interaction Benchmark\nCharacteristics. A given set of database schema, APIs and policies across 2 domains: ‚óè Airline agent ‚óã 500 users, 300 flights, 2000 reservations ‚óã ~10 tools and 50 tasks ‚óè Retail agent ‚óã 500 users, 50 products, 1000 orders ‚óã ~10 tools and 115 tasks Evaluation criteria. Maximize reward and pass^k\n\nPass^k = ‚ÄúProbability that all k attempts succeed‚Äù\n\nRole of benchmarks. ‚óè A projection of performance across a given axis ‚óè Different models may be good at different things\nDefinition. Pareto curve = set of solutions that ‚Äúoptimizes‚Äù a trade-off. https://winston-bosan.github.io/llm-pareto-frontier/\nPossible trade-offs. ‚óè quality vs.¬†cost/latency ‚óè quality vs.¬†safety ‚óè quality vs.¬†context length\n\nBeware of data contamination Problem. Benchmark‚Äôs clues may be contained in the training set\nPrecautions. ‚óè Use an identifier such as a hash ‚óè For tools, use a blocklist ‚óè Evaluate on newer test versions!\nLessons. ‚óè Should not over-index on benchmarks ‚óè Need for ~organic perspectives to complete the picture: Chatbot Arena ‚óè ‚Ä¶just try a few models out yourself!\n\nLicensing Notice: Text and media: CC BY 4.0; Code: Apache License 2.0"
  }
]