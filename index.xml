<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>My Context Window</title>
<link>https://shivam-miglani.github.io/genai-blog/</link>
<atom:link href="https://shivam-miglani.github.io/genai-blog/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Mon, 22 Dec 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Retrieval-Augmented Generation (RAG) Fundamentals</title>
  <dc:creator>Shivam Miglani</dc:creator>
  <link>https://shivam-miglani.github.io/genai-blog/posts/rag/rag.html</link>
  <description><![CDATA[ 




<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Licensing Notice
</div>
</div>
<div class="callout-body-container callout-body">
<p>Text and media: <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> Code and snippets: <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></p>
</div>
</div>
<section id="motivation-and-meaning" class="level2">
<h2 class="anchored" data-anchor-id="motivation-and-meaning">Motivation and Meaning</h2>
<p><strong>R</strong>etrieval <strong>A</strong>ugmented <strong>G</strong>eneration (RAG) is inference-time context injection: pull <strong>relevant</strong> external data and condition generation on it. This <em>grounds</em> responses in authoritative sources while keeping LLMs parametric knowledge (weights) frozen.</p>
<section id="motivation" class="level4">
<h4 class="anchored" data-anchor-id="motivation">Motivation</h4>
<ul>
<li><strong>Knowledge limited to training data</strong>: Your proprietary/domain-specific data isn’t there</li>
<li><strong>Fixed knowledge cut-off date</strong>: The model can’t answer questions about recent events. Without RAG, they will either refuse to answer or <em>hallucinate</em>.</li>
</ul>
<section id="alternatives-trade-offs" class="level5">
<h5 class="anchored" data-anchor-id="alternatives-trade-offs">Alternatives &amp; Trade-offs</h5>
<ul>
<li>Why not fine-tune?
<ul>
<li>Fine-tuning excels at teaching <em>task formats</em> (SQL generation, JSON output) and <em>reasoning styles</em>, not injecting factual knowledge.</li>
<li><strong>Catastrophic forgetting</strong>: Updating knowledge degrades performance on other tasks as model weights get overwritten.</li>
<li><strong>Inefficient</strong>: requires separate fine-tuned checkpoints per domain or use-case or document. It is tricky to learn new knowledge without regressing on old knowledge even with LoRA/QLoRA.</li>
</ul></li>
<li>Ok, why not just stuff everything in the context window?
<ul>
<li><strong>Recall Degradation</strong>: While 1M+ token models ace “single-needle” tests, performance drops significantly (to ~60-70%) when retrieving multiple distributed facts</li>
<li><strong>Cost &amp; Latency</strong>: Processing massive contexts is computationally expensive and slow compared to vector search. Retrieval remains necessary for corpora exceeding the window size.</li>
</ul></li>
<li>RAG is a reasonable pattern:
<ul>
<li>When you need fresh, attributable knowledge with minimal model changes and can tolerate added latency.</li>
</ul></li>
</ul>
</section>
</section>
<section id="the-rag-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="the-rag-pipeline">The RAG Pipeline</h4>
<p>RAG acts as a filter to inject only <em>relevant</em> context. A typical production pipeline looks like this:</p>
<ol start="0" type="1">
<li><strong>Ingestion &amp; Indexing</strong>: Chunk documents, generate embeddings, and upsert into a vector database. <em>Note: In production, this is a continuous sync pipeline, not a one-time setup.</em></li>
<li><strong>Retrieval</strong>: For a user query, search your indexed corpus (vector/keyword) and pull the top‑k relevant chunks, often followed by a <em>re-ranking</em> step for precision.</li>
<li><strong>Augmented (prompt)</strong>: Inject selected chunks into the system prompt or user message with appropriate metadata (source citations).</li>
<li><strong>Generation</strong>: The LLM generates an answer conditioned <em>strictly</em> on the provided context, minimizing external knowledge leakage.</li>
</ol>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-mermaid-ragv1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mermaid-ragv1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-mermaid-ragv1">flowchart TB
    n2["LLM"] L_n2_n4_0@-- generates grounded answer --&gt; n4["Answer"]
    n3["Document Corpus"] L_n3_n5_0@&lt;-- ingestion pipeline&lt;br&gt;(chunk + embed) --&gt; n5["Hybrid index&lt;br&gt;(inverted keywords&lt;br&gt;+ &lt;br&gt;vector embeddings)&lt;br&gt;&lt;br&gt;"]
    n5 L_n5_n6_0@-- "top-k" --&gt; n6["Retrieval &amp;amp; Re-ranking"]
    n6 L_n6_n7_0@-- "top-k re-ranked chunks + citation metadata" --&gt; n7["Prompt Builder"]
    n7 L_n7_n2_0@-- "system prompt (use only given context) + user query + &lt;br&gt;top-k re-ranked chunks &amp;amp; citation metadata" --&gt; n2
    n1["User Query"] L_n1_n8_0@--&gt; n8["Query processing &lt;br&gt;&amp;amp; embedding"]
    n1 L_n1_n7_0@-- user query --&gt; n7
    n8 L_n8_n6_0@-- text + query expansions + embeddings --&gt; n6

    n3@{ shape: docs}
    n5@{ shape: cyl}
    n6@{ shape: rect}
    n7@{ shape: rect}
    n1@{ shape: rect}
    n8@{ shape: rect}

    L_n2_n4_0@{ animation: slow } 
    L_n3_n5_0@{ animation: none } 
    L_n5_n6_0@{ animation: slow } 
    L_n6_n7_0@{ animation: slow } 
    L_n7_n2_0@{ animation: slow } 
    L_n1_n8_0@{ animation: slow } 
    L_n1_n7_0@{ animation: slow } 
    L_n8_n6_0@{ animation: slow }
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mermaid-ragv1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: RAG pipeline flowchart showing ingestion pipeline, query processing, retrieval, augmentation and LLM generation.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="4536ff63" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pyspark</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> delta <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pyspark.sql <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SparkSession</span>
<span id="cb1-4"></span>
<span id="cb1-5">builder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (SparkSession.builder</span>
<span id="cb1-6">           .appName(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LocalDatabricksPrep"</span>)</span>
<span id="cb1-7">           .master(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"local[*]"</span>)</span>
<span id="cb1-8">           .config(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"spark.sql.extensions"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"io.delta.sql.DeltaSparkSessionExtension"</span>)</span>
<span id="cb1-9">           .config(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"spark.sql.catalog.spark_catalog"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"org.apache.spark.sql.delta.catalog.DeltaCatalog"</span>)</span>
<span id="cb1-10">           <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This downloads the Delta jar automatically:</span></span>
<span id="cb1-11">           .config(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"spark.jars.packages"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"io.delta:delta-spark_2.12:3.2.0"</span>) </span>
<span id="cb1-12">)</span>
<span id="cb1-13"></span>
<span id="cb1-14">spark <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> configure_spark_with_delta_pip(builder).getOrCreate()</span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Spark + Delta session created!"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Spark + Delta session created!</code></pre>
</div>
</div>
<div id="3f52f383" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pyspark.sql.functions <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> F</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pyspark.sql.window <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Window</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pyspark.sql.types <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> T</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb3-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pyspark <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SparkFiles</span>
<span id="cb3-6"></span>
<span id="cb3-7"></span>
<span id="cb3-8">url <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv"</span></span>
<span id="cb3-9">spark.sparkContext.addFile(url)</span>
<span id="cb3-10"></span>
<span id="cb3-11"></span>
<span id="cb3-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Download Raw CSV directly</span></span>
<span id="cb3-13">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> spark.read.option(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"header"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"true"</span>).option(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"inferSchema"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"true"</span>).csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"file://"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> SparkFiles.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"netflix_titles.csv"</span>))</span>
<span id="cb3-14">df.show(truncate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
|show_id|type   |title|director         |cast                                                                                                                                                                      |country      |date_added       |release_year|rating|duration |listed_in                                               |description                                                                                                                                          |
+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
|s1     |TV Show|3%   |NULL             |João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi|Brazil       |August 14, 2020  |2020        |TV-MA |4 Seasons|International TV Shows, TV Dramas, TV Sci-Fi &amp; Fantasy  |In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.             |
|s2     |Movie  |7:19 |Jorge Michel Grau|Demián Bichir, Héctor Bonilla, Oscar Serrano, Azalia Ortiz, Octavio Michel, Carmen Beato                                                                                  |Mexico       |December 23, 2016|2016        |TV-MA |93 min   |Dramas, International Movies                            |After a devastating earthquake hits Mexico City, trapped survivors from all walks of life wait to be rescued while trying desperately to stay alive. |
|s3     |Movie  |23:59|Gilbert Chan     |Tedd Chan, Stella Chung, Henley Hii, Lawrence Koh, Tommy Kuan, Josh Lai, Mark Lee, Susan Leong, Benjamin Lim                                                              |Singapore    |December 20, 2018|2011        |R     |78 min   |Horror Movies, International Movies                     |When an army recruit is found dead, his fellow soldiers are forced to confront a terrifying secret that's haunting their jungle island training camp.|
|s4     |Movie  |9    |Shane Acker      |Elijah Wood, John C. Reilly, Jennifer Connelly, Christopher Plummer, Crispin Glover, Martin Landau, Fred Tatasciore, Alan Oppenheimer, Tom Kane                           |United States|November 16, 2017|2009        |PG-13 |80 min   |Action &amp; Adventure, Independent Movies, Sci-Fi &amp; Fantasy|In a postapocalyptic world, rag-doll robots hide in fear from dangerous machines out to exterminate them, until a brave newcomer joins the group.    |
|s5     |Movie  |21   |Robert Luketic   |Jim Sturgess, Kevin Spacey, Kate Bosworth, Aaron Yoo, Liza Lapira, Jacob Pitts, Laurence Fishburne, Jack McGee, Josh Gad, Sam Golzari, Helen Carey, Jack Gilpin           |United States|January 1, 2020  |2008        |PG-13 |123 min  |Dramas                                                  |A brilliant group of students become card-counting experts with the intent of swindling millions out of Las Vegas casinos by playing blackjack.      |
+-------+-------+-----+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------------+------------+------+---------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
only showing top 5 rows</code></pre>
</div>
</div>
<div id="f78c8d83" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df.printSchema()</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>root
 |-- show_id: string (nullable = true)
 |-- type: string (nullable = true)
 |-- title: string (nullable = true)
 |-- director: string (nullable = true)
 |-- cast: string (nullable = true)
 |-- country: string (nullable = true)
 |-- date_added: string (nullable = true)
 |-- release_year: string (nullable = true)
 |-- rating: string (nullable = true)
 |-- duration: string (nullable = true)
 |-- listed_in: string (nullable = true)
 |-- description: string (nullable = true)
</code></pre>
</div>
</div>
<div id="ed3f5d68" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The Task:</span></span>
<span id="cb7-2"></span>
<span id="cb7-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># From netflix_bronze, select title, release_year, and date_added.</span></span>
<span id="cb7-4"></span>
<span id="cb7-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Clean date_added (currently string like "September 25, 2021") into a real date.</span></span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate days_diff = date_added - release_year (assume Jan 1st).</span></span>
<span id="cb7-8"></span>
<span id="cb7-9"></span>
<span id="cb7-10">sub_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.select(</span>
<span id="cb7-11">    F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"title"</span>),</span>
<span id="cb7-12">    F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"release_year"</span>),</span>
<span id="cb7-13">    F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"date_added"</span>),</span>
<span id="cb7-14">)</span>
<span id="cb7-15"></span>
<span id="cb7-16">sub_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sub_df.withColumn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"date_added_clean"</span>, F.to_date(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"date_added"</span>), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MMMM d, yyyy"</span>))</span>
<span id="cb7-17">sub_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sub_df.withColumn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"release_year_clean"</span>, F.to_date(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"release_year"</span>), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span>))</span>
<span id="cb7-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Assuming 1 jan of release year</span></span>
<span id="cb7-19">sub_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sub_df.withColumn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"days_diff"</span>, F.date_diff(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"date_added_clean"</span>), F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"release_year_clean"</span>))) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># default is somehow days</span></span>
<span id="cb7-20"></span>
<span id="cb7-21"></span>
<span id="cb7-22">sub_df.show(n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb7-23"></span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-----+------------+-----------------+----------------+------------------+---------+
|title|release_year|       date_added|date_added_clean|release_year_clean|days_diff|
+-----+------------+-----------------+----------------+------------------+---------+
|   3%|        2020|  August 14, 2020|      2020-08-14|        2020-01-01|      226|
| 7:19|        2016|December 23, 2016|      2016-12-23|        2016-01-01|      357|
|23:59|        2011|December 20, 2018|      2018-12-20|        2011-01-01|     2910|
|    9|        2009|November 16, 2017|      2017-11-16|        2009-01-01|     3241|
|   21|        2008|  January 1, 2020|      2020-01-01|        2008-01-01|     4383|
+-----+------------+-----------------+----------------+------------------+---------+
only showing top 5 rows</code></pre>
</div>
</div>
<div id="4d2bf442" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get a simple list of movies for "Brad Pitt".</span></span>
<span id="cb9-2"></span>
<span id="cb9-3">df.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">filter</span>(F.lower(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast"</span>)).contains(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"brad pitt"</span>)).select(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"title"</span>)).show()</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------------------+
|               title|
+--------------------+
|A Stoning in Fulh...|
|               Babel|
|          By the Sea|
|Inglourious Basterds|
| Killing Them Softly|
|    Ocean's Thirteen|
|      Ocean's Twelve|
|         War Machine|
+--------------------+
</code></pre>
</div>
</div>
<div id="f5d6cb74" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "Who are the top 5 most frequent actors in the dataset?"</span></span>
<span id="cb11-2">temp_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.withColumn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_split"</span>, F.split(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast"</span>), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">","</span>)).withColumn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_exploded"</span>, F.explode(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_split"</span>)))</span>
<span id="cb11-3">temp_df.groupBy(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_exploded"</span>)).agg({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_exploded"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>}).select(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_exploded"</span>, F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count(cast_exploded)"</span>).alias(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>)).orderBy(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>).desc()).show(n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-----------------+-----+
|    cast_exploded|count|
+-----------------+-----+
|      Anupam Kher|   38|
| Takahiro Sakurai|   28|
|          Om Puri|   27|
|   Shah Rukh Khan|   27|
|      Boman Irani|   25|
+-----------------+-----+
only showing top 5 rows</code></pre>
</div>
</div>
<div id="db0c5cb8" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">temp_df.groupBy(</span>
<span id="cb13-2">    F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_exploded"</span>)</span>
<span id="cb13-3">).agg(</span>
<span id="cb13-4">    F.count(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*"</span>).alias(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>)</span>
<span id="cb13-5">).orderBy(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>).desc()).show(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-----------------+-----+
|    cast_exploded|count|
+-----------------+-----+
|      Anupam Kher|   38|
| Takahiro Sakurai|   28|
|          Om Puri|   27|
|   Shah Rukh Khan|   27|
|      Boman Irani|   25|
+-----------------+-----+
only showing top 5 rows</code></pre>
</div>
</div>
<div id="f8acd96f" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">temp_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.withColumn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_split"</span>, F.split(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast"</span>), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">","</span>)).withColumn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_split_clean"</span>, F.transform(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_split"</span>), <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: F.trim(x)))</span>
<span id="cb15-2">temp_df.show()</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+
|show_id|   type| title|            director|                cast|             country|       date_added|release_year|rating| duration|           listed_in|         description|          cast_split|    cast_split_clean|
+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+
|     s1|TV Show|    3%|                NULL|João Miguel, Bian...|              Brazil|  August 14, 2020|        2020| TV-MA|4 Seasons|International TV ...|In a future where...|[João Miguel,  Bi...|[João Miguel, Bia...|
|     s2|  Movie|  7:19|   Jorge Michel Grau|Demián Bichir, Hé...|              Mexico|December 23, 2016|        2016| TV-MA|   93 min|Dramas, Internati...|After a devastati...|[Demián Bichir,  ...|[Demián Bichir, H...|
|     s3|  Movie| 23:59|        Gilbert Chan|Tedd Chan, Stella...|           Singapore|December 20, 2018|        2011|     R|   78 min|Horror Movies, In...|When an army recr...|[Tedd Chan,  Stel...|[Tedd Chan, Stell...|
|     s4|  Movie|     9|         Shane Acker|Elijah Wood, John...|       United States|November 16, 2017|        2009| PG-13|   80 min|Action &amp; Adventur...|In a postapocalyp...|[Elijah Wood,  Jo...|[Elijah Wood, Joh...|
|     s5|  Movie|    21|      Robert Luketic|Jim Sturgess, Kev...|       United States|  January 1, 2020|        2008| PG-13|  123 min|              Dramas|A brilliant group...|[Jim Sturgess,  K...|[Jim Sturgess, Ke...|
|     s6|TV Show|    46|         Serdar Akar|Erdal Beşikçioğlu...|              Turkey|     July 1, 2017|        2016| TV-MA| 1 Season|International TV ...|A genetics profes...|[Erdal Beşikçioğl...|[Erdal Beşikçioğl...|
|     s7|  Movie|   122|     Yasir Al Yasiri|Amina Khalil, Ahm...|               Egypt|     June 1, 2020|        2019| TV-MA|   95 min|Horror Movies, In...|After an awful ac...|[Amina Khalil,  A...|[Amina Khalil, Ah...|
|     s8|  Movie|   187|      Kevin Reynolds|Samuel L. Jackson...|       United States| November 1, 2019|        1997|     R|  119 min|              Dramas|After one of his ...|[Samuel L. Jackso...|[Samuel L. Jackso...|
|     s9|  Movie|   706|       Shravan Kumar|Divya Dutta, Atul...|               India|    April 1, 2019|        2019| TV-14|  118 min|Horror Movies, In...|When a doctor goe...|[Divya Dutta,  At...|[Divya Dutta, Atu...|
|    s10|  Movie|  1920|        Vikram Bhatt|Rajneesh Duggal, ...|               India|December 15, 2017|        2008| TV-MA|  143 min|Horror Movies, In...|An architect and ...|[Rajneesh Duggal,...|[Rajneesh Duggal,...|
|    s11|  Movie|  1922|        Zak Hilditch|Thomas Jane, Moll...|       United States| October 20, 2017|        2017| TV-MA|  103 min|   Dramas, Thrillers|A farmer pens a c...|[Thomas Jane,  Mo...|[Thomas Jane, Mol...|
|    s12|TV Show|  1983|                NULL|Robert Więckiewic...|Poland, United St...|November 30, 2018|        2018| TV-MA| 1 Season|Crime TV Shows, I...|In this dark alt-...|[Robert Więckiewi...|[Robert Więckiewi...|
|    s13|TV Show|  1994|Diego Enrique Osorno|                NULL|              Mexico|     May 17, 2019|        2019| TV-MA| 1 Season|Crime TV Shows, D...|Archival video an...|                NULL|                NULL|
|    s14|  Movie| 2,215| Nottapon Boonprakob|  Artiwara Kongmalai|            Thailand|    March 1, 2019|        2018| TV-MA|   89 min|Documentaries, In...|This intimate doc...|[Artiwara Kongmalai]|[Artiwara Kongmalai]|
|    s15|  Movie|  3022|          John Suits|Omar Epps, Kate W...|       United States|   March 19, 2020|        2019|     R|   91 min|Independent Movie...|Stranded when the...|[Omar Epps,  Kate...|[Omar Epps, Kate ...|
|    s16|  Movie|Oct-01|      Kunle Afolayan|Sadiq Daba, David...|             Nigeria|September 1, 2019|        2014| TV-14|  149 min|Dramas, Internati...|Against the backd...|[Sadiq Daba,  Dav...|[Sadiq Daba, Davi...|
|    s17|TV Show|Feb-09|                NULL|Shahd El Yaseen, ...|                NULL|   March 20, 2019|        2018| TV-14| 1 Season|International TV ...|As a psychology p...|[Shahd El Yaseen,...|[Shahd El Yaseen,...|
|    s18|  Movie|22-Jul|     Paul Greengrass|Anders Danielsen ...|Norway, Iceland, ...| October 10, 2018|        2018|     R|  144 min|   Dramas, Thrillers|After devastating...|[Anders Danielsen...|[Anders Danielsen...|
|    s19|  Movie|15-Aug|  Swapnaneel Jayakar|Rahul Pethe, Mrun...|               India|   March 29, 2019|        2019| TV-14|  124 min|Comedies, Dramas,...|On India's Indepe...|[Rahul Pethe,  Mr...|[Rahul Pethe, Mru...|
|    s20|  Movie|   '89|                NULL|Lee Dixon, Ian Wr...|      United Kingdom|     May 16, 2018|        2017| TV-PG|   87 min|       Sports Movies|Mixing old footag...|[Lee Dixon,  Ian ...|[Lee Dixon, Ian W...|
+-------+-------+------+--------------------+--------------------+--------------------+-----------------+------------+------+---------+--------------------+--------------------+--------------------+--------------------+
only showing top 20 rows</code></pre>
</div>
</div>
<div id="60d14329" class="cell" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># “For the top 10 actors by movie count, what is the average gap in years between their consecutive movies?”</span></span>
<span id="cb17-2">exploded_temp_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp_df.withColumn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"actor"</span>, F.explode(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cast_split_clean"</span>))).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">filter</span>(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"actor"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>).select(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"actor"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"title"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"release_year"</span>)</span>
<span id="cb17-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># exploded_temp_df.show()</span></span>
<span id="cb17-4"></span>
<span id="cb17-5">actor_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> exploded_temp_df.groupBy(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"actor"</span>)).agg(F.count(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*"</span>).alias(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>)).orderBy(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>).desc())</span>
<span id="cb17-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># actor_counts.show()</span></span>
<span id="cb17-7"></span>
<span id="cb17-8">top10 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> actor_counts.limit(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb17-9"></span>
<span id="cb17-10"></span>
<span id="cb17-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># join back on original table</span></span>
<span id="cb17-12">top10_actors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> exploded_temp_df.join(F.broadcast(top10), on<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"actor"</span>, how<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"inner"</span>)</span>
<span id="cb17-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># top10_actors.show()</span></span>
<span id="cb17-14"></span>
<span id="cb17-15"></span>
<span id="cb17-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># window func.</span></span>
<span id="cb17-17"></span>
<span id="cb17-18">w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Window.partitionBy(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"actor"</span>).orderBy(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"release_year"</span>)</span>
<span id="cb17-19"></span>
<span id="cb17-20"></span>
<span id="cb17-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 5. Compute previous movie year using lag, then gap</span></span>
<span id="cb17-22">gaps_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (top10_actors</span>
<span id="cb17-23">    .withColumn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"prev_year"</span>, F.lag(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"release_year"</span>).over(w))</span>
<span id="cb17-24">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">filter</span>(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"prev_year"</span>).isNotNull())</span>
<span id="cb17-25">    .withColumn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gap_years"</span>, F.datediff(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"release_year"</span>), F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"prev_year"</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>F.lit(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">365.0</span>))</span>
<span id="cb17-26">)</span>
<span id="cb17-27"></span>
<span id="cb17-28">gaps_df.show()</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------------+--------------------+------------+-----+---------+------------------+
|       actor|               title|release_year|count|prev_year|         gap_years|
+------------+--------------------+------------+-----+---------+------------------+
|Akshay Kumar|Mujhse Shaadi Karogi|        2004|   29|     2004|               0.0|
|Akshay Kumar|             Bewafaa|        2005|   29|     2004|1.0027397260273974|
|Akshay Kumar|               Insan|        2005|   29|     2005|               0.0|
|Akshay Kumar|         Bhagam Bhag|        2006|   29|     2005|               1.0|
|Akshay Kumar|Humko Deewana Kar...|        2006|   29|     2006|               0.0|
|Akshay Kumar|Jaan-E-Mann: Let'...|        2006|   29|     2006|               0.0|
|Akshay Kumar|     Phir Hera Pheri|        2006|   29|     2006|               0.0|
|Akshay Kumar|     Bhool Bhulaiyaa|        2007|   29|     2006|               1.0|
|Akshay Kumar|     Namastey London|        2007|   29|     2007|               0.0|
|Akshay Kumar|             Welcome|        2007|   29|     2007|               0.0|
|Akshay Kumar|      Action Replayy|        2010|   29|     2007|3.0027397260273974|
|Akshay Kumar|      Tees Maar Khan|        2010|   29|     2010|               0.0|
|Akshay Kumar|       Patiala House|        2011|   29|     2010|               1.0|
|Akshay Kumar|           Thank You|        2011|   29|     2011|               0.0|
|Akshay Kumar|               Joker|        2012|   29|     2011|               1.0|
|Akshay Kumar|           Oh My God|        2012|   29|     2012|               0.0|
|Akshay Kumar|       Rowdy Rathore|        2012|   29|     2012|               0.0|
|Akshay Kumar|                Boss|        2013|   29|     2012|1.0027397260273974|
|Akshay Kumar|Once Upon a Time ...|        2013|   29|     2013|               0.0|
|Akshay Kumar|          Special 26|        2013|   29|     2013|               0.0|
+------------+--------------------+------------+-----+---------+------------------+
only showing top 20 rows</code></pre>
</div>
</div>
<div id="f623a1d3" class="cell" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">gaps_df.groupBy(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"actor"</span>).agg(F.avg(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gap_years"</span>)).alias(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"avg_gap_year"</span>)).orderBy(F.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"avg_gap_year"</span>).asc()).show()</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>+----------------+-------------------+
|           actor|       avg_gap_year|
+----------------+-------------------+
|       Yuki Kaji|0.38482613277133826|
|Takahiro Sakurai| 0.4288649706457926|
|    Akshay Kumar| 0.5361056751467711|
|     Boman Irani| 0.6158061116965227|
|     Anupam Kher| 0.7077848312729702|
|  Shah Rukh Khan| 0.7946817082997581|
|    Paresh Rawal|  1.116122233930453|
|         Om Puri|  1.207746811525744|
|Naseeruddin Shah| 1.2422295701464334|
|Amitabh Bachchan| 1.6934668071654373|
+----------------+-------------------+
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>25/12/23 16:39:25 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 956198 ms exceeds timeout 120000 ms
25/12/23 16:39:25 WARN SparkContext: Killing executors is not supported by current scheduler.
25/12/23 16:39:26 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:26 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:36 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:36 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:46 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:46 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:56 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:39:56 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:23 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:23 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:33 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:33 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:43 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:43 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:53 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:54:53 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:55:03 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 16:55:03 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:17 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:17 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:27 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:27 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:37 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:37 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:47 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:47 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:57 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:10:57 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:18 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:18 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:28 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:28 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:38 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:38 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:48 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:48 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:58 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:26:58 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:42:57 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:42:57 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:07 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:07 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:17 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:17 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:27 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:27 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:37 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:43:37 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:28 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:28 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:38 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:38 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:48 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:48 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:58 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:55:58 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:56:08 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 17:56:08 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:12:53 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:12:53 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:03 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:03 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:13 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:13 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:23 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:13:23 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:28 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:28 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:38 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:38 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:48 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:48 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:58 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:30:58 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:08 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:08 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:18 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:18 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:28 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:31:28 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:04 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:04 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:14 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:14 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:24 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:24 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:34 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:34 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:44 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:48:44 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:32 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:32 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:42 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:42 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:52 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:56:52 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:57:02 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 18:57:02 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:17 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:17 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:27 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:27 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:37 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:37 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:47 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:47 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:57 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:12:57 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:28:57 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:28:57 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:07 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:07 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:17 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:17 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:27 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:27 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:37 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:37 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:47 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    ... 3 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:47 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Exception thrown in awaitResult: 
    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.178.105:54033
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)
    at scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.trySuccess(Promise.scala:99)
    at scala.concurrent.Promise.trySuccess$(Promise.scala:99)
    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)
    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)
    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
    at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)
    at scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)
    at scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at scala.concurrent.Promise.complete(Promise.scala:57)
    at scala.concurrent.Promise.complete$(Promise.scala:56)
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)
    at scala.concurrent.Promise.success(Promise.scala:91)
    at scala.concurrent.Promise.success$(Promise.scala:91)
    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)
    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
    ... 8 more
25/12/23 19:29:47 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times</code></pre>
</div>
</div>
<div id="950e0541" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">df.info()</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 7787 entries, 0 to 7786
Data columns (total 12 columns):
 #   Column        Non-Null Count  Dtype 
---  ------        --------------  ----- 
 0   show_id       7787 non-null   object
 1   type          7787 non-null   object
 2   title         7787 non-null   object
 3   director      5398 non-null   object
 4   cast          7069 non-null   object
 5   country       7280 non-null   object
 6   date_added    7777 non-null   object
 7   release_year  7787 non-null   int64 
 8   rating        7780 non-null   object
 9   duration      7787 non-null   object
 10  listed_in     7787 non-null   object
 11  description   7787 non-null   object
dtypes: int64(1), object(11)
memory usage: 730.2+ KB</code></pre>
</div>
</div>
<div id="9823034e" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'release_year'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'release_year'</span>].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span></code></pre></div></div>
</div>
<div id="c82f4880" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">help</span>(df.describe)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Help on method describe in module pandas.core.generic:

describe(percentiles=None, include=None, exclude=None) -&gt; 'Self' method of pandas.core.frame.DataFrame instance
    Generate descriptive statistics.

    Descriptive statistics include those that summarize the central
    tendency, dispersion and shape of a
    dataset's distribution, excluding ``NaN`` values.

    Analyzes both numeric and object series, as well
    as ``DataFrame`` column sets of mixed data types. The output
    will vary depending on what is provided. Refer to the notes
    below for more detail.

    Parameters
    ----------
    percentiles : list-like of numbers, optional
        The percentiles to include in the output. All should
        fall between 0 and 1. The default is
        ``[.25, .5, .75]``, which returns the 25th, 50th, and
        75th percentiles.
    include : 'all', list-like of dtypes or None (default), optional
        A white list of data types to include in the result. Ignored
        for ``Series``. Here are the options:

        - 'all' : All columns of the input will be included in the output.
        - A list-like of dtypes : Limits the results to the
          provided data types.
          To limit the result to numeric types submit
          ``numpy.number``. To limit it instead to object columns submit
          the ``numpy.object`` data type. Strings
          can also be used in the style of
          ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To
          select pandas categorical columns, use ``'category'``
        - None (default) : The result will include all numeric columns.
    exclude : list-like of dtypes or None (default), optional,
        A black list of data types to omit from the result. Ignored
        for ``Series``. Here are the options:

        - A list-like of dtypes : Excludes the provided data types
          from the result. To exclude numeric types submit
          ``numpy.number``. To exclude object columns submit the data
          type ``numpy.object``. Strings can also be used in the style of
          ``select_dtypes`` (e.g. ``df.describe(exclude=['O'])``). To
          exclude pandas categorical columns, use ``'category'``
        - None (default) : The result will exclude nothing.

    Returns
    -------
    Series or DataFrame
        Summary statistics of the Series or Dataframe provided.

    See Also
    --------
    DataFrame.count: Count number of non-NA/null observations.
    DataFrame.max: Maximum of the values in the object.
    DataFrame.min: Minimum of the values in the object.
    DataFrame.mean: Mean of the values.
    DataFrame.std: Standard deviation of the observations.
    DataFrame.select_dtypes: Subset of a DataFrame including/excluding
        columns based on their dtype.

    Notes
    -----
    For numeric data, the result's index will include ``count``,
    ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and
    upper percentiles. By default the lower percentile is ``25`` and the
    upper percentile is ``75``. The ``50`` percentile is the
    same as the median.

    For object data (e.g. strings or timestamps), the result's index
    will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``
    is the most common value. The ``freq`` is the most common value's
    frequency. Timestamps also include the ``first`` and ``last`` items.

    If multiple object values have the highest count, then the
    ``count`` and ``top`` results will be arbitrarily chosen from
    among those with the highest count.

    For mixed data types provided via a ``DataFrame``, the default is to
    return only an analysis of numeric columns. If the dataframe consists
    only of object and categorical data without any numeric columns, the
    default is to return an analysis of both the object and categorical
    columns. If ``include='all'`` is provided as an option, the result
    will include a union of attributes of each type.

    The `include` and `exclude` parameters can be used to limit
    which columns in a ``DataFrame`` are analyzed for the output.
    The parameters are ignored when analyzing a ``Series``.

    Examples
    --------
    Describing a numeric ``Series``.

    &gt;&gt;&gt; s = pd.Series([1, 2, 3])
    &gt;&gt;&gt; s.describe()
    count    3.0
    mean     2.0
    std      1.0
    min      1.0
    25%      1.5
    50%      2.0
    75%      2.5
    max      3.0
    dtype: float64

    Describing a categorical ``Series``.

    &gt;&gt;&gt; s = pd.Series(['a', 'a', 'b', 'c'])
    &gt;&gt;&gt; s.describe()
    count     4
    unique    3
    top       a
    freq      2
    dtype: object

    Describing a timestamp ``Series``.

    &gt;&gt;&gt; s = pd.Series([
    ...     np.datetime64("2000-01-01"),
    ...     np.datetime64("2010-01-01"),
    ...     np.datetime64("2010-01-01")
    ... ])
    &gt;&gt;&gt; s.describe()
    count                      3
    mean     2006-09-01 08:00:00
    min      2000-01-01 00:00:00
    25%      2004-12-31 12:00:00
    50%      2010-01-01 00:00:00
    75%      2010-01-01 00:00:00
    max      2010-01-01 00:00:00
    dtype: object

    Describing a ``DataFrame``. By default only numeric fields
    are returned.

    &gt;&gt;&gt; df = pd.DataFrame({'categorical': pd.Categorical(['d', 'e', 'f']),
    ...                    'numeric': [1, 2, 3],
    ...                    'object': ['a', 'b', 'c']
    ...                    })
    &gt;&gt;&gt; df.describe()
           numeric
    count      3.0
    mean       2.0
    std        1.0
    min        1.0
    25%        1.5
    50%        2.0
    75%        2.5
    max        3.0

    Describing all columns of a ``DataFrame`` regardless of data type.

    &gt;&gt;&gt; df.describe(include='all')  # doctest: +SKIP
           categorical  numeric object
    count            3      3.0      3
    unique           3      NaN      3
    top              f      NaN      a
    freq             1      NaN      1
    mean           NaN      2.0    NaN
    std            NaN      1.0    NaN
    min            NaN      1.0    NaN
    25%            NaN      1.5    NaN
    50%            NaN      2.0    NaN
    75%            NaN      2.5    NaN
    max            NaN      3.0    NaN

    Describing a column from a ``DataFrame`` by accessing it as
    an attribute.

    &gt;&gt;&gt; df.numeric.describe()
    count    3.0
    mean     2.0
    std      1.0
    min      1.0
    25%      1.5
    50%      2.0
    75%      2.5
    max      3.0
    Name: numeric, dtype: float64

    Including only numeric columns in a ``DataFrame`` description.

    &gt;&gt;&gt; df.describe(include=[np.number])
           numeric
    count      3.0
    mean       2.0
    std        1.0
    min        1.0
    25%        1.5
    50%        2.0
    75%        2.5
    max        3.0

    Including only string columns in a ``DataFrame`` description.

    &gt;&gt;&gt; df.describe(include=[object])  # doctest: +SKIP
           object
    count       3
    unique      3
    top         a
    freq        1

    Including only categorical columns from a ``DataFrame`` description.

    &gt;&gt;&gt; df.describe(include=['category'])
           categorical
    count            3
    unique           3
    top              d
    freq             1

    Excluding numeric columns from a ``DataFrame`` description.

    &gt;&gt;&gt; df.describe(exclude=[np.number])  # doctest: +SKIP
           categorical object
    count            3      3
    unique           3      3
    top              f      a
    freq             1      1

    Excluding object columns from a ``DataFrame`` description.

    &gt;&gt;&gt; df.describe(exclude=[object])  # doctest: +SKIP
           categorical  numeric
    count            3      3.0
    unique           3      NaN
    top              f      NaN
    freq             1      NaN
    mean           NaN      2.0
    std            NaN      1.0
    min            NaN      1.0
    25%            NaN      1.5
    50%            NaN      2.0
    75%            NaN      2.5
    max            NaN      3.0
</code></pre>
</div>
</div>
<div id="734f911d" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">df</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">show_id</th>
<th data-quarto-table-cell-role="th">type</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">director</th>
<th data-quarto-table-cell-role="th">cast</th>
<th data-quarto-table-cell-role="th">country</th>
<th data-quarto-table-cell-role="th">date_added</th>
<th data-quarto-table-cell-role="th">release_year</th>
<th data-quarto-table-cell-role="th">rating</th>
<th data-quarto-table-cell-role="th">duration</th>
<th data-quarto-table-cell-role="th">listed_in</th>
<th data-quarto-table-cell-role="th">description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>s1</td>
<td>TV Show</td>
<td>3%</td>
<td>NaN</td>
<td>João Miguel, Bianca Comparato, Michel Gomes, R...</td>
<td>Brazil</td>
<td>August 14, 2020</td>
<td>2020</td>
<td>TV-MA</td>
<td>4 Seasons</td>
<td>International TV Shows, TV Dramas, TV Sci-Fi &amp;...</td>
<td>In a future where the elite inhabit an island ...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>s2</td>
<td>Movie</td>
<td>7:19</td>
<td>Jorge Michel Grau</td>
<td>Demián Bichir, Héctor Bonilla, Oscar Serrano, ...</td>
<td>Mexico</td>
<td>December 23, 2016</td>
<td>2016</td>
<td>TV-MA</td>
<td>93 min</td>
<td>Dramas, International Movies</td>
<td>After a devastating earthquake hits Mexico Cit...</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>s3</td>
<td>Movie</td>
<td>23:59</td>
<td>Gilbert Chan</td>
<td>Tedd Chan, Stella Chung, Henley Hii, Lawrence ...</td>
<td>Singapore</td>
<td>December 20, 2018</td>
<td>2011</td>
<td>R</td>
<td>78 min</td>
<td>Horror Movies, International Movies</td>
<td>When an army recruit is found dead, his fellow...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>s4</td>
<td>Movie</td>
<td>9</td>
<td>Shane Acker</td>
<td>Elijah Wood, John C. Reilly, Jennifer Connelly...</td>
<td>United States</td>
<td>November 16, 2017</td>
<td>2009</td>
<td>PG-13</td>
<td>80 min</td>
<td>Action &amp; Adventure, Independent Movies, Sci-Fi...</td>
<td>In a postapocalyptic world, rag-doll robots hi...</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>s5</td>
<td>Movie</td>
<td>21</td>
<td>Robert Luketic</td>
<td>Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...</td>
<td>United States</td>
<td>January 1, 2020</td>
<td>2008</td>
<td>PG-13</td>
<td>123 min</td>
<td>Dramas</td>
<td>A brilliant group of students become card-coun...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">...</th>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">7782</th>
<td>s7783</td>
<td>Movie</td>
<td>Zozo</td>
<td>Josef Fares</td>
<td>Imad Creidi, Antoinette Turk, Elias Gergi, Car...</td>
<td>Sweden, Czech Republic, United Kingdom, Denmar...</td>
<td>October 19, 2020</td>
<td>2005</td>
<td>TV-MA</td>
<td>99 min</td>
<td>Dramas, International Movies</td>
<td>When Lebanon's Civil War deprives Zozo of his ...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">7783</th>
<td>s7784</td>
<td>Movie</td>
<td>Zubaan</td>
<td>Mozez Singh</td>
<td>Vicky Kaushal, Sarah-Jane Dias, Raaghav Chanan...</td>
<td>India</td>
<td>March 2, 2019</td>
<td>2015</td>
<td>TV-14</td>
<td>111 min</td>
<td>Dramas, International Movies, Music &amp; Musicals</td>
<td>A scrappy but poor boy worms his way into a ty...</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">7784</th>
<td>s7785</td>
<td>Movie</td>
<td>Zulu Man in Japan</td>
<td>NaN</td>
<td>Nasty C</td>
<td>NaN</td>
<td>September 25, 2020</td>
<td>2019</td>
<td>TV-MA</td>
<td>44 min</td>
<td>Documentaries, International Movies, Music &amp; M...</td>
<td>In this documentary, South African rapper Nast...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">7785</th>
<td>s7786</td>
<td>TV Show</td>
<td>Zumbo's Just Desserts</td>
<td>NaN</td>
<td>Adriano Zumbo, Rachel Khoo</td>
<td>Australia</td>
<td>October 31, 2020</td>
<td>2019</td>
<td>TV-PG</td>
<td>1 Season</td>
<td>International TV Shows, Reality TV</td>
<td>Dessert wizard Adriano Zumbo looks for the nex...</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">7786</th>
<td>s7787</td>
<td>Movie</td>
<td>ZZ TOP: THAT LITTLE OL' BAND FROM TEXAS</td>
<td>Sam Dunn</td>
<td>NaN</td>
<td>United Kingdom, Canada, United States</td>
<td>March 1, 2020</td>
<td>2019</td>
<td>TV-MA</td>
<td>90 min</td>
<td>Documentaries, Music &amp; Musicals</td>
<td>This documentary delves into the mystique behi...</td>
</tr>
</tbody>
</table>

<p>7787 rows × 12 columns</p>
</div>
</div>
</div>


</section>
</section>

 ]]></description>
  <category>RAG</category>
  <guid>https://shivam-miglani.github.io/genai-blog/posts/rag/rag.html</guid>
  <pubDate>Mon, 22 Dec 2025 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
