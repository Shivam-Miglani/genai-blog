<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>My Context Window</title>
<link>https://shivam-miglani.github.io/genai-blog/</link>
<atom:link href="https://shivam-miglani.github.io/genai-blog/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Fri, 26 Dec 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>TIL from Stanford CME295 Transformers &amp; LLMs | Lecture 8 - LLM Evaluation</title>
  <link>https://shivam-miglani.github.io/genai-blog/posts/field_notes/TIL-llm-eval.html</link>
  <description><![CDATA[ 




<p>Output quality - instruction following - coherence - factuality</p>
<p>System performance - latency - cost - reliability</p>
<ul>
<li>human ratings are gold standard.
<ul>
<li>Subjectivity of human ratings -&gt; agreement rate = <img src="https://latex.codecogs.com/png.latex?p_%7Bo%7D"> -&gt; o stands for observed
<ul>
<li>‚ÄúHow much better is our agreement than what we‚Äôd expect just by chance, given how the raters actually use the categories?‚Äù ‚ÄúA coefficient of agreement for nominal scales‚Äù, Cohen, 1960, ‚ÄúMeasuring nominal scale agreement among many raters‚Äù, Fleiss, 1971.</li>
<li>Variants. Cohen‚Äôs Kappa, Fleiss‚Äô Kappa, Krippendorff‚Äôs alpha</li>
</ul></li>
<li>human ratings are slow and expensive</li>
</ul></li>
<li>rule-based metrics
<ul>
<li>METEOR. Metric for Evaluation of Translation with Explicit ORdering
<ul>
<li>write labels once, use as reference</li>
</ul></li>
<li>BLEU (BiLingual Evaluation Understudy)</li>
<li>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
<ul>
<li>Many variants (ROUGE-N, ROUGE-L, etc.) with suite of metrics</li>
</ul></li>
<li>Problem with rule-based:
<ul>
<li>Does not take into account stylistic variations</li>
<li>Correlation with human rating is not that great</li>
<li>still requires human ratings</li>
</ul></li>
</ul></li>
<li>LLM-as-a-Judge
<ul>
<li>Use LLM to rate quality of response.</li>
<li>&lt;prompt, model-response, criteria&gt; -&gt; &lt;score (pass/fail), reason&gt;</li>
<li>Structured Outputs (in practice)</li>
<li>Benefit: No need for reference / label; Interpretability via rationales</li>
<li>Variations: Point-wise or Pair-wise (A or B is better)
<ul>
<li>Position A - position bias
<ul>
<li>Remedy. ‚ÄúTake the average‚Äù, or tweak position embeddings</li>
</ul></li>
<li>Verbosity bias
<ul>
<li>Remedy. Explicit guidelines, few-shot, and/or penalty on output length</li>
</ul></li>
<li>Self-enhancement bias
<ul>
<li>Remedy, Don‚Äôt use agent as judge</li>
</ul></li>
</ul></li>
<li>Best pracitices
<ul>
<li>crisp guidelines</li>
<li>binary scale over granular ones</li>
<li>write rationaile before outputting score (autoregressive)</li>
<li>mitigate biases (position, verbosity, self-enhancement)</li>
<li>calibrate with human judgements</li>
<li>low temperature for reproducibility</li>
</ul></li>
</ul></li>
<li>Typical things we want to evaluate for
<ul>
<li>task performance
<ul>
<li>Usefulness</li>
<li>Factuality</li>
<li>Relevance</li>
</ul></li>
<li>alignment
<ul>
<li>tone</li>
<li>style</li>
<li>safety</li>
</ul></li>
</ul></li>
<li>Factuality
<ul>
<li>quantify factuality - president roosevelt and teddy example</li>
<li>decompose into various facts and do weighted average</li>
</ul></li>
<li>ReAct
<ul>
<li>Tool prediction error
<ul>
<li>LLM directly issues reposnse -&gt; tool router error -&gt; retrain tool router</li>
<li>LLM directly issues response -&gt; tool found but model doesn‚Äôt know how to use it -&gt; SFT train model or better adjust prompt associated to target API</li>
<li>hallucinates tool? -&gt; llm using tool that doesn‚Äôt exist -&gt; perhaps model too weak -&gt; upgrade model
<ul>
<li>or API naming is not logical -&gt; revamp API</li>
<li>or instructions are unclear -&gt; revamp top-level instructions</li>
</ul></li>
<li>LLM uses wrong tool
<ul>
<li>tool router error -&gt; retrain tool router</li>
<li>model chose wrong tool -&gt; SFT train or better adjust prompt associated to target API</li>
</ul></li>
<li>Infers wrong argument:
<ul>
<li>argument can‚Äôt be inferred -&gt; Introduce a helper tool and/or ensure the context carries the right information</li>
<li>Model doesn‚Äôt know how to use the tool -&gt; SFT train or better adjust prompt associated to target API<br>
</li>
</ul></li>
</ul></li>
<li>Tool call errors
<ul>
<li>wrong response or error returned
<ul>
<li>sometimes, errors are legitimitate</li>
<li>otherwise, fix the tool implementation</li>
</ul></li>
<li>no response
<ul>
<li>when final response is hallucinated, we often see this</li>
<li>Return something, even if it‚Äôs an empty JSON; in general, return meaningful tool outputs</li>
</ul></li>
</ul></li>
<li>Response generation error: wrong response
<ul>
<li>Final response doesn‚Äôt convey the tool‚Äôs response
<ul>
<li>model lacks grounding capabilities - upgrade LLM</li>
<li>Tool response spams the context window - Trim/summarize information returned by the backend</li>
<li>Tool response does not convey information meaningfully - Make the tool output format descriptive</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p>Summary<br>
- Failures: does not use/hallucinates/uses the wrong tool, infers wrong argument - Failures: wrong response, no response (tool impl.) - Failure: wrong response (generation)</p>
<p>Modeling - Weak reasoning, grounding capabilities - Too much going on in the context window - Tool modeling isn‚Äôt right Tool - Tool itself has a problem - Output of the tool isn‚Äôt interpretable</p>
<p>Common benchmarks - MMLU for knowledge - Massive Multitask Language Understanding - MCQ ‚Ä¢ Accurately state facts about the world ‚Ä¢ More ‚Äúbreadth‚Äù than ‚Äúdepth‚Äù ‚Ä¢ Reflects pretraining quality</p>
<ul>
<li>Reasoning benchmarks
<ul>
<li>AIME (American Invitational Mathematics Examination)
<ul>
<li>output correct answer on ~30 math problems Geometry, Algebra, Analysis</li>
</ul></li>
<li>PIQA (Physical Interaction: Question Answering )
<ul>
<li>Characteristics. 2 possible choices per question on everyday situations anchored in Physics. Has approximately 20,000 examples. Evaluation criteria. Find the right choice among Sol1/Sol2.</li>
</ul></li>
</ul></li>
<li>Coding
<ul>
<li>SWE bench (SoftWare Engineering benchmark)
<ul>
<li>‚Ä¢ Generate syntactically correct code ‚Ä¢ Tests for programming proficiency ‚Ä¢ Proxy for tool use abilities</li>
</ul></li>
</ul></li>
</ul>
<p>Characteristics. 2,294 software engineering problems from real GitHub issues across 12 popular Python repositories. Each problem has: ‚óè a base commit ‚óè an already merged PR with tests</p>
<p>Evaluation criteria. Generated PR passes all test cases.</p>
<ul>
<li><p>Safety</p></li>
<li><p>Prevent harmful, toxic, inappropriate behavior ‚Ä¢ Surfaces vulnerabilities before deployment ‚Ä¢ Alignment with custom preferences</p></li>
<li><p>HarmBench (Harmful Behavior Benchmark) Characteristics. 510 unique harmful behaviors (400 text-based, 110 multimodal) split into: ‚óè ‚ÄúStandard‚Äù ‚óè ‚ÄúCopyright‚Äù ‚óè ‚ÄúContextual‚Äù ‚óè ‚ÄúMultimodal‚Äù Criteria: violate laws and widely-held norms Evaluation criteria. Attack success rate (ASR).</p></li>
<li><p>classifier as output</p></li>
<li><p>there are both contextual as well as multi-modal behaviors</p></li>
<li><p>Agents</p>
<ul>
<li>ùúè-bench = Tool-Agent-User Interaction Benchmark</li>
<li>Characteristics. A given set of database schema, APIs and policies across 2 domains: ‚óè Airline agent ‚óã 500 users, 300 flights, 2000 reservations ‚óã ~10 tools and 50 tasks ‚óè Retail agent ‚óã 500 users, 50 products, 1000 orders ‚óã ~10 tools and 115 tasks Evaluation criteria. Maximize reward and pass^k</li>
</ul></li>
<li><p>Pass^k = ‚ÄúProbability that all k attempts succeed‚Äù</p></li>
</ul>
<p>Role of benchmarks. ‚óè A projection of performance across a given axis ‚óè Different models may be good at different things</p>
<p>Definition. Pareto curve = set of solutions that ‚Äúoptimizes‚Äù a trade-off. https://winston-bosan.github.io/llm-pareto-frontier/</p>
<p>Possible trade-offs. ‚óè quality vs.&nbsp;cost/latency ‚óè quality vs.&nbsp;safety ‚óè quality vs.&nbsp;context length</p>
<hr>
<p>Beware of data contamination Problem. Benchmark‚Äôs clues may be contained in the training set</p>
<p>Precautions. ‚óè Use an identifier such as a hash ‚óè For tools, use a blocklist ‚óè Evaluate on newer test versions!</p>
<p>Lessons. ‚óè Should not over-index on benchmarks ‚óè Need for ~organic perspectives to complete the picture: Chatbot Arena ‚óè ‚Ä¶just try a few models out yourself!</p>
<blockquote class="blockquote">
<p><strong>Licensing Notice</strong>: Text and media: <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>; Code: <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></p>
</blockquote>



 ]]></description>
  <category>LLM Evaluation</category>
  <guid>https://shivam-miglani.github.io/genai-blog/posts/field_notes/TIL-llm-eval.html</guid>
  <pubDate>Fri, 26 Dec 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>TIL from Stanford CME295 Transformers &amp; LLMs | Lecture 7 - Agentic LLMs</title>
  <link>https://shivam-miglani.github.io/genai-blog/posts/field_notes/TIL-llm-agents.html</link>
  <description><![CDATA[ 




<section id="my-notes-and-reflections-on-lecture-7---agentic-llms" class="level2">
<h2 class="anchored" data-anchor-id="my-notes-and-reflections-on-lecture-7---agentic-llms">My notes and reflections on Lecture 7 - Agentic LLMs</h2>
<p>Lecture emphasises the shift from ‚ÄúPrompt Engineering‚Äù to ‚ÄúContext Engineering‚Äù</p>
<section id="rag-is-needed-for-fresh-knowledge" class="level3">
<h3 class="anchored" data-anchor-id="rag-is-needed-for-fresh-knowledge">RAG is needed for fresh knowledge</h3>
<ol type="1">
<li>Model has knowledge cut-off and it is tricky to update it (think catastrophic forgetting, LoRA/ fine-tuning and doing it for every knowledge-update or use-case)</li>
<li>even with large context windows of 100k-1M tokens, we still need <strong>retrieval</strong> because there are problems like:
<ul>
<li>finding needle in haystack - high recall for single needle but for multiple needles (more real-world scenario) recall drops massively -&gt; garbage in, garbage out still applies</li>
<li>we have rate limits on #tokens; higher costs for more tokens; full corpus can‚Äôt fit in context window. RAG reduces cost per token.</li>
</ul></li>
</ol>
</section>
<section id="rag-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="rag-pipeline">RAG pipeline</h3>
<ol type="1">
<li>Candidate Retrieval: Millions of chunks to hundreds of candidates using bi-encoder embeddings and Approximate Nearest Neighbors (ANN)
<ul>
<li>Bi-Encoder: Query and document chunk encoded independently via SentenceBERT (SBERT) -&gt; compute fast cosine similarity. Also called siamese</li>
<li>Hybrid: embedding search + BM25</li>
<li>hyperparameters:Embedding size, chunk size, overlap between chunks</li>
</ul></li>
<li>Reranking (optional): rescore candidates using Cross-Encoder -&gt; query and document fed simultaneously -&gt; self-attention magic -&gt; more accurate score than simple cosine similarity</li>
<li>Context Composition:</li>
<li>Generation</li>
</ol>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    C["Retrieve &lt;br&gt;(High Recall w Hybrid)"] --&gt; D["Rerank &lt;br&gt;(High Precision w Cross-Encoder)"]
    D --&gt; n1["Compose Context&lt;br&gt;(Clean, Dedup, Scope)"]
    n1 --&gt; n2["Generate"]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="high-value-tweaks-to-retrieval-step" class="level3">
<h3 class="anchored" data-anchor-id="high-value-tweaks-to-retrieval-step">High value tweaks to retrieval step</h3>
<ul>
<li><strong>Contextual Retrieval</strong>: use cheap LLM to contextualize chunks to make them ‚Äúself-contained‚Äù
<ul>
<li>e.g., ‚ÄúHe won the election‚Äù -&gt; ‚ÄúDonald Trump won the 2024 election‚Ä¶‚Äù before embedding</li>
<li>A chunk saying ‚ÄúIt increased by 5%‚Äù is mathematically useless to an embedding model if the subject (‚ÄúRevenue‚Äù or ‚ÄúChurn‚Äù?) was in the previous chunk.</li>
<li>widely used, no added latency, solves for ‚Äúlost in the middle‚Äù problem better than overlapping windows</li>
<li><strong>Prompt Caching</strong>: cache activations of static prompt prefixes saves costs (upto ~90%) -&gt; when writing a prompt, put reusable part on top (system, instructions, examples, static context etc.) as it is <em>decoder</em> only</li>
</ul></li>
<li><strong>HyDE</strong>: generate fake document of query Q (as it is usually shorter, a question and doesn‚Äôt look like document) and then compare with document embeddings
<ul>
<li>niche, introduces latency</li>
</ul></li>
</ul>
</section>
<section id="eval-metrics-for-retrieval" class="level3">
<h3 class="anchored" data-anchor-id="eval-metrics-for-retrieval">Eval metrics for Retrieval</h3>
<ul>
<li>NDGC (normalized discounted cumulative gain)
<ul>
<li>NDCG@10:If the right answer is in the top 10 but at rank #9, your system effectively failed (users won‚Äôt read it). NDCG penalizes this heavily.</li>
</ul></li>
<li>MRR (mean reciprocal rank) - MRR (Mean Reciprocal Rank) is often more honest than Recall. Recall@10 says: ‚ÄúWe found the answer in the top 10 results!‚Äù (Great, but if it was result #10, the LLM might have ignored it due to attention decay). MRR says:‚ÄúOn average, the answer appeared at Rank 1.2.‚Äù (This confirms the model actually saw the data).</li>
<li>MAP (mean average precision)</li>
<li>Precision@k: Did we fetch only relevant stuff, or did we pollute the context window with noise? (needle in haystack)</li>
<li>Recall@k: Did we find it at all?</li>
</ul>
<section id="mteb-massive-text-embedding-benchmark" class="level4">
<h4 class="anchored" data-anchor-id="mteb-massive-text-embedding-benchmark">MTEB (Massive Text Embedding Benchmark)</h4>
<ul>
<li>for embedding model performance</li>
</ul>
</section>
</section>
<section id="tool-calling-for-structured-data" class="level3">
<h3 class="anchored" data-anchor-id="tool-calling-for-structured-data">Tool Calling for Structured Data</h3>
<ul>
<li>tool calling for structured data / deterministic operations, modeled as functions with arguments and return values</li>
</ul>
</section>
<section id="tool-selection" class="level3">
<h3 class="anchored" data-anchor-id="tool-selection">Tool Selection</h3>
<ul>
<li>Classification problem. Model outputs a probability distribution over available tools or <code>null</code> (if no tool required)</li>
</ul>
</section>
<section id="mcp-model-context-protocol" class="level3">
<h3 class="anchored" data-anchor-id="mcp-model-context-protocol">MCP (Model Context Protocol)</h3>
<ul>
<li>sits between LLM and tool, integration logic is standardised</li>
<li>Standardizes how an LLM reads a PDF, a SQL row, or a Slack message. It replaces your custom def get_slack_messages(): function.</li>
<li><a href="https://modelcontextprotocol.io/specification/2025-11-25">Nov 2025 version</a></li>
</ul>
</section>
<section id="agents-and-react-reasoning-acting-framework" class="level3">
<h3 class="anchored" data-anchor-id="agents-and-react-reasoning-acting-framework">Agents and ReAct (Reasoning + Acting) Framework</h3>
<ul>
<li><em>Agent</em> is a system that autonomously pursues goals and completes tasks on a user‚Äôs behalf</li>
<li>Traditional vs Reasoning vs Agent (tool calls)</li>
<li><code>while goal is not achieved</code>: Observe -&gt; Plan -&gt; Act</li>
<li>in practice, ReAct -&gt; LangGraph (state machine) - stricter</li>
<li>hallucinations are a (big) problem</li>
<li>agents interact with each other - A2A protocol (Google)
<ul>
<li>It‚Äôs gRPC/REST for Agents.</li>
<li>Standardizes how a ‚ÄúTravel Agent‚Äù asks a ‚ÄúCalendar Agent‚Äù for availability. It handles the negotiation of intent, not just the reading of bytes.</li>
</ul></li>
</ul>
</section>
<section id="safety-and-guardrails" class="level3">
<h3 class="anchored" data-anchor-id="safety-and-guardrails">Safety and Guardrails</h3>
<ul>
<li><p><strong>Prompt Injection</strong> - external content (e.g., website) are vulnerable to indirect prompt injection where hidden text on a webpage hijacks agent‚Äôs instructions</p></li>
<li><p><strong>Guardrails</strong> - lighter specialized models that can scan inputs/outputs for toxicity and policy violations before LLM processes them</p></li>
</ul>
</section>
<section id="strategy" class="level3">
<h3 class="anchored" data-anchor-id="strategy">strategy</h3>
<ul>
<li>Good to start simple, then iterate and progressively scale up</li>
<li>Good to start with capable models, optimize on size later</li>
<li>Transparency / observability helps with user trust and debuggability</li>
</ul>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ul>
<li><a href="https://arxiv.org/abs/1908.10084">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a>
<ul>
<li>The reason we can search 10M documents in milliseconds (pre-computed embeddings) vs.&nbsp;seconds (Cross-Encoders).</li>
</ul></li>
<li>HyDE paper | <a href="https://arxiv.org/pdf/2212.10496">Precise Zero-Shot Dense Retrieval without Relevance Labels</a></li>
<li>Anthropic <a href="https://www.anthropic.com/engineering/contextual-retrieval">Contextual Retrieval</a></li>
<li><a href="https://sbert.net/examples/cross_encoder/applications/README.html">Cross-Encoder</a></li>
<li><a href="https://www.tdcommons.org/cgi/viewcontent.cgi?params=/context/dpubs_series/article/8702/&amp;path_info=Automatic_Tool_Selection_to_Reduce_Large_Language_Model_Latency.pdf">Automatic Tool Selection to Reduce Large Language Model Latency</a></li>
<li><a href="https://modelcontextprotocol.io/specification/2025-11-25">MCP - Nov 2025 - specification</a></li>
<li>ReAct paper | <a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a>
<ul>
<li>The ‚ÄúWhile Loop‚Äù of AI. The paper that proved LLMs perform better when they ‚Äútalk to themselves‚Äù before acting.</li>
</ul></li>
<li><a href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/">A2A: A New Era of Agent Interoperability</a> | <a href="https://a2a-protocol.org/latest/specification/">Google‚Äôs Agent2Agent Protocol (A2A)</a>
<ul>
<li>The first major attempt to standardize inter-agent communication (horizontal) rather than just tool communication (vertical)</li>
</ul></li>
<li><a href="https://arxiv.org/pdf/2402.10753">ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages</a></li>
<li><a href="https://aclanthology.org/2024.emnlp-main.82.pdf">‚ÄúTowards Tool Use Alignment of Large Language Models‚Äù, Chen et al., 2024.</a></li>
<li><a href="https://arxiv.org/pdf/2412.14470">AGENT-SAFETYBENCH: Evaluating the Safety of LLM Agents</a></li>
<li><a href="https://www.anthropic.com/news/disrupting-AI-espionage">Anthropic Cyber Attack</a></li>
</ul>
<blockquote class="blockquote">
<p><strong>Licensing Notice</strong>: Text and media: <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>; Code: <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></p>
</blockquote>


</section>
</section>

 ]]></description>
  <category>Agentic LLMs</category>
  <category>RAG</category>
  <guid>https://shivam-miglani.github.io/genai-blog/posts/field_notes/TIL-llm-agents.html</guid>
  <pubDate>Fri, 26 Dec 2025 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
